%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newcommand{\lyxaddress}[1]{
\par {\raggedright #1
\vspace{1.4em}
\noindent\par}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[ backend=biber]{biblatex}
\addbibresource{../proposal/prop.bib}
\addbibresource{cites1.bib}
%%\addbibresource{C:/Users/Yu-Tsung/workspace/Contextuality-and-Nonlocality-in-Discrete-Quantum-Theory/proposal/prop.bib}
%%\addbibresource{"../proposal/prop.bib"}
%%\addbibresource{prop.bib}
\ExecuteBibliographyOptions{sorting=none,maxbibnames=5,doi=false,isbn=false,url=false}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{bbold}
\usepackage{wesa}


\theoremstyle{remark}
\newtheorem{example}{Example}\newtheorem{definition}{Definition}\newtheorem{thm}{Theorem}

\newcommand{\Hilb}{\mathcal{H}}
\newcommand{\events}{\ensuremath{\mathcal{E}}}
\newcommand{\qevents}{\ensuremath{\mathcal{E}}}
\newcommand{\pmeas}{\ensuremath{\mu}}
\newcommand{\imposs}{{\mbox{\wesa{impossible}}}}
\newcommand{\likely}{{\mbox{\wesa{likely}}}}
\newcommand{\unlikely}{{\mbox{\wesa{unlikely}}}}
\newcommand{\necess}{{\mbox{\wesa{certain}}}}
\newcommand{\unknown}{{\mbox{\wesa{unknown}}}}
\newcommand{\ket}[1]{{\left\vert{#1}\right\rangle}}
\newcommand{\op}[2]{\ensuremath{\left\vert{#1}\middle\rangle\middle\langle{#2}\right\vert}}
\newcommand{\proj}[1]{\op{#1}{#1}}
\newcommand{\ps}{\texttt{+}}
\newcommand{\ms}{\texttt{-}}
\newcommand{\ip}[2]{\ensuremath{\left\langle{#1}\middle\vert{#2}\right\rangle}}
\newcommand{\Tr}{\mathop{\mathrm{Tr}}\nolimits}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\rmi}{\mathrm{i}}

\makeatother

\begin{document}

\title{Application for Rethinking Foundations of Physics 2017}

\author{Yu-Tsung Tai}
\maketitle

\lyxaddress{Department of Mathematics and School of Informatics and Computing\\
Indiana University, Bloomington}

\section{Statement of Motivation}

Physical processes are ultimately bounded in space, time, energy, and
other resources. Yet, for centuries, the mathematical formalization of
such processes has been founded on the infinitely precise real or
complex numbers. Indeed, physics textbooks casually refer to entities
such as $e$, $\pi$, $\sqrt{2}$, etc. From a computational perspective,
such numbers do not exist in their entirety ``for free.''  For
example, the state of the art algorithms for computing the $n$th
binary digit of~$\pi$ require on the order of $O(n\log^{O(1)}(n))$
operations~\cite{journals/moc/BaileyBP97}. In other words, simply
referring to the $n$th digit of $\pi$ requires more and more resources
as $n$ gets larger. Taking such resource bounds into consideration is
what founded computer science as a discipline and is crucial for
understanding the very nature of computation and, following
Feynman~\cite{Feynman1982Simulating}, Landauer~\cite{Landauer1996188},
and others, for understanding the very nature of physical processes.
 
\begin{quote}
  We therefore believe that the foundations of physics must be
  revisited from a computational perspective that only allows
  finitely-communicable evidence \emph{and} accounts for the resources
  needed to calculate, store, and communicate such evidence.
\end{quote}

\noindent This marriage of quantum mechanics and computer science first
envisioned and popularized by Feynman has created an awkward, but
opportune, moment. The embarrassing dilemma was concisely described by
Aaronson as follows~\cite{AaronsonArkhipov2011,BFRDARW2013}.  Consider
the following three statements:
\begin{enumerate}
\item[(A)] Textbook quantum mechanics is correct.
\item[(B)] There does not exist an efficient classical factoring
  algorithm.
\item[(C)] The extended Church-Turing thesis --- that probabilistic
  Turing machines can efficiently simulate any physically realizable
  model of computation --- is
  correct~\cite{BernsteinVazirani1997,Kaye2007}.
\end{enumerate}
There is overwhelming evidence to support each of these
statements. The theoretical framework of quantum mechanics (A) has
withstood decades of experimental confirmation. Entire industries are
founded on the assumption (B) that algorithms like RSA are secure and
they also have withstood years of attempted
attacks~\cite{boneh1999twenty,wiki:RSAFactoring}. Finally the entire
field of complexity theory in computer science which has also
withstood years of field testing rests, in essence, on assumption
(C)~\cite{Aaronson2005,Piccinini2015}.  And yet at least one of these
three statements \emph{must be false}! Indeed if textbook quantum
mechanics is false we concede (A). If it is correct Shor's efficient
factoring algorithm is realizable. If there is a corresponding
efficient classical factoring algorithm then we concede (B). If not
then we concede~(C).

It is unlikely that there will be a simple a resolution to this
awkward situation. It is more likely that the resolution will emerge
from deep and careful analyses of the foundations of each field. As
the world-renowned logician Girard states, the foundations of physics
and the foundations of logic and computation appear strongly
correlated:
\begin{quote}
  In other terms, what is so good in logic that quantum physics should
  obey? Can't we imagine that our conceptions about logic are wrong,
  so wrong that they are unable to cope with the quantum miracle?
  [\ldots] Instead of teaching logic to nature, it is more reasonable
  to learn from her. Instead of interpreting quantum into logic, we
  shall interpret logic into quantum~\cite{MSC:1444836}.
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Question}

The meaning of quantum probability 

The reality of the quantum state 

--- next section

The meaning of quantum probability: probability is approximate;
intervals; Born; no state when approximation is bad.

quantum state: Bayesian perspective; probability is subjective; hence
state is subjective; given previous results that show that state does
not exist when you have poor approximations; ?


In contrast to our previous works that assume the current quantum
theory is just an approximation to the discrete reality, we now assume
the nature really use real numbers. However, these real numbers could
not be directly accessed by human who is bounded by finite resources.
Therefore, even if the nature might have more computational power
than a Turing machine, human might not be able to acquire and use
the computational power more than a Turing machine.

In order to model the bounded-resource human observers, we would like
the theories of measurement and probability replaced by a subjective
probability model, called the interval-valued probability. The question
is how quantum mechanics would like with the interval-valued probability.

In particular, based on conventional probability, Gleason's theorem~\cite{gleason1957,Redhead1987-REDINA,peres1995quantum}
suggests that idea to define a state as a density matrix~\cite{Varadarajan2008}.
Since Gleason's theorem could not directly apply on interval-valued
probability measures, we would like to ask: How a quantum state looks
like with the interval-valued probability? When the number and precision
of the intervals increase, would the quantum interval-valued probability
measures converge to the quantum probability measure induced by the
Born rule~\cite{Born1983,Mermin2007,Jaeger2007}?

\section{Kick-off Talk}


In
previous work, we examined the Hilbert space mathematical framework of
quantum mechanics and teased apart its various aspects that directly
rely on the infinite-precision real or complex numbers. We note two
remarkable earlier results and an open question:
\begin{itemize}
\item UNIQUE-SAT: Schumacher and
  Westmoreland~\cite{Schumacher2012-SCHMQT} argue that much of the
  structure of traditional quantum mechanics is maintained in the
  presence of finite fields. In particular, they establish that the
  quantum theory based on the finite field of booleans retains the
  following characteristics of quantum mechanics: the notions of
  superposition, interference, entanglement, and mixed states of
  quantum systems; the time evolution of quantum systems using
  invertible linear operators; the complementarity of incompatible
  observables; the exclusion of local hidden variable theories and the
  impossibility of cloning quantum states; and the presence of natural
  counterparts of quantum information protocols such as superdense
  coding and teleportation. However we have argued that this theory is
  not physically realizable as it allows the solution of
  computationally difficult problems like
  UNIQUE-SAT~\cite{Valiant198685} in constant time~\cite{usat}.

\item Deutsch-Jozsa: In a follow-up paper~\cite{DQT2014}, we proposed
  a variant of quantum mechanics in which the underlying field of
  complex numbers is replaced by special finite fields with enough
  structure to model Hermitian products that \emph{locally} behave
  like inner products. The trick is to recognize that, as the
  characteristic of the finite field increases, larger and larger
  regions of the finite field become totally ordered, thereby
  restoring, with some restrictions, all the usual properties of the
  inner product needed for conventional quantum mechanics. Using this
  theory to analyze the Deutsch-Jozsa algorithm reveals that the
  characteristic of the finite field plays an essential role: as the
  size of the finite field grows, so does the numerical range of the
  intermediate and final results of the algorithms being implemented.
  The theory naturally recovers a deterministic measure of the
  intrinsic resources required for a given level of complexity; this
  measure is normally completely hidden in computations with real
  numbers, and explicitly exposing it is one of the significant
  achievements of our discrete field analysis of quantum computation.
  This solves the conundrum that the conventional Deutsch-Jozsa
  algorithm~\cite{DeutschJozsa1992,Jaeger2007}
  mysteriously continues to work for larger and larger input
  functions without any apparent increase in resources.  Our analysis
  of this problem reveals that as the size of the input increases, it
  is necessary to increase the size of the finite field and hence the
  size of the underlying available numeric coefficients.  This
  observation does not fully explain the power of quantum computing
  over classical computing, but at least it explains that some of the
  power of quantum computing depends on increasingly larger precision
  in the underlying field of numbers.

\item An important special case of a generalized measurement is the
  symmetric, informationally complete, positive operator valued
  measure (SIC-POVM), which has applications in quantum
  cryptography. This problem has been formulated in the context of
  standard quantum theory where fields are complex numbers.  Although
  a rigorous proof for the existence of SIC-POVMs for arbitrary
  Hilbert space dimensions is still missing, it would be illuminating
  to look for a formulation of the same problem in a discrete quantum
  theory defined in terms of finite Galois fields.
\end{itemize}

A long term, ultimate, goal of this research proposal would be to
combine the various pieces into a coherent computational model of
quantum mechanics that keeps explicit track of resources needed to
represent the interactive system representing the quantum state, its
evolution, the observables, and the probabilities. Such a
computational model of quantum mechanics would encompass classical
(reversible) computation and potentially revolutionize quantum
thinking with finite resources. The explicit manipulation of resources
enables one of key idea that emerged in our previous work: the
resources used to represent the quantum system are in principle
unrelated to the resources available to the various observers. We
conjecture that this asymmetry may prove key to resolving some of the
quantum paradoxes.  The nature of quantum mechanics realized in terms
of discrete fields and finitely-communicable probability estimates is
amenable to direct computational exploration for reasonable field
sizes and observational accuracy. We have established a set of utility
libraries in Mathematica that supports a wide variety of such
numerical experiments. Concurrently to this effort, we are also
building on our previous work~\cite
{VDS2009,AGVS2005,VAS2006,finiteQC,Sabry:2003:MQC:871895.871900}
to build semantically well-founded implementations of the
computational models investigated in this proposal.

\paragraph*{Research Questions.} The aim of these implementations
include:
\begin{itemize}
\item Provide an experimental testbed to simulate physical phenomena
  (e.g.~\cite {OrtizEtAl2001,SommEtAl2002}) that complements the
  Mathematica libraries in which quantum algorithms can be simulated;
\item Establish connections between quantum programming concepts and
  classical programming concepts. We plan to make our software
  available for educational and research purposes; and 
\item Analyze the computational complexity of quantum theories and
  simulations based on discrete fields. What other (apart from
  UNIQUE-SAT) supernatural algorithms can be implemented in
  computational models based on discrete fields?
\item Determine, through numerical computations, solutions for
  SIC-POVM for small vector space dimensions.
\end{itemize}






propose to revisit quantum mechanics, quantum information, quantum
computation, and the foundations of physics from this resource-aware
perspective. 


Soon after the discovery of quantum effects in the early 1900s, it
became evident that the quantum world requires a fresh worldview, not
merely an extension of the repertoire of entities and constructions
which explain classical physics. In the past thirty years, however,
the first quantum revolution mostly relied on just that: extensions of
classical logics, classical models of computation, and classical
analysis of algorithms. Fundamentally, the foundations of logic and
computing still embrace, to a large extent, the notion that the truth
of a statement is ``absolute'' and independent of any reasoning,
understanding, or action, i.e., logical statements are viewed as
abstract entities for which it makes no sense to talk about
entanglement with an observer: their truth or falsity is absolute and
does not depend on the notion of observation. This absolute view of
truth and the use of classical logic for reasoning about this absolute
truth have both been critiqued for centuries by philosophers and other
scientists but it is the advent of quantum science that really
devastates this worldview of classical determinism. Indeed, quantum
science suggests that the properties of physical systems can only be
learned by observers interacting with the system, and that this
interaction may entangle the observer with the system and disturb the
properties of interest. This suggests that the possibility structure
of our universe is not a fixed, Boolean structure, as we supposed
classically, but is in fact a non-Boolean structure that changes
dynamically~\cite{bub2006}.

We believe that the foundations of physics should be revisited from a
logical and computational approach

More directly, world-renowned logician Girard states:
Our vision for this revised computational and logic theory builds upon
programming abstractions which assume evolving, incomplete, and
inconsistent data, and principles for reasoning about programs which
are based on finitely communicable evidence between subsystems or
systems and observers, rather than absolute truth. The most natural
way to achieve this shift in our ``computational thinking'' and to
consolidate many of the new promising trends in computational,
informational, and logical theory is to design new programming
models. For it is indeed a widely accepted thesis that programming
languages shape thought processes and design
solutions~\cite{iverson80,johansen-berg01}. Our approach therefore is
to develop new programming models based on both logical foundations
and pragmatic considerations, to connect them to current
computational, informational, and logical theory, and to make
compelling arguments for the models by using them to write several
novel and challenging applications.

There is significant IU expertise in logic foundations, including
unconventional logics such paraconsistent logics, relevance logic,
multivalued logic with Bayesian inference, situation theory, and the
Dempster-Shafer theory of belief functions and plausible reasoning,
which can handle incomplete, evolving, and inconsistent information in
a controlled and discriminating way. There is also significant IU
expertise in the design and implementation of programming models, type
theories, and semantics that encompass both the classical and quantum
worlds. What IU lacks in expertise is an expert that bridges the
programming models with actual quantum algorithms: this is a new breed
of theoreticians with a strong background in both physics, complexity
theory, and cryptography.











Our initial results in that domain showed how subtle the
issues can be~\cite{usat,geometry2013,DQT2014}: a straightforward
replacement of the complex numbers by a finite field yields a variant
of quantum mechanics in which computationally hard problems like
UNIQUE-SAT (which decides whether a given Boolean formula is
unsatisfiable or has exactly one satisfying assignment) can be
deterministically solved in constant time. To eliminate such
unrealistic theories requires delicate analysis of the structure of
the Hilbert space, the process of observation, and the notion of
probability teasing apart their reliance on the infinitely precise
real numbers~\cite{geometry2013,DQT2014}. In this proposal, our aim is
to shift focus from the infinitely-specified but not directly
observable quantum states, to observable measurable properties of
quantum systems and their probabilities. Furthermore, we insist that
our theories of measurement and probability only refer to finitely
communicable evidence within feasible computational bounds. It follows
that states, observations, and probabilities all become ``fuzzy'',
i.e., specified by intervals of confidence that can only increase in
precision if the available resources increase proportionally. Our
notion of ``fuzzy quantum mechanics'' is related to existing
work~\cite{GranikCaulfield1996,Pykacz2013,SNL2009,Gudder2005,aerts1993physical}
but, as will be explained in more detail, is distinguished by its
unique computational character.

We will begin by reviewing existing work that recasts classical
probability spaces in a resource-aware setting and move to our
proposal which, briefly speaking, aims at recasting quantum
probability and hence quantum measurement to a corresponding
resource-aware setting.

In particular, the objectives to be achieved include:
\begin{itemize}
\item A computational framework for quantum measurement where quantum
  systems are described in terms of interactive processes. The
  framework takes care of the resources required (including the ones
  related to the observer) to determine a given property of the
  measured system.
\item Develop a measurement framework based on quantum interval-valued
  probabilities measures.
\item Assess the validity of fundamental theorems of quantum
  mechanics, such as Gleason, Bell, and Kochen-Specker, in a quantum
  formulation stripped away of infinite resources. We will aim to
  provide a definite answer to the Meyer-Mermin
  debate~\cite{PhysRevLett.83.3751, Mermin1999, BarrettKent2004} on
  the impact of finite precision measurements on the relevance of the
  Kochen-Specker
  theorem~\cite{kochenspecker1967,Redhead1987-REDINA,peres1995quantum,Jaeger2007}.
\item Study the computational complexity of quantum theories and
  simulations based on discrete (computable) fields.
\item Study the problem of finding a complete set of mutually unbiased
  bases
  (SIC-POVM)~\cite{1751-8121-45-8-085306,1751-8121-47-33-335302,Caves2002}
  over finite Galois fields.
\end{itemize}
We argue that no matter what the results are their impact will be
strong. Positive results that formulate a computable, with clear
resource bounds, theory of quantum measurement and quantum probability
would be essential for understanding and realizing quantum
computation. Any negative results would redirect research that aims at
realizing quantum computation to other approaches. At an intellectual
level, the results have the potential to solve or clarify several
paradoxes in quantum mechanics, quantum information, computability,
complexity theory, and perhaps quantum gravity.







For centuries, the mathematical formalization of physical processes
has been founded on the infinitely precise real or complex
numbers~\cite{Ziegler2007,weihrauch2012computable,blum2012complexity}.
For example, almost every description of quantum mechanics, quantum
computation, or quantum experiments refers to entities such as $\rme$,
$\pi$, $\sqrt{2}$, etc (see, e.g.,
\cite{Redhead1987-REDINA,544199,Mermin2007}).  Whether these models
based on real numbers faithfully describe the nature or just an
approximation is still an open question. If they are faithful, how the
nature store and compute these real numbers to produce our
experimental results would be the next natural question.  There are at
least three possibilities:
\begin{itemize}
\item For any computable number~\cite{Turing_1937}, the nature only store
the code of the Turing machine which computes the given number, and
manipulate these codes directly during the computation~\cite{Ziegler2007,weihrauch2012computable}.
\item The nature stores real numbers on infinitely long Turing machine tapes,
and handle these numbers by a Turing machine~\cite{Ziegler2007,weihrauch2012computable}.
In another word, the nature stores real numbers as a whole, but compute
them piecewisely.
\item The nature stores and computes real numbers as a whole~\cite{Ziegler2007,blum2012complexity}.
\end{itemize}
All these possibilities are not very desirable because they suggest
the nature might have more computational power than a Turing machine.
On one hand, if the nature only stores computable numbers by their
Turing machine codes, the nature might be able to decide whether two
Turing machines output exactly the same numbers, since this is used
to decide whether an observable is degenerated or not. However, testing
whether two Turing machines are output-equivalent is undecidable by
Rice's theorem~\cite{Rice_1953,Ziegler2007,weihrauch2012computable},
i.e., the nature has more computational power than a Turing machine.
On the other hand, if the nature stores real numbers as a whole, any
particular real number could serve as an oracle and give the nature
more computational power than a Turing machine. For example, maybe
the mass of the electron in some representation is the halting oracle
for some representation of Turing machines. Then, we could solve the
halting problem by just measuring the mass of the electron. This kind
of possibility has been online discussed by Peter Shor and others~\cite{PhysicsStackExchange16889}.

Since all possibilities that quantum theory is faithful are not very
desirable, maybe every description of quantum theory is just an approximation.
In this case, there is the other possibility:
\begin{itemize}
\item The real and complex numbers only exist in our model. The nature is
essentially discrete, and could be simulated by a Turing machine.
\end{itemize}
We have followed this approach to study quantum theory over discrete
finite fields~\cite{usat,geometry2013,DQT2014}. This yields a variant
of quantum mechanics in which computationally hard problems like UNIQUE-SAT
(which decides whether a given Boolean formula is unsatisfiable or
has exactly one satisfying assignment~\cite{Valiant198685,Papadimitriou1993,AroraBarak2009})
can be deterministically solved in constant time. We then delicate
analysis of the structure of the Hilbert space, the process of observation,
and the notion of probability by teasing apart their reliance on the
infinitely precise real numbers~\cite{geometry2013,DQT2014}.
















In the kick-off talk, we will begin by reviewing existing work that
recasts classical probability spaces in a resource-aware setting and
move to aim at recasting quantum probability and quantum measurement.
In particular, we develop a measurement framework based on quantum
interval-valued probability measures~(IVPM). Surprisingly, we found
a quantum IVPM which can not be induced by a state, while Shapley
proved a classical convex IVPM can always be induced by a classical
``state''~\cite{Shapley1971,Grabisch2016}, and Gleason proved
a infinitely-specified quantum probabilities measure can always be
induced by a quantum state~\cite{gleason1957,Redhead1987-REDINA,peres1995quantum}.
Nevertheless, a quantum IVPM could still correspond to many possible
states if it is broken into pieces. Our examples also suggest that
a quantum IVPM could be more and more likely induced by a state while
the intervals become sharper, i.e., the measurement resources increase.

\subsection{Classical Probability}

A \emph{probability space} specifies the necessary conditions for
reasoning coherently about collections of uncertain events~\cite{Kolmogorov1950,Shafer1976,Griffiths2003,Swart2013}.
We review the conventional presentation of probability spaces and
then discuss the computational resources needed to estimate probabilities.

%%%


\subsubsection{Classical Probability Spaces}

The conventional definition of a probability space builds upon the
field of real numbers. In more detail, a probability space consists
of a \emph{sample space} $\Omega$, a space of \emph{events}~$\events$,
and a \emph{probability measure}~$\pmeas$ mapping events in $\events$
to the real interval $[0,1]$. We will only consider \emph{finite}
sets of events and restrict our attention to non-empty finite sets
$\Omega$ as the sample space. The space of events $\events$ includes
every possible subset of $\Omega$: it is the powerset~$2^{\Omega}=\left\{ E~\middle|~E\subseteq\Omega\right\} $.
For future reference, we emphasize that events are the primary notion
of interest and that the sample space is a convenient artifact that
allows us to treat events as sets obeying the laws of Boolean algebra~\cite{Boole1948,Redhead1987-REDINA,Griffiths2003}.

\begin{definition}[Probability Measure]\label{def:ClassicalProbabilitySpace}
Given the set of events $\events$, a \emph{probability measure} is
a function $\pmeas:\events\rightarrow[0,1]$ such that: 
\begin{itemize}
\item $\pmeas(\emptyset)=0$, 
\item $\pmeas(\Omega)=1$, 
\item for every event $E$, $\pmeas\left(\Omega\backslash E\right)=1-\pmeas\left(E\right)$
where $\Omega\backslash E$ is the complement event of $E$, and 
\item for every collection $\left\{ E_{i}\right\} _{i=1}^{N}$ of pairwise
disjoint events, $\pmeas\left(\bigcup_{i=1}^{N}E_{i}\right)=\sum_{i=1}^{N}\pmeas(E_{i})$. 
\end{itemize}
\end{definition} There is some redundancy in the definition that
will be useful when moving to quantum probability spaces.

\noindent \begin{example}[Two-coins experiment]\label{ex1} Consider
an experiment that tosses two coins. We have four possible outcomes
that constitute the sample space $\Omega=\{HH,HT,TH,TT\}$. There
are 16 total events including the event $\{HH,HT\}$ that the first
coin lands heads up, the event $\{HT,TH\}$ that the two coins land
on opposite sides, and the event $\{HT,TH,TT\}$ that at least one
coin lands tails up. Here is a possible probability measure for these
events: 
\[
\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) & = & 0\\
\pmeas(\{HH\}) & = & 1/3\\
\pmeas(\{HT\}) & = & 0\\
\pmeas(\{TH\}) & = & 2/3\\
\pmeas(\{TT\}) & = & 0\\
\pmeas(\{HH,HT\}) & = & 1/3\\
\pmeas(\{HH,TH\}) & = & 1\\
\pmeas(\{HH,TT\}) & = & 1/3
\end{array} & \begin{array}{rcl}
\pmeas(\{HT,TH\}) & = & 2/3\\
\pmeas(\{HT,TT\}) & = & 0\\
\pmeas(\{TH,TT\}) & = & 2/3\\
\pmeas(\{HH,HT,TH\}) & = & 1\\
\pmeas(\{HH,HT,TT\}) & = & 1/3\\
\pmeas(\{HH,TH,TT\}) & = & 1\\
\pmeas(\{HT,TH,TT\}) & = & 2/3\\
\pmeas(\{HH,HT,TH,TT\}) & = & 1
\end{array}\end{array}
\]
\end{example}

\noindent It is useful to think that this probability measure is completely
determined by the ``reality'' of the two coins in question and their
characteristics, in the sense that each pair of coins induces a measure,
and each measure must correspond to some pair of coins. The measure
above would be induced by two particular coins such that the first
coin is twice as likely to land tails up than heads up and the second
coin is double-headed. In a strict computational or experimental setting,
one should question the reliance of the definition of probability
space on the uncountable and uncomputable real interval~$[0,1]$~\cite{Turing_1937,Ziegler2007,weihrauch2012computable}.
This interval includes numbers like $0.h_{1}h_{2}h_{3}\ldots$ where
$h_{i}$ is 1 or 0 depending on whether Turing machine $\mathit{TM}_{i}$
halts or not. Such numbers cannot be computed. This interval also
includes numbers like $\frac{\pi}{4}$ which can only be computed
with increasingly large resources as the precision increases. Therefore,
in a resource-aware computational or experimental setting, it is more
appropriate to consider probability measures that map events to a
set of elements computable with a fixed set of resources. We expand
on this observation in the next section and then recall its formalization
using interval-valued probability measures~\cite{Weichselberger2000,JamisonLodwick2004}.

%%%%%


\subsubsection{Measuring Probabilities: Buffon's Needle Problem\label{subsec:Measuring-Probabilities:-Buffon}}

In the previous example, we assumed the probability~$\pmeas(E)$
of each event~$E$ is known a priori. In reality, although each event
is assumed to have a probability, the exact value of $\pmeas(E)$
may not be known. According to the \emph{frequency interpretation
of probability} (which we will revisit when moving to the quantum
case)~\cite{Venn1876,Hajek2012}, to determine the probability of
an event, we run $M$ independent trials which gives us an approximation
of the (assumed) ``true'' or ``real'' probability. Let $x_{i}$
be 1 or 0 depending on whether the event~$E$ occurs in the $i$-th
trial or not, then $\pmeas(E)$ could be approximated to given accuracy~$\epsilon>0$
by the relative frequency~$\frac{1}{M}\sum_{i=1}^{M}x_{i}$ with
the probability converging to one as $M$ goes to infinity, i.e.,
\[
\forall\epsilon>0,\lim_{M\rightarrow\infty}\pmeas\left(\left|\pmeas(E)-\frac{1}{M}\sum_{i=1}^{M}x_{i}\right|<\epsilon\right)=1\textrm{ .}
\]
This fact is called the law of large numbers~\cite{Bernoulli2006,Kolmogorov1950,Uspensky1937,Shafer1976,544199}.

Let's look at a concrete example. Suppose we drop a needle of length
$\ell$ onto a floor made of equally spaced parallel lines a distance
$h$ apart, where $\ell<h$. It is a known fact that the probability
of the needle crossing a line is $\frac{2\ell}{\pi h}$~\cite{Buffon1777,DeMorgan1872,Hall1873,Uspensky1937}.
Consider an experimental setup consisting of a collection of $M$
identical needles of length $\ell$. We throw the $M$ needles one
needle at a time, and observe the number $M_{c}$ of needles that
cross a line, thus estimating the probability of a needle crossing
a line to be $\frac{M_{c}}{M}$. In an actual experiment with $500$
needles and the ratio $\frac{\ell}{h}=0.75$~\cite{Hall1873}, it
was found that $236$ crossed a line so the relative frequency is
$0.472$ whereas the idealized mathematical probability is $0.4774\ldots$.
In a larger experiment with $5000$ needles and the ratio $\frac{\ell}{h}=0.8$~\cite{Uspensky1937},
the relative frequency was calculated to be $0.5064$ whereas the
idealized mathematical probability is $0.5092\ldots$. We see that
the observed probability approaches $\frac{2\ell}{\pi h}$ but only
if \emph{larger and larger resources} are expended. These resource
considerations suggest that it is possible to replace the real interval
$[0,1]$ with rational numbers up to a certain precision related to
the particular experiment in question. There is clearly a connection
between the number of needles and the achievable precision: in the
hypothetical experiment with 3 needles, it is not sensible to retain
100 digits in the expansion of $\frac{2\ell}{\pi h}$.

There is another more subtle assumption of unbounded computational
power in the experiment. We are assuming that we can always determine
with certainty whether a needle is crossing a line. But ``lines''
on the the floor have thickness, their distance apart is not exactly
$h$, and the needles' lengths are not all absolutely equal to $\ell$.
These perturbations make the events ``fuzzy.'' Thus, in an experiment
with limited resources, it is not possible to talk about the idealized
event that exactly $M_{c}$ needles cross lines as this would require
the most expensive needles built to the most precise accuracy, laser
precision for drawing lines on the floor, and the most powerful microscopes
to determine if a needle does cross a line. Instead we might talk
about the event that $M_{c}-\delta$ needles evidently cross lines
and $M_{c}+\delta'$ needles plausibly cross lines where $\delta$
and $\delta'$ are resource-dependent approximations. This fuzzy notion
of events leads to probabilities being only calculable within intervals
of confidence reflecting the certainty of events and their plausibility.
This is indeed consistent with published experiments: in an experiment
with $3204$ needles and the ratio $\frac{\ell}{h}=0.6$~\cite{DeMorgan1872},
$1213$ needles clearly crossed a line and $11$ needles were close
enough to plausibly be considered as crossing the line: we would express
the probability in this case as the interval $\left[\frac{1213}{3204},\frac{1224}{3204}\right]$
expressing that we are certain that the event has probability at least
$\frac{1213}{3204}$ but it is possible that it would have probability
$\frac{1224}{3204}$.

%%%%%


\subsubsection{Classical Interval-Valued Probability Measures}

As motivated above, an event $E_{1}$ may have an interval of probability
$[l_{1},r_{1}]$. Assume that another disjoint event $E_{2}$ has
an interval of probability $[l_{2},r_{2}]$, what is the interval
of probability for the event $E_{1}\cup E_{2}$? The answer is somewhat
subtle: although it is possible to use the sum of the intervals $[l_{1}+l_{2},r_{1}+r_{2}]$
as the combined probability, one can find a much tighter interval
if information \emph{against} the event (i.e., information about the
complement event) is also taken into consideration. Formally, for
a general event $E$ with probability $[l,r]$, the evidence that
contradicts $E$ is evidence supporting the complement of $E$. The
complement of $E$ must therefore have probability $\left[1-r,1-l\right]$
which we abbreviate $\left[1,1\right]-\left[l,r\right]$. Given a
sample space~$\Omega$ and its set of events~$\events$, a function~$\bar{\mu}:\events\rightarrow[0,1]$
is a classical interval-valued probability measure if and only if
$\bar{\mu}$ satisfies the following conditions~\cite{JamisonLodwick2004}
where the last line uses $\subseteq$ to allow for tighter intervals
that exploit the complement event: 
\begin{itemize}
\item $\bar{\mu}(\emptyset)=[0,0]$. 
\item $\bar{\mu}(\Omega)=[1,1]$. 
\item For any event $E$, $\bar{\mu}\left(\Omega\backslash E\right)=\left[1,1\right]-\bar{\mu}\left(E\right)$ 
\item For a collection $\left\{ E_{i}\right\} _{i=1}^{M}$ of pairwise disjoint
events, we have $\bar{\mu}\left(\bigcup_{i=1}^{M}E_{i}\right)\subseteq\sum_{i=1}^{M}\bar{\mu}\left(E_{i}\right)$. 
\end{itemize}
\begin{example}[Two-coin experiment with interval probability]\label{ex3}
We split the unit interval $[0,1]$ in the following four closed sub-intervals:
$[0,0]$ which we call \imposs, $[0,\frac{1}{2}]$ which we call
\unlikely, $[\frac{1}{2},1]$ which we call \likely, and $[1,1]$
which we call \necess. Using these new values, we can modify the
probability measure of Ex.~\ref{ex1} by mapping each numeric value
to the smallest sub-interval containing it to get the following: 
\[
\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\bar{\mu}(\emptyset) & = & \imposs\\
\bar{\mu}(\{HH\}) & = & \unlikely\\
\bar{\mu}(\{HT\}) & = & \imposs\\
\bar{\mu}(\{TH\}) & = & \likely\\
\bar{\mu}(\{TT\}) & = & \imposs\\
\bar{\mu}(\{HH,HT\}) & = & \unlikely\\
\bar{\mu}(\{HH,TH\}) & = & \necess\\
\bar{\mu}(\{HH,TT\}) & = & \unlikely
\end{array} & \begin{array}{rcl}
\bar{\mu}(\{HT,TH\}) & = & \likely\\
\bar{\mu}(\{HT,TT\}) & = & \imposs\\
\bar{\mu}(\{TH,TT\}) & = & \likely\\
\bar{\mu}(\{HH,HT,TH\}) & = & \necess\\
\bar{\mu}(\{HH,HT,TT\}) & = & \unlikely\\
\bar{\mu}(\{HH,TH,TT\}) & = & \necess\\
\bar{\mu}(\{HT,TH,TT\}) & = & \likely\\
\bar{\mu}(\{HH,HT,TH,TT\}) & = & \necess
\end{array}\end{array}
\]
Despite the absence of infinitely precise numeric information, the
probability measure is quite informative: it reveals that the second
coin is double-headed and that the first coin is biased. To understand
the $\subseteq$-condition, consider the following calculation: 
\begin{eqnarray*}
\bar{\mu}(\{HH\})+\bar{\mu}(\{HT\})+\bar{\mu}(\{TH\})+\bar{\mu}(\{TT\}) & = & \imposs+\unlikely+\imposs+\likely\\
 & = & \left[0,0\right]+\left[0,\frac{1}{2}\right]+\left[0,0\right]+\left[\frac{1}{2},1\right]\\
 & = & \left[\frac{1}{2},\frac{3}{2}\right]
\end{eqnarray*}
If we were to equate $\bar{\mu}(\Omega)$ with the sum of the individual
probabilities, we would get that $\bar{\mu}(\Omega)=\left[\frac{1}{2},\frac{3}{2}\right]$.
However, using the fact that $\bar{\mu}(\emptyset)=\imposs$, we have
$\bar{\mu}\left(\Omega\right)=1-\bar{\mu}\left(\emptyset\right)=\necess=[1,1]$.
This interval is tighter and a better estimate for the probability
of the event $\Omega$, and of course it is contained in $[\frac{1}{2},\frac{3}{2}]$.
However it is only possible to exploit the information about the complement
when all four events are combined. Thus the $\subseteq$-condition
allows us to get an estimate for the combined event from each of its
constituents and then gather more evidence knowing the aggregate event.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Quantum Probability}

The mathematical framework of classical probability above assumes
that there exists a predetermined set of events that are independent
of the particular experiment â€” classical physics is non-contextual~\cite{kochenspecker1967,Redhead1987-REDINA,peres1995quantum,Jaeger2007}.
However, even in classical situations, the structure of the event
space is often only partially known and the precise dependence of
two events on each other cannot, a priori, be determined with certainty.
In the quantum framework, this partial knowledge is compounded by
the fact that there exist non-commuting events which cannot happen
simultaneously. To accommodate these more complex situations, conventional
approaches to quantum probability abandon the sample space~$\Omega$
and reason directly about events which are generalized from plain
sets to projection operators. A quantum probability space therefore
consists of just two components: a set of events $\qevents$ often
formalized as projection operators and a probability measure $\mu:\qevents\rightarrow[0,1]$
formalized using the Born rule~\cite{Born1983,Mermin2007,Jaeger2007}.

%%%%%


\subsubsection{Quantum Events}

\begin{definition}[Projection Operators; Orthogonality~\cite{10.2307/2308516,Redhead1987-REDINA,peres1995quantum,Griffiths2003,Swart2013}]\label{def:Projection}
Given a Hilbert space $\Hilb$, an event (an experimental proposition~\cite{BirkhoffVonNeumann1936},
a question~\cite{10.2307/2308516,Abramsky2012}, or an elementary
quantum test~\cite{peres1995quantum}) is represented as a (self-adjoint
or orthogonal~\cite{Griffiths2003,Maassen2010}) projection operator
$P:\Hilb\rightarrow\Hilb$ onto a linear subspace of $\Hilb$. The
following define projections and list some of their properties: 
\begin{itemize}
\item $\mathbb{0}$ is a projection. 
\item For any pure state~$\ket{\psi}$, $\proj{\psi}$ is a projection
operator. 
\item Projection operators $P_{0}$ and $P_{1}$ are \emph{orthogonal} if
$P_{0}P_{1}=P_{1}P_{0}=\mathbb{0}$. The sum of two projection operators~$P_{0}+P_{1}$
is also a projection operator if and only if they are orthogonal. 
\item Conversely, every projection~$P$ can be expressed as $\sum_{j=1}^{N}\proj{\psi_{j}}$,
where $P$ actually projects onto the linear subspace with orthonormal
basis~$\left\{ \ket{\psi_{j}}\right\} _{j=1}^{N}$. 
\item A set of projections $\left\{ P_{i}\right\} _{i=1}^{N}$ is called
an \emph{ideal measurement} if it is a partition of the identity,
i.e., $\sum_{i=1}^{N}P_{i}=\mathbb{1}$~\cite{Swart2013}. In this
case, projections $\left\{ P_{i}\right\} _{i=1}^{N}$ must be mutually
orthogonal~\cite{Griffiths2003,Halmos1957}, and $N$ must be less
or equal to the dimension of the Hilbert space. 
\item If $P$ is a projection operator, then $\mathbb{1}-P$ is also a projection
operator, called its \emph{complement}. It is orthogonal to $P$,
and corresponds to the complement event~$\Omega\backslash E$ in
classical probability~\cite{Griffiths2003}. 
\item Projection operators $P_{0}$ and $P_{1}$ \emph{commute} if $P_{0}P_{1}=P_{1}P_{0}$.
The product of two projection operators~$P_{0}P_{1}$ is also a projection
operator if and only if they commute. This corresponds to the classical
intersection between events~\cite{peres1995quantum,Griffiths2003}. 
\item For two commuting projection operators $P_{0}$ and $P_{1}$, their
\emph{disjunction}~$P_{0}\vee P_{1}$ is defined to be $P_{0}+P_{1}-P_{0}P_{1}$~\cite{Griffiths2003}. 
\end{itemize}
\end{definition}

\begin{example}[One-qubit quantum probability space]Consider a one-qubit
Hilbert space with each event interpreted as a possible post-measurement
state~\cite{peres1995quantum,Mermin2007,Jaeger2007}. For example,
the event $\proj{0}$ indicates that the post-measurement state will
be $\ket{0}$; the probability of such an event depends on the current
state; the event $\proj{1}$ indicates that the post-measurement state
will be $\ket{1}$; the event $\proj{\ps}$ where $\ket{\ps}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$
indicates that the post-measurement state will be $\ket{\ps}$; the
event $\mathbb{1}=\proj{0}+\proj{1}$ indicates that the post-measurement
state will be a linear combination of $\ket{0}$ and $\ket{1}$ and
is clearly certain; finally the empty event $\mathbb{0}$ states that
the post-measurement state will be the empty state and is impossible.
As in the classical case, a probability measure is a function that
maps events to $[0,1]$. Here is a partial specification of a possible
probability measure that would be induced by a system whose current
state is $\ket{0}$, $\mu\left(\mathbb{0}\right)=0$, $\mu\left(\mathbb{1}\right)=1$,
$\mu\left(\proj{0}\right)=1$, $\mu\left(\proj{1}\right)=0$, $\mu\left(\proj{\ps}\right)=1/2$,
\ldots{}. Note that, similarly to the classical case, the probability
of $\mathbb{1}$ is 1 and the probability of collections of orthogonal
events (e.g., $\proj{0}+\proj{1}$) is the sum of the individual probabilities.
A collection of non-orthogonal events (e.g., $\proj{0}$ and $\proj{\ps}$)
is however not even a valid event. In the classical example, we argued
that each probability measure is uniquely determined by two actual
coins. A similar (but much more subtle) argument is valid also in
the quantum case. By postulates of quantum mechanics and Gleason's
theorem, it turns out that for large enough quantum systems, each
probability measure is uniquely determined by an actual quantum state
as discussed next. \end{example}

%%%%%


\subsubsection{Quantum Probability Measures}

Given our setup, the definition of a quantum probability measure is
a small variation on the classical definition.

\begin{definition}[Quantum Probability Measure~\cite{10.2307/2308516,gleason1957,Redhead1987-REDINA,Maassen2010}]\label{def:QuantumProbabilitySpace}
Given a Hilbert space $\Hilb$ with its set of events~$\events$,
a \emph{quantum probability measure} is a function~$\mu:\events\rightarrow[0,1]$
such that: 
\begin{itemize}
\item $\mu(\mathbb{0})=0$. 
\item $\mu(\mathbb{1})=1$. 
\item For any projection $P$, $\mu\left(\mathbb{1}-P\right)=1-\mu\left(P\right)$. 
\item For a set of mutually orthogonal projections $\left\{ P_{i}\right\} _{i=1}^{N}$,
we have $\mu\left(\sum_{i=1}^{N}P_{i}\right)=\sum_{i=1}^{N}\mu\left(P_{i}\right)$. 
\end{itemize}
\end{definition}

\noindent A quantum probability measure can be easily constructed
if one knows the current state of the quantum system by using the
Born rule. Specifically, for each pure normalized quantum state $\ket{\phi}$,
the Born rule induces a probability measure $\mu_{\phi}^{B}$ defined
as $\mu_{\phi}^{B}(P)=\ip{\phi}{P\phi}$. The situation generalizes
to mixed states $\rho=\sum_{j=1}^{N}q_{j}\proj{\phi_{j}}$, where
$\sum_{j=1}^{N}q_{j}=1$ in which case the generalized Born rule induces
a probability measure $\mu_{\rho}^{B}$ defined as $\mu_{\rho}^{B}\left(P\right)=\Tr\left(\rho P\right)=\sum_{j=1}^{N}q_{j}\mu_{\phi_{j}}^{B}\left(P\right)$~\cite{peres1995quantum,544199,Jaeger2007}.
Conversely every probability measure must be of this form.

\begin{thm}[Gleason's theorem~\cite{gleason1957,Redhead1987-REDINA,peres1995quantum}]\label{cor:Gleason's}In
a Hilbert space $\Hilb$ of dimension $d\geq3$, given a quantum probability
measure~$\mu:\events\rightarrow[0,1]$, there exists a unique mixed
state~$\rho$ such that $\mu=\mu_{\rho}^{B}$. \end{thm}

%%%%%


\subsubsection{Measuring Quantum Probabilities}

Similarly to the classical case, it is possible to estimate quantum
probabilities by utilizing the frequentist approach of the previous
section, assuming identical measurements conditions in each repeated
experiment~\cite{peres1995quantum}. For instance, if one wants to
determine the probability that the spin of a given silver atom is
$+\hbar/2$, a Stern-Gerlach apparatus is built where ideally an inhomogeneous
magnetic field is generated along, let's say, the quantization axis
$z$. One then produces a collimated beam of identically prepared
(neutral) silver atoms that is directed between the poles of the magnet
where a predetermined field-gradient along the $z$ direction has
been established. Under appropriate experimental conditions we will
observe that the beam, after traversing the magnetic-field region,
will be deflected towards two regions identified by distinguished
spots on a detector situated behind the apparatus~\cite{Stern1988,peres1995quantum,544199,Griffiths2003}.
Each of the two discrete values is associated to either $+\hbar/2$
or $-\hbar/2$, commonly called ``spin up'' and ``spin down'',
respectively. By ``counting'' the number of atoms that are deflected
in the ``spin up'' region one can, in principle, estimate the probability
that the prepared state of the silver atom state has spin $+\hbar/2$.
Notice that a real experiment does not necessarily represent an ``ideal
measurement''. For example, not all silver atoms will be identically
prepared, or the field-gradient could not be large enough to distinguish
between the spin up and down situations simply producing a large single
blot. In other words, the closer we get to an ideal measurement the
better we determine those probabilities at the cost of significantly
increasing the number of resources. It is not very well appreciated
in the literature that Bohr attempted to argue against the measurability
of the spin of a free electron~\cite{Bohr1985,MartensDeMuynck1994,McEvoy2001}.
Essentially, Bohr argued (and Mott later on justified his assertion
by an elegant use of uncertainty relations~\cite{10.2307/j.ctt7ztxn5.15})
that a Stern-Gerlach experiment could not succeed in establishing
the spin of an unbound electron because the Lorentz force would blur
the detected pattern. This example illustrates the case of a fundamental
physical limitation that not even infinite resources could mitigate.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Quantum Interval-valued Probability Measures}

As argued in the previous sections, given fixed finite resources,
it is only possible to estimate the quantum probabilities within an
interval of confidence. It is therefore natural to propose the notion
of a ``quantum interval-valued probability measure'' that combines
the definitions of conventional quantum probability measures with
classical interval-probabilities.

\begin{definition}[Quantum Interval-valued Probability Measure]\label{def:QuantumInterval-valuedProbability}
Given a Hilbert space $\Hilb$ with quantum events (projections)~$\events$,
and a collection of intervals~$\mathscr{I}$, a \emph{quantum $\mathscr{I}$-interval-valued
probability measure} is a function~$\bar{\mu}:\events\rightarrow\mathscr{I}$
such that: 
\begin{itemize}
\item $\bar{\mu}(\mathbb{0})=\left[0,0\right]$. 
\item $\bar{\mu}(\mathbb{1})=\left[1,1\right]$. 
\item For any projection $P$, $\bar{\mu}\left(\mathbb{1}-P\right)=\left[1,1\right]-\bar{\mu}\left(P\right)$. 
\item For a set of mutually orthogonal projections $\left\{ P_{i}\right\} _{i=1}^{N}$,
we have $\bar{\mu}\left(\sum_{i=1}^{N}P_{i}\right)\subseteq\sum_{i=1}^{N}\bar{\mu}\left(P_{i}\right)$. 
\end{itemize}
\end{definition}

\noindent It is easy to establish that quantum interval-valued probability
measures generalize conventional quantum probability measures. In
particular, any quantum probability measure can be recast as an interval-valued
measure using the three intervals $\left[0,0\right]$, $\left[1,1\right]$
and \emph{$\left[0,1\right]$}, where $\left[0,0\right]$ and $\left[1,1\right]$
are called \imposs~and \necess~as before, and \emph{$\left[0,1\right]$}
is called \unknown~because it provides no information. Given a quantum
probability measure~$\mu:\events\rightarrow\left[0,1\right]$, we
define a quantum interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$
by $\bar{\mu}(P)=\iota\left(\mu(P)\right)$, where $\iota:\left[0,1\right]\rightarrow\mathscr{I}$
is defined by 
\[
\iota(x)=\begin{cases}
\necess & \textrm{if }x=1\textrm{ ;}\\
\imposs & \textrm{if }x=0\textrm{ ;}\\
\unknown & \textrm{otherwise.}
\end{cases}
\]
This measure represents the beliefs of an experimenter with no prior
knowledge about the particular quantum system in question. Formally,
we can ask: what can we deduce about the state of a quantum system
given a quantum interval-valued probability measure, i.e., given observations
done with finite resources. In the case the intervals are infinitely
precise the question reduces to Gleason's theorem which states that
the state of the quantum system is uniquely determined by the probability
measure. But surely the less resources are available, the less precise
the intervals, and the less we expect to know about the state of the
system. To formally state and answer this question we begin with defining
the \emph{core} and \emph{convexity} of a probability measure as follows.

\begin{definition} Given a quantum interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$: 
\begin{itemize}
\item A \emph{core} of $\bar{\mu}$ is the set $\mathrm{core}\left(\bar{\mu}\right)=\left\{ \pmeas:\events\rightarrow[0,1]~\middle|~\forall E\in\events.~\pmeas\left(E\right)\in\bar{\mu}\left(E\right)\right\} $. 
\item $\bar{\mu}$ is called convex if $\bar{\mu}\left(P_{0}\vee P_{1}\right)+\bar{\mu}\left(P_{0}P_{1}\right)\subseteq\bar{\mu}\left(P_{0}\right)+\bar{\mu}\left(P_{1}\right)$
for all commuting $P_{0},P_{1}\in\events$. 
\end{itemize}
\end{definition}

\noindent The core of an interval-valued probability measure is the
set of real-valued infinitely-precise probability measures it approximates.
An interval-valued probability measure is convex if whenever certain
intervals exist then combinations of these intervals must also exist,
guaranteeing we can add and manipulate probabilities coherently. In
the classical world, every convex measure has a non-empty core, which
means that the interval-valued probability measure must approximate
at least one infinitely-precise measure.

\begin{thm}[Shapley~\cite{Shapley1971,Grabisch2016}]\label{thm:Shapley}
Every (classical) convex interval-valued probability measure has a
non-empty core. \end{thm}

In informal terms, this result states that although measurements done
with limited resources and poor precision may not uniquely identify
the true state of the system, there is always at least one system
that is consistent with the measurements. Surprisingly, we discovered
a counterexample to this statement in the quantum case, i.e., we constructed
the following convex quantum interval-valued probability measure with
an empty core.

\begin{example}[Three-dimensional quantum three-interval-valued probability measure]\label{ex:three-dimensional-three-value}
Given a three dimensional Hilbert space with an orthonormal basis
$\left\{ \ket{0},\ket{1},\ket{2}\right\} $. Let\emph{ $\mathscr{I}=\left\{ \necess,\imposs,\unknown\right\} $},
$\ket{\ps}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$, $\ket{\ps'}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{2})$,
and 
\begin{eqnarray*}
\bar{\mu}(\mathbb{0})=\bar{\mu}(\proj{0})=\bar{\mu}(\proj{\ps})=\bar{\mu}(\proj{\ps'}) & = & \imposs\\
\bar{\mu}(\mathbb{1})=\bar{\mu}(\mathbb{1}-\proj{0})=\bar{\mu}(\mathbb{1}-\proj{\ps})=\bar{\mu}(\mathbb{1}-\proj{\ps'}) & = & \necess\\
\bar{\mu}(P) & = & \unknown\quad\textrm{otherwise}
\end{eqnarray*}
It is straightforward to verify that $\bar{\mu}$ is a convex quantum
interval-valued probability measure. Now assume there exists a real-valued
quantum probability measure $\mu$ such that $\mu(P)\in\bar{\mu}(P)$
for every event (projection)~$P$. We derive a contradiction as follows.
Suppose $\mu(P)\in\bar{\mu}(P)$ then we must have $\mu(\proj{0})=\mu(\proj{\ps})=\mu(\proj{\ps'})=0$.
By Gleason's theorem, there is a mixed state~$\rho=\sum_{j=1}^{N}q_{j}\proj{\phi_{j}}$
such that $\mu\left(P\right)=\sum_{j=1}^{N}q_{j}\ip{\phi_{j}}{P\phi_{j}}$,
where $\sum_{j=1}^{N}q_{j}=1$ and $q_{j}>0$. However, no pure state~$\ket{\phi}$
can satisfy $\ip{\phi}{0}=\ip{\phi}{\ps}=\ip{\phi}{\ps'}=0$; a contradiction.
\end{example}

\begin{figure}
\begin{centering}
\includegraphics[scale=0.38]{../proposal/measure4} 
\par\end{centering}
\caption{\label{fig:three-dimensional-4-value}This figure illustrates cases
1 and 2 of example~\ref{ex:three-dimensional-4-value} plotted in
$\mathbb{R}^{3}$. The red and green dotted vectors are $\ket{0}$
and $\ket{\ps}$ respectively. All possible real vectors of the subspaces
$\ket{0_{\theta,\gamma}^{\perp}}$ and $\ket{\ps_{\theta,\gamma}^{\perp}}$
are drawn in the red and green circles, respectively. Within the circles,
a dotted vector~$\ket{\psi}$ means $\bar{\mu}(\proj{\psi})=\likely$;
otherwise, $\bar{\mu}(\proj{\psi})=\unlikely$. The gray vector is
a generic vector~$\ket{0_{\theta,\gamma}^{\perp}}$, and the red
and green solid vectors are normalized $\ket{0}\times\ket{0_{\theta,\gamma}^{\perp}}$
and $\ket{\ps}\times\ket{0_{\theta,\gamma}^{\perp}}$, respectively,
where $\times$ is the usual cross product in $\mathbb{R}^{3}$.}
\end{figure}

Looking closely at the example above, we might think that the probability
measure $\bar{\mu}$ is induced by the state $\ket{2}$ because $\bar{\mu}(\proj{0})=\bar{\mu}(\proj{\ps})=\imposs$,
or we might think it is induced by the state $\ket{1}$ because $\bar{\mu}(\proj{0})=\bar{\mu}(\proj{\ps'})=\imposs$,
or yet we might think it is induced by the mixed state $\frac{\mathbb{1}}{3}$
because $\bar{\mu}(\proj{\phi})\ne\necess$ for all $\ket{\phi}$.
Each of these possibilities is compatible with some but not all of
the observations. All is not lost however: if the observations are
made more precise, we conjecture that some of the inconsistencies
disappear. A partial proof of this conjecture is the following example
which refines the previous probability measure by adding more precise
intervals. It remains to prove that this refined measure is convex,
however.

\begin{example}[Three-dimensional quantum four-interval-valued probability measure]\label{ex:three-dimensional-4-value}
Given a three dimensional Hilbert space with an orthonormal basis
$\left\{ \ket{0},\ket{1},\ket{2}\right\} $. Let \emph{ }$\mathscr{I}=\left\{ \imposs,\unlikely,\likely,\necess\right\} $,
$\ket{\ps}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$, and $\ket{\ms}=\frac{1}{\sqrt{2}}(\ket{0}-\ket{1})$.
The definition of $\bar{\mu}$ below refers to Fig.~\ref{fig:three-dimensional-4-value}
which plots the 1-dimensional projectors: 
\begin{enumerate}
\item Let: 
\begin{eqnarray*}
\bar{\mu}(\mathbb{0})=\bar{\mu}(\proj{0})=\bar{\mu}(\proj{\ps}) & = & \imposs\\
\bar{\mu}(\mathbb{1})=\bar{\mu}(\mathbb{1}-\proj{0})=\bar{\mu}(\mathbb{1}-\proj{\ps}) & = & \necess
\end{eqnarray*}
where $\ket{0}$ and $\ket{\ps}$ are plotted as the red and green
dotted vectors, respectively. 
\item The red and green circles are are the states orthogonal to $\ket{0}$
and $\ket{\ps}$, respectively, and can be parametrized as $\ket{0_{\theta,\gamma}^{\perp}}=\rme^{\rmi\gamma}\sin\theta\ket{1}+\cos\theta\ket{2}$
and $\ket{\ps_{\theta,\gamma}^{\perp}}=-\rme^{\rmi\gamma}\sin\theta\ket{\ms}+\cos\theta\ket{2}$,
where $0\le\theta\le\frac{\pi}{2}$ and $0\le\gamma<2\pi$. The dotted
half of those states need special treatment, i.e., whenever $0\le\theta<\frac{\pi}{2}$
and $0\le\gamma<\pi$, we define 
\begin{eqnarray*}
\bar{\mu}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)=\bar{\mu}\left(\proj{\ps_{\theta,\gamma}^{\perp}}\right) & = & \likely\\
\bar{\mu}\left(\mathbb{1}-\proj{0_{\theta,\gamma}^{\perp}}\right)=\bar{\mu}\left(\mathbb{1}-\proj{\ps_{\theta,\gamma}^{\perp}}\right) & = & \unlikely
\end{eqnarray*}
\item Otherwise, $\bar{\mu}(\proj{\psi})=\unlikely$ and $\bar{\mu}(\mathbb{1}-\proj{\psi})=\likely$. 
\end{enumerate}
It is straightforward but tedious to check that $\bar{\mu}$ is a
quantum interval-valued probability measure. Although we have not
verified that $\bar{\mu}$ is convex, the following argument establishes
that is has an empty core. Assume there is a real-valued probability
measure satisfying $\mu_{\rho}^{B}(P)\in\bar{\mu}(P)$ for all $P\in\events$.
Because $\mu_{\rho}^{B}(\proj{0})\in\bar{\mu}(\proj{0})=\imposs$
and $\mu_{\rho}^{B}(\proj{\ps})\in\bar{\mu}(\proj{\ps})=\imposs$,
we must have $\mu_{\rho}^{B}(\proj{0})=\mu_{\rho}^{B}(\proj{\ps})=0$
so that $\mu_{\rho}^{B}=\mu_{\ket{2}}^{B}$. However, 
\[
\mu_{\ket{2}}^{B}(\proj{2})=1\notin\unlikely=\bar{\mu}(\proj{2})\textrm{ .}
\]
This measure however is ``better'' than the previous one in the
sense that it might only be induced by $\ket{2}$ or the density matrix
$\frac{\mathbb{1}}{3}$, but not by $\ket{1}$. \end{example}

In general, we conjecture that if the measurement equipment is made
more and more precise, the corresponding interval-valued probability
measure will be closer and closer to the Born rule. In the limit case,
$\mathscr{I}=\left\{ \left\{ a\right\} ~\middle|~a\in\left[0,1\right]\right\} $
we do indeed recover the conventional Gleason's theorem and the Born
rule.

\section{About Myself}

\subsection{Occupation }

PhD Student

\subsection{Main area of work}

Quantum Mechanics and Computing over Finite Fields, Quantum Interval-valued
Probability

\subsection{Dietary requirements or allergies}

I donâ€™t eat shrimp.

\subsection{Whether you would like to apply for funding?}

I would like to apply for funding, but I havenâ€™t been to Europe beforeâ€¦
So I don't know the specific amount I needâ€¦

\begin{comment}
\bibliographystyle{plain}
\bibliography{../proposal/prop.bib}
\bibliography{cites1.bib}
\end{comment}
\printbibliography 
\end{document}
