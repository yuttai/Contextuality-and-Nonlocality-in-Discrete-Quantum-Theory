%% LyX 2.2.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{verbatim}
\usepackage{mathrsfs}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[ backend=biber,sorting=none,maxbibnames=5]{biblatex}
\addbibresource{discreteGBKS.bib}
\usepackage{fullpage}
\usepackage{url}
\usepackage{bbm}
\usepackage{bbold}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{array}
\usepackage{mathrsfs}
\usepackage[all]{xy}
\usepackage{wesa}


% \makeatletter
% \newtheoremstyle{indented}
%   {3pt}% space before
%   {3pt}% space after
%   {\addtolength{\@totalleftmargin}{3.5em}
%    \addtolength{\linewidth}{-3.5em}
%    \parshape 1 3.5em \linewidth}% body font
%   {}% indent
%   {\bfseries}% header font
%   {.}% punctuation
%   {.5em}% after theorem header
%   {}% header specification (empty for default)
% \makeatother
% \theoremstyle{indented}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{case}{Case}
\newtheorem{conjecture}{Conjecture}
\newcommand{\events}{\ensuremath{\mathcal{E}}}
\newcommand{\qevents}{\ensuremath{\mathcal{E}}}
\newcommand{\pmeas}{\ensuremath{\mu}}
\newcommand{\Hilb}{\mathcal{H}}
\newcommand{\ps}{\texttt{+}}
\newcommand{\ms}{\texttt{-}}
\newcommand{\poss}{{\mbox{\wesa{possible}}}}
\newcommand{\imposs}{{\mbox{\wesa{impossible}}}}
\newcommand{\likely}{{\mbox{\wesa{likely}}}}
\newcommand{\unlikely}{{\mbox{\wesa{unlikely}}}}
\newcommand{\necess}{{\mbox{\wesa{certain}}}}
\newcommand{\overflow}{{\mbox{\wesa{overflow}}}}
\newcommand{\unknown}{{\mbox{\wesa{unknown}}}}

\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\yutsung}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{purple}{Yu-Tsung says: #1}\end{minipage}}}
\newcommand{\amr}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{green}{Amr says: #1}\end{minipage}}}
\newcommand{\ffzd}[1]{{\mathbb{F}^{d\;*}_{#1}}}
\def\C{{\mathbb{C}}}
\newcommand{\ff}[1]{\mathbb{F}_{#1}}
\newcommand{\expect}[2]{ \langle #1 | #2 | #1 \rangle}
\newcommand{\Tr}{\mathop{\mathrm{Tr}}\nolimits}
\newcommand{\gerardo}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{OliveGreen}{Gerardo says: #1}\end{minipage}}}
\newcommand{\andy}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{blue}{Andy says: #1}\end{minipage}}}
\usepackage{tensor}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\rmi}{\mathrm{i}}
\usepackage[braket]{qcircuit} 
\newcommand{\xyR}[1]{\xymatrixrowsep={#1}}
\newcommand{\xyC}[1]{\xymatrixcolsep={#1}}
\newcommand{\ip}[2]{\ensuremath{\left\langle{#1}\middle\vert{#2}\right\rangle}}
\newcommand{\melem}[3]{\ensuremath{\left\langle{#1}\middle\vert{#2}\middle\vert{#3}\right\rangle}}
\newcommand{\op}[2]{\ensuremath{\left\vert{#1}\middle\rangle\middle\langle{#2}\right\vert}}
\newcommand{\pr}[2]{\ip{#1}{#2}}
\newcommand{\proj}[1]{\op{#1}{#1}}

\makeatother

\begin{document}

\title{Interval Probability and Measurement for Fuzzy Quantum Theories}

\date{\today}

\maketitle
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}

Fuzzy quantum mechanics: 
\begin{itemize}
\item \url{http://cds.cern.ch/record/518511/files/0107054.pdf} 
\item \url{http://link.springer.com/chapter/10.1007%2F978-3-642-35644-5_18#page-1} 
\item \url{http://link.springer.com/chapter/10.1007%2F978-3-540-93802-6_20#page-1} 
\item \url{http://www.du.edu/nsm/departments/mathematics/media/documents/preprints/m0412.pdf} 
\item \url{http://www.space-lab.ru/files/pages/PIRT_VII-XII/pages/text/PIRT_X/Bobola.pdf} 
\item \url{http://www.vub.ac.be/CLEA/aerts/publications/1993LiptovskyJan.pdf} 
\end{itemize}
\noindent Pseudo-randomness: 
\begin{itemize}
\item \url{https://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Pseudo%20Randomness/How_To_Generate_Cryptographically_Strong_Sequences_Of_Pseudo-Random_Bits.pdf}:
``the randomness of an event is relative to a specific model of computation
with a specified amount of computing resources.'' 
\item Another version \url{https://pdfs.semanticscholar.org/3e9c/5f6f48d9ef426655dc799e9b287d754e86c1.pdf} 
\end{itemize}
%%%%%


\subsection{Plan}

In the remainder of the paper, we consider variations of quantum probability
spaces motivated by computation of numerical quantities in a world
with limited resources: 
\begin{itemize}
\item Instead of the Hilbert space $\Hilb$ (constructed over the uncountable
and uncomputable complex numbers $\mathbb{C}$), we will consider
variants constructed over finite fields~\cite{DQT2014,geometry2013}. 
\item Instead of real-valued probability measures producing results in the
uncountable and uncomputable interval $[0,1]$, we will consider finite
set-valued probability measures~\cite{PuriRalescu1983}. 
\end{itemize}
We will then ask if it is possible to construct variants of quantum
probability spaces under these conditions. The main question is related
to the definition of probability measures: is it possible to still
define a probability measure as a function that depends on a single
state? Specifically, 
\begin{itemize}
\item given a state $\ket{\psi}$, is there a probability measure mapping
events to probabilities that only depends on $\ket{\psi}$? In the
conventional quantum probability space, the answer is yes by the Born
rule~\cite{Born1983,Mermin2007} and the map is given by: $P\mapsto\ip{\psi}{P\psi}$. 
\item given a probability measure $\mu$ mapping each event $P$ to a probability,
is there a \emph{unique} state $\psi$ such that $\mu(P)=\ip{\psi}{P\psi}$?
In the conventional case, the answer is yes by Gleason's theorem~\cite{gleason1957,peres1995quantum,Redhead1987-REDINA}. 
\end{itemize}
\andy{Quantum meeting: the basics of classical and quantum standard
and interval probabilities are becoming clearer (except need clearer
exposition). The key step is the replacement of \textquotedbl{}sum\textquotedbl{}
and \textquotedbl{}=\textquotedbl{} in the rule $\mu\left(\tensor*[_{\mathscr{O}}]{\sum}{_{i=0}^{N-1}}P_{i}\right)=\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}\mu\left(P_{i}\right)$
by various logical operations on sets instead of actually summing
unit norm partitions of unity in $\mathbb{R}$. What remains is to
determine how the Meyer/Mermin debate plays out for finite precision
or uncertain measurements of events, and its implication for the validity
of the Kochen-Specker theorem, and hence for the validity of Gleason's
theorem. Do we have anything ANALOGOUS to a Gleason theorem for interval
probability quantum mechanics? If so, what are the implications for
Kochen-Specker and Bell analogs for interval probabilities, and what
are the implications for the Meyer/Mermin debate? And if not, what
are the consequences? Finally, given whatever remains of Gleason/Kochen-Specker
for uncertain event measurements, what happens when we redefine \textquotedbl{}sum\textquotedbl{}
yet again for $\ff{p^{2}}$ valued quantum theories (and degenerate
states, and density matrices) to create a non-wrapping extension of
interval probability calculus to Galois fields? (The objective is
to put possible/impossible and its extensions on a solid footing for
DQC, and also to set up a seamless transition back to a continuous
CQC limit that is consistent, while incorporating such issues as the
cost of measurement precision.) }

\yutsung{A mathematical reason why Gleason's theorem should not be
valid in the interval-valued probability quantum mechanics. Every
classical probability measure has a one-to-one correspondence to its
Radon-Nikodym derivative. The Radon-Nikodym theorem extends to Gleason's
theorem in quantum, while the role of Radon-Nikodym derivative is
replaced by the density matrix. If there is no easy Radon-Nikodym
theorem for classical interval-valued probability measure, it is natural
that there is no easy Gleason's theorem for quantum interval-valued
probability measure...

If Gleason's theorem is not valid, a non-Born quantum probability
measure might exist in reality... If there exists a non-Born quantum
probability measure which cannot correspond to any state vectors,
then what's the post-measurement quantum probability measure of it
after measurement? If the updated post-measurement postulate is strange
enough, then commuting observables may not interchange their measurement
order... And commuting observables is compatible is the fundamental
assumption for the Kochen-Specker theorem, the Bell theorem, and everything\dots{}
If commuting observables becomes non-compatible, everything might
need to be rewritten\dots{} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Classical Probability Spaces}

A \emph{probability space} specifies the necessary conditions for
reasoning coherently about collections of uncertain events~\cite{Kolmogorov1950,Shafer1976,Griffiths2003,Swart2013}.
We review the conventional presentation of probability spaces and
then discuss the computational resources needed to estimate probabilities.

%%%


\subsection{Classical Real-Valued Probability Spaces}

The conventional definition of a probability space builds upon the
field of real numbers. In more detail, a probability space consists
of a \emph{sample space} $\Omega$, a space of \emph{events}~$\events$,
and a \emph{probability measure}~$\pmeas$ mapping events in $\events$
to the real interval $[0,1]$. In this paper, we will only consider
\emph{finite} sets of events: we therefore restrict our attention
to non-empty finite sets $\Omega$ as the sample space. The space
of events $\events$ includes every possible subset of $\Omega$:
it is the powerset~$2^{\Omega}=\left\{ E\middle|E\subseteq\Omega\right\} $. 

\begin{definition}[Probability Measure]\label{def:ClassicalProbabilitySpace}
Given the set of events $\events$, a \emph{probability measure} is
a function $\pmeas:\events\rightarrow[0,1]$ such that: 
\begin{itemize}
\item $\pmeas(\emptyset)=0$. 
\item $\pmeas(\Omega)=1$. 
\item For any event $E$, 
\begin{equation}
\pmeas\left(\Omega\backslash E\right)=1\tensor*[_{\mathbb{R}}]{-}{}\pmeas\left(E\right)\textrm{ ,}\label{eq:ClassicalReal-valuedProbability-Complement}
\end{equation}
where $\Omega\backslash E$ is the complement event, and $1\tensor*[_{\mathbb{R}}]{-}{}\pmeas\left(E\right)$
explicitly specifies $1$ and $\pmeas\left(E\right)\in\mathbb{R}$.
Besides $\mathbb{R}$, we will prepose other symbols later to specify
the type of operations, and they may be dropped when there is no ambiguity. 
\item For a collection $\left\{ E_{i}\right\} _{i=0}^{N-1}$ of pairwise
disjoint events, $\pmeas(\bigcup_{i=0}^{N-1}E_{i})=\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}\pmeas(E_{i})$.
Similar to the above condition, $\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}\pmeas(E_{i})$
explicitly specifies $\pmeas(E_{i})\in\mathbb{R}$. 
\end{itemize}
\qed\end{definition}

Notice that $\pmeas(\emptyset)=0$ and equation~(\ref{eq:ClassicalReal-valuedProbability-Complement})
can be induced by other conditions. They are listed in here so that
we can compare definition~\ref{def:ClassicalProbabilitySpace}, lemma~\ref{lem:classicalProbabilityMeasure},
definition~\ref{def:QuantumProbabilitySpace}, and definition~\ref{def:QuantumInterval-valuedProbability}
easily. We will compare these lemma and definitions when we formulate
quantum interval-valued probability measures later.

\begin{example}[Two-coins experiment]\label{ex1} Consider an
experiment that tosses two coins. We have four possible outcomes that
constitute the sample space $\Omega=\{HH,HT,TH,TT\}$. There are 16
total events including for example the event $\{HH,HT\}$ that the
first coin lands heads up, the event $\{HT,TH\}$ that the two coins
land on opposite sides, and the event $\{HT,TH,TT\}$ that at least
one coin lands tails up. Here is a possible probability measure for
these events: 
\[
\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) & = & 0\\
\pmeas(\{HH\}) & = & 1/3\\
\pmeas(\{HT\}) & = & 0\\
\pmeas(\{TH\}) & = & 2/3\\
\pmeas(\{TT\}) & = & 0\\
\pmeas(\{HH,HT\}) & = & 1/3\\
\pmeas(\{HH,TH\}) & = & 1\\
\pmeas(\{HH,TT\}) & = & 1/3
\end{array} & \begin{array}{rcl}
\pmeas(\{HT,TH\}) & = & 2/3\\
\pmeas(\{HT,TT\}) & = & 0\\
\pmeas(\{TH,TT\}) & = & 2/3\\
\pmeas(\{HH,HT,TH\}) & = & 1\\
\pmeas(\{HH,HT,TT\}) & = & 1/3\\
\pmeas(\{HH,TH,TT\}) & = & 1\\
\pmeas(\{HT,TH,TT\}) & = & 2/3\\
\pmeas(\{HH,HT,TH,TT\}) & = & 1
\end{array}\end{array}
\]
The assignment satisfies the two constraints for probability measures:
the probability of the entire sample space is 1, and the probability
of every collection of disjoint events (e.g., $\{HT\}\cup\{TH\}=\{HT,TH\}$)
is the sum of the individual probabilities. The probability of collections
of non-disjoint events (e.g., $\{HT,TH\}\cup\{TH,TT\}=\{HT,TH,TT\}$)
may add to something different than the probabilities of the individual
events. It is useful to think that this probability measure is completely
determined by the two coins in question and their characteristics,
in the sense that each pair of coins induces a measure, and each measure
must correspond to some pair of coins. The measure above is induced
by two coins such that the first coin is twice as likely to land tails
up than heads up and the second coin is double-headed. \qed\end{example}

Although specifying a probability measure for every event looks complex,
it can be simply constructed by 
\begin{equation}
\pmeas(E)=\sideset{_{\mathbb{R}}}{_{\omega\in E}}\sum f(\omega)\textrm{ ,}\label{eq:Radon-Nikodym}
\end{equation}
where $f:\Omega\rightarrow[0,1]$, $\sideset{_{\mathbb{R}}}{_{\omega\in\Omega}}\sum f(\omega)=1$,
and $f$ is called the Radon-Nikodym derivative of $\pmeas$ (with
respect to the counting measure)~\cite{Nikodym1930,Folland1999}.
The converse is also valid and called the Radon-Nikodym theorem.

\begin{thm}[Radon-Nikodym theorem for finite probability space~\cite{Kolmogorov1950,Swart2013}]
For every probability measure~$\pmeas$, there exists a unique Radon-Nikodym
derivative~$f$ such that equation~(\ref{eq:Radon-Nikodym}) holds.\qed\end{thm}

For example, the Radon-Nikodym derivative of $\pmeas$ in example~\ref{ex1}
is: 
\[
\begin{array}{rcl}
f\left(HH\right)=1/3,\quad f\left(HT\right)=0,\quad f\left(TH\right)=2/3,\quad f\left(TT\right)=0\textrm{ .}\end{array}
\]

In a strict computational or experimental setting, one may question
the reliance of the definition of probability space on the uncountable
and uncomputable real interval $[0,1]$. This interval includes numbers
like $0.h_{1}h_{2}h_{3}\ldots$ where $h_{i}$ is 1 or 0 depending
on whether Turing machine $M_{i}$ halts or not. Such numbers cannot
be computed. This interval also includes numbers like $\frac{\pi}{4}$
which can only be computed with increasingly large resources as the
precision increases. % \yutsung{Check the meaning of the ``computing $\frac{\pi}{4}$'', because
% Bailey Borwein Plouffe formula can computing the $n$th binary digit of $\pi$ 
% using base 16 directly.}
Therefore, in a resource-aware computational or experimental setting,
it is more appropriate to consider probability measures that map events
to a set of elements computable with a fixed set of resources. We
expand on this observation and then consider %% two approaches from the literature: set-valued probability
%% measures~\cite{Artstein1972,PuriRalescu1983} and 
interval-valued probability measures~\cite{Weichselberger2000,JamisonLodwick2004}
in detail.

%%%%%


\subsection{Measuring Probabilities: Buffon's Needle Problem\label{subsec:Measuring-Probabilities:-Buffon}}

In the previous section, the probability~$\pmeas(E)$ of each event~$E$
is known a priori. In reality, although each event is assumed to have
a probability, the exact value of $\pmeas(E)$ may not be known. If
we want to know its value, we can run $N$ independent trials. Let
$x_{i}=1$ or $0$ denote whether the event~$E$ occurs in the $i$-th
trial or not, respectively, then $\pmeas(E)$ could be approximated
to given accuracy~$\epsilon>0$ by the relative frequency~$\frac{1}{N}\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}x_{i}$
with the probability converging to one as $N$ goes to infinity, i.e.,
\[
\forall\epsilon>0,\lim_{N\rightarrow\infty}\pmeas\left(\left|\pmeas(E)-\frac{1}{N}\sideset{_{\mathbb{R}}}{_{i=0}^{N-1}}\sum x_{i}\right|<\epsilon\right)=1\textrm{ .}
\]
This fact is called the law of large numbers~\cite{Bernoulli2006,Kolmogorov1950,Uspensky1937,Shafer1976,544199}.

Suppose we drop a needle of length $\ell$ onto a floor made of equally
spaced parallel lines a distance $h$ apart, where $\ell<h$. It is
a known fact that the probability of the needle crossing a line is
$\frac{2\ell}{\pi h}$~\cite{Buffon1777,DeMorgan1872,Hall1873,Uspensky1937}.
We analyze this situation in the mathematical framework of probability
spaces paying special attention to the resources needed to estimate
the probability computationally or experimentally.

To formalize the experiment, we consider an experimental setup consisting
of a collection of $N$ identical needles of length $\ell$. We throw
the $N$ needles one needle at a time, and observe the number $M$
of needles that cross a line. The sample space can be expressed as
the set $\{X,-\}^{N}$ of sequences of characters of length $N$ where
each character is either $X$ to indicate a needle crossing a line
or $-$ to indicate a needle not crossing a line. If $N=3$, the probability
of the event that exactly 2 needles cross lines~$\{-XX,X{-}X,XX-\}$
can be estimated by the relative frequency~$\frac{2}{3}$. Generally,
the probability of the event that exactly $M$ needles out of the
$N$ total needles cross lines can be estimated by $\frac{M}{N}$.

In an actual experiment with $500$ needles and the ratio $\frac{\ell}{h}=0.75$~\cite{Hall1873},
it was found that $236$ crossed a line so the relative frequency
is $0.472$ whereas the idealized mathematical probability is $0.4774\ldots$.
In a larger experiment with $5000$ needles and the ratio $\frac{\ell}{h}=0.8$~\cite{Uspensky1937},
the relative frequency was calculated to be $0.5064$ whereas the
idealized mathematical probability is $0.5092\ldots$. We see that
the observed probability approaches $\frac{2\ell}{\pi h}$ but only
if \emph{larger and larger resources} are expended. These resource
considerations suggest that it is possible to replace the real interval
$[0,1]$ with rational numbers up to a certain precision related to
the particular experiment in question. There is clearly a connection
between the number of needles and the achievable precision: in the
hypothetical experiment with 3 needles, it is not sensible to retain
100 digits in the expansion of $\frac{2\ell}{\pi h}$.

There is however another more subtle assumption of unbounded computational
power in the experiment. We are assuming that we can always determine
with certainty whether a needle is crossing a line. But ``lines''
on the the floor have thickness, their distance apart is not exactly
$h$, and the needles' lengths are not all absolutely equal to $\ell$.
These perturbations make the events ``fuzzy.'' Thus, in an experiment
with limited resources, it is not possible to talk about the idealized
event that exactly $M$ needles cross lines as this would require
the most expensive needles built to the most precise accuracy, laser
precision for drawing lines on the floor, and the most powerful microscopes
to determine if a needle does cross a line. Instead we might talk
about the event that $M-\delta$ needles evidently cross lines and
$M+\delta'$ needles plausibly cross lines where $\delta$ and $\delta'$
are resource-dependent approximations. This fuzzy notion of events
leads to probabilities being only calculable within intervals of confidence
reflecting the certainty of events and their plausibility. This is
indeed consistent with published experiments: in an experiment with
$3204$ needles and the ratio $\frac{\ell}{h}=0.6$~\cite{DeMorgan1872},
$1213$ needles clearly crossed a line and $11$ needles were close
enough to plausibly be considered as crossing the line: we would express
the probability in this case as the interval $\left[\frac{1213}{3204},\frac{1224}{3204}\right]$
expressing that we are certain that the event has probability at least
$\frac{1213}{3204}$ but it is possible that it would have probability
$\frac{1224}{3204}$.

Recall that the relative frequency will approximate the probability
of an event if the event has a probability. What if the event doesn't
have infinity precise probability because of the experimental limit?
In this case, two sequences of independent copies of experimental
results can have their relative frequencies converge almost surely
to different limits~\cite{Marinacci1999,Teran2014}. In other words,
to get a better approximation of the probability, the quality of the
experimental equipment cannot be compensated by the number of independent
trials.

% %%%%%
% \subsection{Set-valued Probability Measures}
% Instead of using every point in the real interval $[0,1]$ as a
% potential value for a probability measure, we can lump points together
% in sets and consider probability measures up to set membership. The
% simplest such situation is to partition the interval $[0,1]$ into two
% sets: the set $\{0\}$ (which we will call $\imposs$) and the set
% containing all the points in the half-open interval $(0,1]$ (which we
% will call $\poss$). We will call the resulting collection
% $\left\{ \imposs,\poss\right\} $, the collection $\mathscr{L}_{2}$. To
% completely specify a probability space, we also need to specify how to
% ``add'' the probabilities in $\mathscr{L}_2$. Given two subsets of the
% real interval, $I_1$ and $I_2$, the general formula for adding them
% is:\footnote{Notice that the set-valued measure
%   in~\cite{Artstein1972,PuriRalescu1983}, only require
%   $I_{1}+I_{2}=\left\{ a_{1}+a_{2}\middle|a_{1}\in I_{1},a_{2}\in
%     I_{2}\right\} $
%   because they focused on measure theory. In probability theory,
%   $2\in\pmeas\left(E\right)$ doesn't really make sense so that we
%   consider intersect $[0,1]$ in our definition.}
% \[
% I_{1} \vee I_{2}=\left\{ x+y~\middle|~ x \in I_{1}, y \in I_{2}\right\} ~\cap~ [0,1]\textrm{ .}
% \]
% In the case of $\mathscr{L}_2$, this reduces to $x \vee y = \imposs$
% iff $x = y = \imposs$.

% \amr{
% From Puri and Ralescu's paper: Fix $n=1$ (only 1-dimension). Conditions on $\mu$:
% \begin{itemize}
% \item $\mu(E) \neq \emptyset$ for every $E \in \events$, 
% \item $1 \in \mu(\Omega)$, 
% \item for every disjoint family $A_i$ we have, $\mu(\bigcup_i A_i) =
%   \sum_i \mu(A_i)$
% \end{itemize}
% where the sum of subsets of $\mathbb{R}$ (not $\mathbb{R}^n$ as in the
% paper) is as follows. Let $X$ and $Y$ be subsets of $\mathbb{R}$, then
% $X + Y = \{ x + y ~|~ x \in X, y \in Y \}$
% }\\
% \yutsung{Actually, using $X+Y=\{x+y~|~x\in X,y\in Y\}$ directly
% might be more confusing than I thought... Because our example will
% be follows:

% For $\{0\}=\imposs$ and $(0,\infty)=\poss$, it will become
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \{0\}\\
% \pmeas(\{HH\}) & = & (0,\infty)\\
% \pmeas(\{HT\}) & = & \{0\}\\
% \pmeas(\{TH\}) & = & (0,\infty)\\
% \pmeas(\{TT\}) & = & \{0\}\\
% \pmeas(\{HH,HT\}) & = & (0,\infty)\\
% \pmeas(\{HH,TH\}) & = & (0,\infty)\\
% \pmeas(\{HH,TT\}) & = & (0,\infty)
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & (0,\infty)\\
% \pmeas(\{HT,TT\}) & = & \{0\}\\
% \pmeas(\{TH,TT\}) & = & (0,\infty)\\
% \pmeas(\{HH,HT,TH\}) & = & (0,\infty)\\
% \pmeas(\{HH,HT,TT\}) & = & (0,\infty)\\
% \pmeas(\{HH,TH,TT\}) & = & (0,\infty)\\
% \pmeas(\{HT,TH,TT\}) & = & (0,\infty)\\
% \pmeas(\{HH,HT,TH,TT\}) & = & (0,\infty)
% \end{array}\end{array}
% \]

% For $\{0\}=\imposs$ and $(0,1]=\poss$, it will become the following
% and gives an extra-value~$(0,2]$.
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \{0\}\\
% \pmeas(\{HH\}) & = & (0,1]\\
% \pmeas(\{HT\}) & = & \{0\}\\
% \pmeas(\{TH\}) & = & (0,1]\\
% \pmeas(\{TT\}) & = & \{0\}\\
% \pmeas(\{HH,HT\}) & = & (0,1]\\
% \pmeas(\{HH,TH\}) & = & (0,2]\\
% \pmeas(\{HH,TT\}) & = & (0,1]
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & (0,1]\\
% \pmeas(\{HT,TT\}) & = & \{0\}\\
% \pmeas(\{TH,TT\}) & = & (0,1]\\
% \pmeas(\{HH,HT,TH\}) & = & (0,2]\\
% \pmeas(\{HH,HT,TT\}) & = & (0,1]\\
% \pmeas(\{HH,TH,TT\}) & = & (0,2]\\
% \pmeas(\{HT,TH,TT\}) & = & (0,1]\\
% \pmeas(\{HH,HT,TH,TT\}) & = & (0,2]
% \end{array}\end{array}
% \]

% For $\{0\}=\imposs$, $[0,\frac{1}{2}]=\unlikely$, and $[\frac{1}{2},1]=\likely$,
% it will become the following and gives an extra-value~$[\frac{1}{2},\frac{3}{2}]$.
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \{0\}\\
% \pmeas(\{HH\}) & = & [0,\frac{1}{2}]\\
% \pmeas(\{HT\}) & = & \{0\}\\
% \pmeas(\{TH\}) & = & [\frac{1}{2},1]\\
% \pmeas(\{TT\}) & = & \{0\}\\
% \pmeas(\{HH,HT\}) & = & [0,\frac{1}{2}]\\
% \pmeas(\{HH,TH\}) & = & [\frac{1}{2},\frac{3}{2}]\\
% \pmeas(\{HH,TT\}) & = & [0,\frac{1}{2}]
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & [\frac{1}{2},1]\\
% \pmeas(\{HT,TT\}) & = & \{0\}\\
% \pmeas(\{TH,TT\}) & = & [\frac{1}{2},1]\\
% \pmeas(\{HH,HT,TH\}) & = & [\frac{1}{2},\frac{3}{2}]\\
% \pmeas(\{HH,HT,TT\}) & = & [0,\frac{1}{2}]\\
% \pmeas(\{HH,TH,TT\}) & = & [\frac{1}{2},\frac{3}{2}]\\
% \pmeas(\{HT,TH,TT\}) & = & [\frac{1}{2},1]\\
% \pmeas(\{HH,HT,TH,TT\}) & = & [\frac{1}{2},\frac{3}{2}]
% \end{array}\end{array}
% \]

% Actually, this approach is more natural to me when I think in measure
% theory, but I guess the readers might be confused if write in this
% way?}

% To summarize, an $\mathscr{L}$-valued probability is a function
% $\pmeas:\events\rightarrow\mathscr{L}$ such that:
% \begin{itemize}
% \item $1\in\pmeas(\Omega)$, and 
% \item for a collection $E_{i}$ of pairwise disjoint events, $\pmeas(\bigcup_{i}E_{i})=\bigvee_i\pmeas(E_{i})$. 
% \end{itemize}

% \begin{example}[Two-coin probability space with finite set-valued
% probability measure] \label{ex2} Under the new set-valued requirement,
% the probability measure in the first example becomes: 
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \imposs\\
% \pmeas(\{HH\}) & = & \poss\\
% \pmeas(\{HT\}) & = & \imposs\\
% \pmeas(\{TH\}) & = & \poss\\
% \pmeas(\{TT\}) & = & \imposs\\
% \pmeas(\{HH,HT\}) & = & \poss\\
% \pmeas(\{HH,TH\}) & = & \poss\\
% \pmeas(\{HH,TT\}) & = & \poss
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & \poss\\
% \pmeas(\{HT,TT\}) & = & \imposs\\
% \pmeas(\{TH,TT\}) & = & \poss\\
% \pmeas(\{HH,HT,TH\}) & = & \poss\\
% \pmeas(\{HH,HT,TT\}) & = & \poss\\
% \pmeas(\{HH,TH,TT\}) & = & \poss\\
% \pmeas(\{HT,TH,TT\}) & = & \poss\\
% \pmeas(\{HH,HT,TH,TT\}) & = & \poss
% \end{array}\end{array}
% \]
% Despite the fact that we have lost all numeric information, the probability
% measure still reveals that the second coin is double-headed. We have
% however lost the information regarding the bias in the first coin.
% This information can be recovered with a more refined probability
% measure as we show next. \qed\end{example}

% Although $\mathscr{L}_{2}$-valued probability measure is quite intuitive,
% set-valued probability measure is not that intuitive if we extend
% to more values. For example, consider the following three \emph{overlapping}
% closed sub-intervals: $[0,0]$ is still call \imposs, $[0,\frac{1}{2}]$
% which we call \unlikely, ad $[\frac{1}{2},1]$ which we call \likely.
% The following example is a set-valued probability measure corresponding
% to the probability measure in the first example.

% \begin{example}\label{Two-coin-set-valued-again}[Two-coin probability
% space with set-valued probability measure, again] 
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \imposs\\
% \pmeas(\{HH\}) & = & \unlikely\\
% \pmeas(\{HT\}) & = & \imposs\\
% \pmeas(\{TH\}) & = & \likely\\
% \pmeas(\{TT\}) & = & \imposs\\
% \pmeas(\{HH,HT\}) & = & \unlikely\\
% \pmeas(\{HH,TH\}) & = & \likely\\
% \pmeas(\{HH,TT\}) & = & \unlikely
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & \likely\\
% \pmeas(\{HT,TT\}) & = & \imposs\\
% \pmeas(\{TH,TT\}) & = & \likely\\
% \pmeas(\{HH,HT,TH\}) & = & \likely\\
% \pmeas(\{HH,HT,TT\}) & = & \unlikely\\
% \pmeas(\{HH,TH,TT\}) & = & \likely\\
% \pmeas(\{HT,TH,TT\}) & = & \likely\\
% \pmeas(\{HH,HT,TH,TT\}) & = & \likely
% \end{array}\end{array}
% \]
% In this example, we can get the information that the first coin is
% weighted and the second coin is double-headed. However, we may notice
% that the set-valued probability of the whole space~$\{HH,HT,TH,TT\}$
% are different between these two examples. This problem can be fixed
% when we moved on to the next section. \qed\end{example}

% We will return to finite set-valued probability measures in Sec.~\ref{sec:?}.

%%%%%


\subsection{Classical Interval-valued probability measures}

As motivated above, an event $E_{1}$ may have an interval of probability
$[l_{1},r_{1}]$. Assume that another disjoint event $E_{2}$ has
interval probability $[l_{2},r_{2}]$, what is the interval probability
of the event $E_{1}\cup E_{2}$? The answer is somewhat subtle: although
it is possible to use the sum of the intervals $[l_{1}+l_{2},r_{1}+r_{2}]$
as the combined probability, one can do find a much tighter interval
if information \emph{against} the event (i.e., information about the
complement event) is also taken into consideration. Formally, for
a general event $E$ with probability $[l,r]$, the evidence that
contradicts $E$ is an evidence supporting the complement of $E$.
The complement of $E$ must therefore have probability $\left[1-r,1-l\right]$
which we abbreviate $\left[1,1\right]\tensor*[_{\mathscr{I}}]{-}{}\left[l,r\right]$,
where the preposing $\mathscr{I}$ specifies we subtract intervals.
Given a collection of intervals $\mathscr{I}$, an $\mathscr{I}$\textendash interval-valued
probability measure is a function $\bar{\mu}:\events\rightarrow\mathscr{I}$
such that~\cite{JamisonLodwick2004}:\footnote{The right-end of the interval-valued probability measure is a special
case of (sub-additive) capacity~\cite{Choquet1954,Graf1980,Goodman1997}
or outer measure~\cite{Folland1999}. The left-end of the interval-valued
probability measure is a special case of (super-additive cooperative)
game~\cite{Shapley1971,Grabisch2016}.} 
\begin{itemize}
\item $\bar{\mu}(\emptyset)=[0,0]$. 
\item $\bar{\mu}(\Omega)=[1,1]$. 
\item For every collection of pairwise disjoint events $\left\{ E_{i}\right\} _{i=0}^{N-1},\left\{ E_{i}'\right\} _{i=0}^{N'-1}\subseteq\events$
with $\Omega=\left(\bigcup_{i=0}^{N-1}E_{i}\right)\bigcup\left(\bigcup_{i=0}^{N'-1}E_{i}'\right)$,
we have 
\[
\bar{\mu}\left(\bigcup_{i=0}^{N-1}E_{i}\right)\subseteq\left[\max\left\{ 1-\sideset{_{\mathbb{R}}}{_{i=0}^{N'-1}}\sum r_{i}',\sideset{_{\mathbb{R}}}{_{i=0}^{N-1}}\sum l_{i}\right\} ,\min\left\{ 1-\sideset{_{\mathbb{R}}}{_{i=0}^{N'-1}}\sum l_{i}',\sideset{_{\mathbb{R}}}{_{i=0}^{N-1}}\sum r_{i}\right\} \right]\textrm{ ,}
\]
where $\bar{\mu}\left(E_{i}\right)=[l_{i},r_{i}]$ and $\bar{\mu}\left(E_{i}'\right)=[l_{i}',r_{i}']$
for all $i$.
\end{itemize}
In order to understand this definition, we tear the last condition
apart and provide an equivalent definition of interval-valued probability
measures.

\begin{lemma}\label{lem:classicalProbabilityMeasure}Given a sample
space~$\Omega$ and its event~$\events$, a function~$\bar{\mu}:\events\rightarrow[0,1]$
is a classical interval-valued probability measure if and only if
$\bar{\mu}$ satisfies the following conditions: 
\begin{itemize}
\item $\bar{\mu}(\emptyset)=[0,0]$.
\item $\bar{\mu}(\Omega)=[1,1]$. 
\item For any event $E$, 
\begin{equation}
\bar{\mu}\left(\Omega\backslash E\right)=\left[1,1\right]\tensor*[_{\mathscr{I}}]{-}{}\bar{\mu}\left(E\right)\textrm{ .}\label{eq:ClassicalInterval-valuedProbability-Complement}
\end{equation}
\item For a collection $\left\{ E_{i}\right\} _{i=0}^{N-1}$ of pairwise
disjoint events, we have $\bar{\mu}\left(\bigcup_{i=0}^{N-1}E_{i}\right)\subseteq\tensor*[_{\mathscr{I}}]{\sum}{_{i=0}^{N-1}}\bar{\mu}\left(E_{i}\right)$,
where $\tensor*[_{\mathscr{I}}]{\sum}{_{i=0}^{N-1}}[l_{i},r_{i}]=\left[\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}l_{i},\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}r_{i}\right]$.
We may drop the preposing $\mathscr{I}$ when summands are clearly
intervals. 
\end{itemize}
\qed\end{lemma}

% The singleton closed interval $[0,0]$ is a required special value
% for empty set, and a natural generalization is to consider another
% special closed interval $[1,1]$ which we call \necess\ for the whole
% space. In general, if the probability of an event $E$ is $[a,b]$,
% we think of the left-endpoint~$a$ as representing the strength of
% the evidence that supports $E$, and the right-endpoint~$b$ as the
% strength of the evidence that contradicts $E$.

% Thus if we have an event $E$ with probability $[a,b]$ where $a=0.1$
% and $b=0.7$, we have that: 
% \begin{itemize}
% \item the strength of evidence supporting $E$ is 0.1; since either $E$
% or its complement must happen, we conclude that there is 0.9 evidence
% supporting the complement of $E$; 
% \item the strength of evidence contradicting $E$ is 0.7; again since either
% $E$ or its complement must happen, we conclude that there is 0.3
% evidence contradicting the complement of $E$. 
% \end{itemize}
%% \yutsung{Do we use the law of excluded middle here? You remind me
%% Agda : ) Did Homotopy Type Theory people said anything about the probability?}\\

% Turning things around, the strength of evidence that contradicts $E$
% is evidence supporting the complement of $E$. The complement of $E$
% must therefore have probability $\left[1-b,1-a\right]$ which we
% abbreviate $1-\left[a,b\right]$, so the probability measure $\bar{\mu}$
% should satisfy:
% \begin{itemize}
% \item $\bar{\mu}(\emptyset)=[0,0]$, 
% \item $\bar{\mu}(\Omega)=[1,1]$, and 
% \item $\bar{\mu}\left(\Omega\backslash E\right)=1-\bar{\mu}\left(E\right)$
% \end{itemize}
% However, if we want $\bar{\mu}(\Omega)=[1,1]$, and the probability assignment
% for singleton sets $\bar{\mu}(\{HH\})=\unlikely$, $\bar{\mu}(\{HT\})=\imposs$,
% $\bar{\mu}(\{TH\})=\likely$, and $\bar{\mu}(\{TT\})=\imposs$ as in example~\ref{Two-coin-set-valued-again},
% we only have $\bar{\mu}(\Omega)\subsetneq\bar{\mu}(\{HH\})+\bar{\mu}(\{HT\})+\bar{\mu}(\{TH\})+\bar{\mu}(\{TT\})$.
% In general, we only require
% \begin{itemize}
% \item for a collection $E_{i}$ of pairwise disjoint events, we have $\bar{\mu}\left(\right)\subseteq\sum_{i}\bar{\mu}\left(E_{i}\right)$. 
% \end{itemize}
% This statement means that the evidences of $$ is
% at least as strong as putting all the evidences of $E_{i}$ together,
% but some evidence may only be acquired for $$ as
% the whole. Therefore, $\bar{\mu}\left(\right)$ is a
% subset of $\sum_{i}\bar{\mu}\left(E_{i}\right)$, but may not equal.
% In our example, 
% \begin{eqnarray*}
%  &  & \bar{\mu}(\{HH\})+\bar{\mu}(\{HT\})+\bar{\mu}(\{TH\})+\bar{\mu}(\{TT\})\\
%  & = & \imposs+\unlikely+\imposs+\likely\\
%  & = & \left[0,0\right]+\left[0,\frac{1}{2}\right]+\left[0,0\right]+\left[\frac{1}{2},1\right]=\left[\frac{1}{2},\frac{3}{2}\right]
% \end{eqnarray*}
% The above equation told us $\bar{\mu}(\Omega)\subseteq\left[\frac{1}{2},\frac{3}{2}\right]$.
% However, because of $\bar{\mu}(\emptyset)=\imposs$, we have $\bar{\mu}\left(\Omega\right)=1-\bar{\mu}\left(\emptyset\right)=\necess$,
% but $\bar{\mu}(\emptyset)=\imposs$ cannot be used to reasoning the probability
% of $\{HH\}$, $\{HT\}$, $\{TH\}$ and $\{TT\}$ individually. The
% full probability assignment is shown in the following example. 

\noindent We will explain why the last condition is expressed using
$\subseteq$ by the following.

\begin{example}[Two-coin experiment with interval probability]
\label{ex3} We split the unit interval $[0,1]$ in the following
four closed sub-intervals: $[0,0]$ which we call \imposs, $[0,\frac{1}{2}]$
which we call \unlikely, $[\frac{1}{2},1]$ which we call \likely,
and $[1,1]$ which we call \necess. Using these new values, we can
modify the probability measure of Ex.~\ref{ex1} by mapping each
numeric value to the smallest sub-interval containing it to get the
following: 
\[
\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\bar{\mu}(\emptyset) & = & \imposs\\
\bar{\mu}(\{HH\}) & = & \unlikely\\
\bar{\mu}(\{HT\}) & = & \imposs\\
\bar{\mu}(\{TH\}) & = & \likely\\
\bar{\mu}(\{TT\}) & = & \imposs\\
\bar{\mu}(\{HH,HT\}) & = & \unlikely\\
\bar{\mu}(\{HH,TH\}) & = & \necess\\
\bar{\mu}(\{HH,TT\}) & = & \unlikely
\end{array} & \begin{array}{rcl}
\bar{\mu}(\{HT,TH\}) & = & \likely\\
\bar{\mu}(\{HT,TT\}) & = & \imposs\\
\bar{\mu}(\{TH,TT\}) & = & \likely\\
\bar{\mu}(\{HH,HT,TH\}) & = & \necess\\
\bar{\mu}(\{HH,HT,TT\}) & = & \unlikely\\
\bar{\mu}(\{HH,TH,TT\}) & = & \necess\\
\bar{\mu}(\{HT,TH,TT\}) & = & \likely\\
\bar{\mu}(\{HH,HT,TH,TT\}) & = & \necess
\end{array}\end{array}
\]
Despite the absence of any numeric information, the probability measure
is quite informative: it reveals that the second coin is double-headed
and that the first coin is biased. To understand the $\subseteq$-condition,
consider the following calculation: 
\begin{eqnarray*}
 &  & \bar{\mu}(\{HH\})+\bar{\mu}(\{HT\})+\bar{\mu}(\{TH\})+\bar{\mu}(\{TT\})\\
 & = & \imposs+\unlikely+\imposs+\likely\\
 & = & \left[0,0\right]+\left[0,\frac{1}{2}\right]+\left[0,0\right]+\left[\frac{1}{2},1\right]=\left[\frac{1}{2},\frac{3}{2}\right]
\end{eqnarray*}
If we were to equate $\bar{\mu}(\Omega)$ with the sum of the individual
probabilities, we would get that $\bar{\mu}(\Omega)=\left[\frac{1}{2},\frac{3}{2}\right]$.
However, using the fact that $\bar{\mu}(\emptyset)=\imposs$, we have
$\bar{\mu}\left(\Omega\right)=1-\bar{\mu}\left(\emptyset\right)=\necess=[1,1]$.
This interval is tighter and a better estimate for the probability
of the event $\Omega$, and of course it is contained in $[\frac{1}{2},\frac{3}{2}]$.
However it is only possible to exploit the information about the complement
when all four events are combined. Thus the $\subseteq$-condition
allows us to get an estimate for the combined event from each of its
constituents and then gather more evidence knowing the aggregate event.\qed\end{example}

In contrast to real-valued probability measures, there is no easy
Radon-Nikodym theorem to simplify an interval-valued probability measure
to its Radon-Nikodym derivative. So far, the interval-valued Radon-Nikodym
Theorems are either too restricted to apply to all interval-valued
probability measures~\cite{Choquet1954,Graf1980,Goodman1997} or
corresponding an interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$
to another set function~$\bar{f}:\events\rightarrow\bar{\mathscr{I}}$
which is not as simple as the original Radon-Nikodym derivative~\cite{GilboaSchmeidler1994,Goodman1997}.

Although there is no easy Radon-Nikodym theorem, an interval-valued
probability measure can still be understood by its relation with real-valued
probability measures. For example, the real-valued probability measure
in example~\ref{ex1} can construct the interval-valued probability
measure in example~\ref{ex3}. Conversely, we could understand the
interval-valued probability measure in example~\ref{ex3} by the
real-valued probability measure and its Radon-Nikodym derivative in
example~\ref{ex1}. The relation between example~\ref{ex1} and
\ref{ex3} can be abstracted into the definition of core. Besides
of core, we also define another property, convex, which will be used
later.

\begin{definition}~Given an interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$:
\begin{itemize}
\item A \emph{core}\footnote{The core is also called a structure~\cite{Weichselberger2000}.}
of $\bar{\mu}$ is the set $\mathrm{core}\left(\bar{\mu}\right)=\left\{ \textrm{probability measure }\pmeas:\events\rightarrow[0,1]\middle|\forall E\in\events.\pmeas\left(E\right)\in\bar{\mu}\left(E\right)\right\} $
\cite{Shapley1971,GilboaSchmeidler1994,Marinacci1999,Teran2014}.
\item $\bar{\mu}$ is called \emph{convex}, \emph{2-monotone}, or\emph{
supermodular} if $\bar{\mu}\left(E_{0}\cup E_{1}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(E_{0}\cap E_{1}\right)\subseteq\bar{\mu}\left(E_{0}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(E_{1}\right)$
for all $E_{0},E_{1}\in\events$~\cite{Shapley1971,GilboaSchmeidler1994,Marinacci1999,Teran2014,Grabisch2016}.\footnote{\yutsung{In order to simplify the discussion, let $l$ be the left-end
of an interval-valued probability measure~$\bar{\mu}$. Because of
equation~(\ref{eq:ClassicalInterval-valuedProbability-Complement}),
$\bar{\mu}$ is convex if and only if 
\begin{equation}
l\left(E_{0}\cup E_{1}\right)\tensor*[_{\mathbb{R}}]{+}{}l\left(E_{0}\cap E_{1}\right)\ge l\left(E_{0}\right)\tensor*[_{\mathbb{R}}]{+}{}l\left(E_{1}\right)\textrm{ ,}\label{eq:convex-game}
\end{equation}
which is a special case of the Dempster-Shafer condition~\cite{Shafer1976,Grabisch2016}
\begin{equation}
l\left(\bigcup_{i=0}^{N-1}E_{i}\right)\ge\sideset{_{\mathbb{R}}}{_{I\subseteq\left\{ 0,\ldots,N-1\right\} }}\sum\left(-1\right)^{\left|I\right|+1}l\left(\bigcap_{i\in I}E_{i}\right)\textrm{ .}\label{eq:Dempster-Shafer-game}
\end{equation}
Therefore, in order to understand the meaning of equation~(\ref{eq:convex-game}),
we could try to understand meaning of the Dempster-Shafer belief function.
Unfortunately, when people introduce the Dempster-Shafer theory, they
usually don't start from equation~(\ref{eq:Dempster-Shafer-game}).
Instead, they introduce a basic belief assignment~$m\left(E\right)=\tensor*[_{\mathbb{R}}]{\sum}{_{E'\subseteq E}}\left(-1\right)^{\left|E\setminus E'\right|}l\left(E'\right)$
first. $m\left(E\right)$ represents the belief committed to the event~$E$
and only to it, in the sense that it could not be committed to any
proper subset of $E$. Then, equation~(\ref{eq:Dempster-Shafer-game})
is equivalent to $m\left(E\right)\ge0$ for all $E$ which is intuitive.

Comparing to the Dempster-Shafer condition~(\ref{eq:Dempster-Shafer-game}),
the equivalent condition for convexity is more complex. At least,
when $l$ is convex, we have 
\begin{equation}
m\left(\left\{ \omega_{0},\omega_{1}\right\} \right)\ge0\label{eq:convex-positive}
\end{equation}
 for all $\left\{ \omega_{0},\omega_{1}\right\} \subseteq\Omega$,
although equation~(\ref{eq:convex-positive}) is not strong enough
to imply $l$ is convex.}}
\end{itemize}
\qed\end{definition}

It is common to consider convex interval-valued probability measures.
For example, a special case of convex interval-valued probability
measures consists the Dempster-Shafer belief function as the left-end
and the Dempster-Shafer plausibility as the right-end~\cite{Dempster1967,Shafer1976,GilboaSchmeidler1994,Goodman1997,Marinacci1999,Teran2014}.
Dempster-Shafer functions, also called \emph{completely} or \emph{totally
monotone}, plays an important role in the theory of non-additive belief.
Furthermore, convex interval-valued probability measures have a good
property that they can always be associated with real-valued probability
measures.

\begin{thm}[Shapley~\cite{Shapley1971}]\label{thm:Shapley}
Every convex interval-valued probability measure has a nonempty core.\qed\end{thm}

For general interval-valued probability measures, the following theorem
gives an algorithmic way to decide whether it has a non-empty core. 

\begin{thm}[Bondareva-Shapley~\cite{Shapley1967,Bondareva1968}]\label{thm:Bondareva-Shapley}
Let $l$ be the left-end of an interval-valued probability measure~$\bar{\mu}$,
$\bar{\mu}$ has a non-empty core if and only if for any minimal balanced
sets~$\left\{ E_{i}\right\} _{i=0}^{N-1}$, we have $\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}\gamma_{i}l\left(E_{i}\right)\le1$,
where $\left\{ E_{i}\right\} _{i=0}^{N-1}$ is called minimal balanced
sets if they are distinct, $\emptyset\ne E_{i}\subsetneq\Omega$,
and there exist a unique positive coefficient~$\left\{ \gamma_{i}\right\} _{i=0}^{N-1}$
such that $\tensor*[_{\mathbb{R}}]{\sum}{_{i=0\textrm{ and }\omega\in E_{i}}^{N-1}}\gamma_{i}=1$
for all $\omega\in\Omega$.\qed\end{thm}

The Bondareva-Shapley theorem can be used to algorithmically decide
non-empty core because the minimal balanced sets can be generated
by an algorithm~\cite{Peleg1965,Grabisch2016}. However, the number
of minimal balanced sets grows very rapidly with $\left|\Omega\right|$,
the size of $\Omega$~\cite{Shapley1967,Zhao2008}\footnote{\yutsung{Prove the number of minimal balanced sets grow exponentially
or more?}}, so that we have only verified every interval-valued probability
measure has a non-empty core when $\left|\Omega\right|\le5$ so far.\footnote{\yutsung{The program for $\left|\Omega\right|=6$ blows up the memory
again. Hence, currently I only run a extremely slow backtrack program
searching for a IVPM with an empty core...

Although the computer simulation supports the idea that every IVPM
has a nonempty core, if we look the equations, $\bar{\mu}$ is an
interval-valued probability measure if and only if its left-end~$l$
satisfies equation~(\ref{eq:convex-game}) when $E_{0}\cap E_{1}=\emptyset$
or $E_{0}\cup E_{1}=\Omega$. This condition is much weaker than convex,
and I will be surprise if this condition is strong enough to imply
a nonempty core.}} Based on the previous fact, we have the following conjecture.

\begin{conjecture}Every interval-valued probability measure has a
nonempty core.\qed\end{conjecture}

% \gerardo{Set-valued is a particular case?}

% \yutsung{Set-valued is not a particular case of interval-valued probability
% because their rules are different. A set-valued probability measure
% should satisfy: 
% \begin{enumerate}
% \item $1\in\bar{\mu}(\Omega)$, and 
% \item for a collection $E_{i}$ of pairwise disjoint events, $\bar{\mu}(\bigcup_{i}E_{i})=\sum_{i}\bar{\mu}(E_{i})$. 
% \end{enumerate}
% which are completely opposite to the interval-valued probability measures. 

% It seems hard to make a consistent story with the general set-valued
% probability measure framework (maybe except possible/impossible?),
% so we just comment out set-valued probability measure here...}

% to think of the above combination in the context of the evidence
% against the events established by the dual events. Writing in terms of
% the explicit intervals we see:
% \[\begin{array}{rcl@{\qquad}rcl}
% \bar{\mu}(\{HH\}) & = & [0,\frac{1}{2}] & \bar{\mu}(\{HT\}) & = & [0,0] \\
% \bar{\mu}(\{TH\}) & = & [\frac{1}{2},1] &  \bar{\mu}(\{TT\}) & = & [0,0]
% \end{array}\]
% The probability for $\bar{\mu}(\{HH,TH\})$ is $[X,Y]$ where $X$ comes
% from the positive evidence for $\{HH\}$ and $\{TH\}$ and from the
% evidence against  is $[X,Y]$ where $X$ comes
% \[
% [\max(1,\frac{1}{2}),\min(1,\frac{3}{2})]
% \]

% must be at least as strong as the
% evidence for $\{HH\}$ and $\{TH\}$ individually and hence we have the
% tentative bounds on $\bar{\mu}(\{HH,TH\})$ as
% $[\frac{1}{2},\frac{3}{2}]$. The dual events, however, establish
% evidence against $\{HH,TH\}$ with bounds $[

% A=HH
% B=TH
% C=HT
% D=TT

% To calculate the probability of $\{HH\}\cup\{TH\}$ we gather 
% evidence not only from the events $\{HH\}$ and $\{TH\}$, but also from 
% the dual events $\{HT,TH,TT\}$ and $\{HH,HT,TT\}$. The minimal
% probability for $\{HH\}\cup\{TH\}$ is\max(

% To understand this mystery, we look at
% another more general example and then give the formal mathematical
% definition of interval-valued probabilities. 

% \begin{example}[Dempster-Shafer Theory of Evidence] We have three
%   employees whose precise ages $A_1$, $A_2$, and $A_3$ are not
%   known. All is given is a range of ages for each employee:
%   $A_1 \in \{ 23,24 \}$, $A_2 \in \{ 20,21,22\}$, and
%   $A_3 \in \{ 21,22 \}$. The sample space in this case is
%   $A_1 \times A_2 \times A_3$ which represents all the possible
%   combinations of ages for the three employees:
% \[\begin{array}{rcl}
% \Omega &=& \{ 
%         (23,20,21), (23,20,22), (23,21,21), (23,21,22), (23,22,21), (23,22,22), \\
% && ~(24,20,21), (24,20,22), (24,21,21), (24,21,22), (24,22,21), (24,22,22) \}
% \end{array}\]
% Subsets of $\Omega$ represent events as usual. Consider the event
% $\Omega$ that \emph{some} employee's age is in the range
% $\{20,21,22\}$: since that event covers the entire sample space its
% probability must be 1. We can however produce a more informative
% answer by reasoning as follows: it is \emph{impossible} for the first
% employee's age to be in the range $\{20,21,22\}$; it is \emph{certain}
% that the second employee's age is in that range; and it is
% \emph{possible} that the third's employee age is in that
% range. Aggregating the results, we see that it is \emph{necessary} for
% one out of three employees to have their age in the required range,
% and it is \emph{possible} for an additional employee to have their age
% in the required range. We summarize this information by reporting that
% the probability of this event is $[\frac{1}{3},\frac{2}{3}]$ where the
% first number reports the \emph{certainty} of the event and the second
% reports the \emph{possibility} of the event. We could also have
% reasoned about the dual event that \emph{no} employee's age is in the
% range $\{20,21,22\}$ to get the probability $[\frac{1}{3},\frac{2}{3}]$

%  dually
% about the evidence \emph{against} the event. The probability in this
% case 

% Now consider the event
% that some employee's age is in the range $\{23,24\}$. Reasoning as
% above, the probability of this event is $[\frac{1}{3},\frac{1}{3}]$ as
% only the first employee qualifies. Clearly if we were to ask the
% probability that some employee's age is in the range
% $\{20,21,22,23,24\}$ the result should be $[1,1]$ as this range covers
% all the possibilities. The way to calculate this result from the two
% previous ones is as follows: the evidence for the combined event to be
% necessary is at least as strong as the evidence that each disjoint
% event that contributes to it is necessary. So the lower bound
% probability for the combined event is at least
% $\frac{1}{3}$. Similarly the upper bound probability is at least
% $1$. But now we can reason using the complements of the events about
% the necessity and possibility of refuting each event. For the event
% that some employee's age is \emph{not} in the range $\{20,21,22\}$ we
% have a probability $[\frac{1}{3},\frac{1}{3}]$ because it is necessary
% that $A_1$ is not in that range. Similarly, the event that some employee's age is
% \emph{not} in the range $\{23,24\}$ is
% $[\frac{2}{3},\frac{2}{3}]$. Thus although the positive evidence for
% necessity 

% \qed\end{example}

% \newpage

% \begin{example}[Dempster-Shafer Theory of Evidence] We have five
%   employees whose precise age is not known. All is given is a range for
%   each employee:
% \[\begin{array}{rcl}
% \textrm{Age}(M_{1}) & \in & \{ 23,24 \}=D_{1}\\
% \textrm{Age}(M_{2}) & \in & \{ 20,21,22\}=D_{2}\\
% \textrm{Age}(M_{3}) & \in & \{ 20,21 \}=D_{3}
% \end{array}\]
% What is the probability that an employee's age is in the range
% $\{22,23,24\}$? Looking at the data, it is \emph{possible} for $M_2$'s age
% to be within the range, it is \emph{not possible} for $M_3$'s age to
% be within the range, and it is \emph{certain} or \emph{necessary} that
% $M_1$'s age is in the range. Clearly saying that an event is
% \emph{necessary} is equivalent to saying that its complement is
% \emph{not possible}. Aggregating the results for the three employees,
% we can calculate that is necessary that 1 employees have their ages
% within the range, and it is possible that 2 employees have their ages
% within the range. We express this formally as saying that the
% probability of the event is $\left[\frac{1}{3},\frac{2}{3}\right]$. Now consider
% another event asking whether the ages are some other disjoint range
% $\{20,21\}$. Reasoning in a similar way we calculate that the
% probability for this event is $\left[\frac{1}{3},\frac{2}{3}\right]$. Now let's
% take the two events together and ask about the possibility of the ages to be in the range
% $\{20,21,22,23,24\}$. Clearly that probability must be $[1,1]$ as every
% employee's age is in that range. This problem
% looks puzzle if we want to attack it directly, but will be more clear
% if we think in terms of conditional probability. We will compute the
% conditional probability for the usual real-valued probability first
% and for the interval-valued probability later.
% \begin{itemize}
% \item We randomly draw an employee among the three, so each employee has
% probability $\frac{1}{3}$ to be drawn, i.e.,
% \[
% \bar{\mu}\left(\left\{ i=1\right\} \right)=\bar{\mu}\left(\left\{ i=2\right\} \right)=\bar{\mu}\left(\left\{ i=3\right\} \right)=\frac{1}{3}
% \]
% In order to compute the usual real-valued probability, we need a probability
% distribution within the possible range $D_{1}$, $D_{2}$, and $D_{3}$.
% Let's assume they are equally probable, i.e., 
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{1})=23\right\} \right) & = & \frac{1}{2}\\
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{1})=24\right\} \right) & = & \frac{1}{2}
% \end{array} & \begin{array}{rcl}
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{2})=20\right\} \right) & = & \frac{1}{3}\\
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{2})=21\right\} \right) & = & \frac{1}{3}\\
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{2})=22\right\} \right) & = & \frac{1}{3}
% \end{array}\end{array}\begin{array}{rcl}
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{3})=20\right\} \right) & = & \frac{1}{2}\\
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{3})=21\right\} \right) & = & \frac{1}{2}
% \end{array}
% \]
% Then, the probability that an employee's age is in the range $\{22,23,24\}$
% can be computed as follow:
% \begin{eqnarray*}
%  &  & \bar{\mu}\left(\left\{ i\middle|\textrm{Age}(M_{i})\in\{22,23,24\}\right\} \right)\\
%  & = & \bar{\mu}\left(\left\{ i=1\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{1})\in\{22,23,24\}\right\} \right)\\
%  &  & +\bar{\mu}\left(\left\{ i=2\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{2})\in\{22,23,24\}\right\} \right)\\
%  &  & +\bar{\mu}\left(\left\{ i=3\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{3})\in\{22,23,24\}\right\} \right)\\
%  & = & \frac{1}{3}\cdot1+\frac{1}{3}\cdot\frac{1}{3}+\frac{1}{3}\cdot0=\frac{4}{9}
% \end{eqnarray*}
% \item Assume we don't know the probability distributions within the possible
% range $D_{1}$, $D_{2}$, and $D_{3}$. All we know is whether they
% are \emph{possible} or \emph{necessary}. Then we replace the above
% computation from exact value to interval and consider
% \begin{eqnarray*}
%  &  & \bar{\mu}\left(\left\{ i\middle|\textrm{Age}(M_{i})\in\{22,23,24\}\right\} \right)\\
%  & = & \bar{\mu}\left(\left\{ i=1\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{1})\in\{22,23,24\}\right\} \right)\\
%  &  & +\bar{\mu}\left(\left\{ i=2\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{2})\in\{22,23,24\}\right\} \right)\\
%  &  & +\bar{\mu}\left(\left\{ i=3\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{3})\in\{22,23,24\}\right\} \right)\\
%  & = & \frac{1}{3}\cdot[1,1]+\frac{1}{3}\cdot[0,1]+\frac{1}{3}\cdot[0,0]=\left[\frac{1}{3},\frac{2}{3}\right]
% \end{eqnarray*}
% \yutsung{Check the conditional probability rule in the Dempster-Shafer
% Theory!}
% \end{itemize}
% \qed\end{example}

% In interval-valued probability measures, if the probability of an
% event $E$ is $[a,b]$, we think of the left-endpoint~$a$ as
% representing the strength of the evidence that supports $E$, and the
% right-endpoint~$b$ as the strength of the evidence that contradicts
% $E$. 

% Thus if we have an event $E$ with probability $[a,b]$ where
% $a=0.1$ and $b=0.7$, we have that:
% \begin{itemize}
% \item the strength of evidence supporting $E$ is 0.1; since either $E$
%   or its complement must happen, we conclude that there is 0.9
%   evidence supporting the complement of $E$; 
% \item the strength of evidence contradicting $E$ is 0.7; again since
%   either $E$ or its complement must happen, we conclude that there is
%   0.3 evidence contradicting the complement of $E$.
% \end{itemize}

% \yutsung{Do we use the law of excluded middle here? You remind me Agda : ) Did
% Homotopy Type Theory people said anything about the probability?}

% Turning things around, the strength of evidence that contradicts
% $E$ is evidence supporting the complement of $E$. The complement of $E$ must
% therefore have probability $\left[1-b,1-a\right]$ which we abbreviate $1-\left[a,b\right]$:

% \begin{eqnarray*}
% \bar{\mu}\left(\emptyset\right) & = & \imposs\\
% \bar{\mu}\left(\Omega\right) & = & \necess\\
% \bar{\mu}\left(\Omega\backslash E\right) & = & 1-\bar{\mu}\left(E\right)
% \end{eqnarray*}
% Next, if we define $\sum_{i}\left[a_{i},b_{i}\right]=\left[\sum_{i}a_{i},\sum_{i}b_{i}\right]$,
% then 
% \begin{itemize}
% \item for a collection $E_{i}$ of pairwise disjoint events, we have $\bar{\mu}\left(\bigcup_{i}E_{i}\right)\subseteq\sum_{i}\bar{\mu}\left(E_{i}\right)$. 
% \end{itemize}
% Notice that the equality may not hold in general. This statement says
% that the evidences of $\bigcup_{i}E_{i}$ is at least as strong as
% putting all the evidences of $E_{i}$ together, but some evidence
% may only be acquired for $\bigcup_{i}E_{i}$ as the whole. Therefore,
% $\bar{\mu}\left(\bigcup_{i}E_{i}\right)$ is a subset of $\sum_{i}\bar{\mu}\left(E_{i}\right)$,
% but may not equal. In our example, 
% \begin{eqnarray*}
%  &  & \bar{\mu}(\{HH,TH\})=\necess=[1,1]\\
%  & \subseteq & \left[\frac{1}{2},\frac{3}{2}\right]=\left[0,\frac{1}{2}\right]+\left[\frac{1}{2},1\right]=\unlikely+\likely=\bar{\mu}(\{HH\})+\bar{\mu}(\{TH\})\textrm{ .}
% \end{eqnarray*}
% However, $\bar{\mu}(\{HH,TH\})$ is a proper subset of $\bar{\mu}(\{HH\})+\bar{\mu}(\{TH\})$
% because if we check the complement of $\{HH,TH\}$, we have 
% \begin{eqnarray*}
%  &  & \bar{\mu}(\{HH,TH\})=1-\bar{\mu}(\{HT,TT\})=1-\imposs=\necess\textrm{ ,}
% \end{eqnarray*}
% and $\bar{\mu}(\{HT,TT\})=\imposs$ cannot be used to reasoning the probability
% of $\{HH\}$ and $\{TH\}$ individually. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{\label{sec:Quantum-Probability-Spaces}Quantum Probability Spaces}

The mathematical framework above assumes that there exists a predetermined
set of events that are independent of the particular experiment. However,
in many practical situations, the structure of the event space is
only partially known and the precise dependence of two events on each
other cannot, a priori, be determined with certainty. In the quantum
framework, this partial knowledge is compounded by the fact that there
exist non-commuting events which cannot happen simultaneously. To
accommodate these more complex situations, we abandon the sample space~$\Omega$
and reason directly about events. A quantum probability space therefore
consists of just two components: a set of events $\qevents$ and a
probability measure $\mu:\qevents\rightarrow[0,1]$. To properly explain
the quantum probability, we will first discuss projection operators
as quantum events, provide an easy example, define quantum probability
measures, and extend to quantum interval-valued probability.

\subsection{Quantum Events}

\begin{definition}[Projection Operators; Orthogonality~\cite{10.2307/2308516,Redhead1987-REDINA,peres1995quantum,Griffiths2003,Swart2013}]
\label{def:Projection} Given a Hilbert space $\Hilb$, an event\footnote{An event is formally called an experimental proposition~\cite{BirkhoffVonNeumann1936},
a question~\cite{10.2307/2308516,DBLP:journals/corr/abs-0910-2393},
or an elementary quantum test~\cite{peres1995quantum}.} mathematically is represented as a projection operator $P:\Hilb\rightarrow\Hilb$
onto a linear subspace~$S$ of $\Hilb$. The set of all events~$\events$
can be defined recursively as follow: \footnote{``Projection'' is sometimes called ``orthogonal projection\textquotedbl{}
or ``self-adjoint projection\textquotedbl{} to emphasize $P^{\dagger}=P$~\cite{Griffiths2003,Maassen2010}.} 
\begin{itemize}
\item $\mathbb{0}$ is a projection. 
\item For any pure state~$\ket{\psi}$, $\proj{\psi}$ is a projection
operator. 
\item Projection operators $P_{0}$ and $P_{1}$ are \emph{orthogonal} if
$P_{0}P_{1}=P_{1}P_{0}=\mathbb{0}$. The sum of two projection operators~$P_{0}\tensor*[_{\mathscr{O}}]{+}{}P_{1}$
is also a projection operator if and only if they are orthogonal,
where the preposing subscript $\mathscr{O}$ means $\tensor*[_{\mathscr{O}}]{+}{}$
is an operation between operators. 
\item Conversely, every projection~$P$ can be expressed as $\tensor*[_{\mathscr{O}}]{\sum}{_{j=0}^{N-1}}\proj{\psi_{j}}$,
where $P$ actually projects onto the linear subspace~$S$ with an
orthonormal basis~$\left\{ \ket{\psi_{j}}\right\} _{j=0}^{N-1}$. 
\end{itemize}
\qed\end{definition}

Because quantum events are projection operators, operations and properties
of quantum events can be written in terms of those of operators.

\begin{definition}[Ideal Measurement; Complement; Commutativity;
disjunction]~ 
\begin{itemize}
\item A set of projections $\left\{ P_{i}\right\} _{i=0}^{N-1}$ is called
an \emph{ideal measurement} if it is a partition of the identity,
i.e., $\tensor*[_{\mathscr{O}}]{\sum}{_{i=0}^{N-1}}P_{i}=\mathbb{1}$~\cite{Swart2013}.
In this case, projections $\left\{ P_{i}\right\} _{i=0}^{N-1}$ must
be mutually orthogonal~\cite{Griffiths2003}.
\item If $P$ is a projection operator, then $\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}P$
is also a projection operator, called \emph{complement}. It is orthogonal
to $P$, and corresponds to the complement event~$\Omega\backslash E$
in classical probability~\cite{Griffiths2003}. 
\item Projection operators $P_{0}$ and $P_{1}$ \emph{commute} if $P_{0}P_{1}=P_{1}P_{0}$.
The product of two projection operators~$P_{0}P_{1}$ is also a projection
operator if and only if they commute. This corresponds to the classical
intersection between events~\cite{peres1995quantum,Griffiths2003}. 
\item For two commuting projection operators $P_{0}$ and $P_{1}$, their
\emph{disjunction}~$P_{0}\tensor*[_{\mathscr{O}}]{\vee}{}P_{1}$
is defined to be $P_{0}\tensor*[_{\mathscr{O}}]{+}{}P_{1}\tensor*[_{\mathscr{O}}]{-}{}P_{0}P_{1}$~\cite{Griffiths2003}. 
\end{itemize}
\qed\end{definition}

Based on the above definition and properties of quantum events, we
can present an example before giving the formal definition of quantum
probability measures.

\begin{example}[One-qubit quantum probability space] Consider
a one-qubit Hilbert space with states $\ket{\phi}$, which can be
expressed as a linear combination of $\ket{0}$ and $\ket{1}$, i.e.,
$\ket{\phi}=\alpha\ket{0}+\beta\ket{1}$ such that $|\alpha|^{2}+|\beta|^{2}=1$,
$\alpha,\beta\in\C$. The set of events associated with this Hilbert
space consists of all projection operators. Each event is interpreted
as a possible post-measurement state of a quantum system in current
state $\ket{\phi}$. For example, the event $\proj{0}$ indicates
that the post-measurement state will be $\ket{0}$; the event $\proj{1}$
indicates that the post-measurement state will be $\ket{1}$; the
event $\proj{\ps}$ where $\ket{\ps}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$
indicates that the post-measurement state will be $\ket{\ps}$; the
event $\mathbb{1}=\proj{0}\tensor*[_{\mathscr{O}}]{+}{}\proj{1}$
indicates that the post-measurement state will be a linear combination
of $\ket{0}$ and $\ket{1}$; and the empty event $\mathbb{0}$ states
that the post-measurement state will be the empty state. As in the
classical case, a probability measure is a function that maps events
to $[0,1]$. Here is a partial specification of a possible probability
measure: 
\[
\begin{array}{rcl}
\mu\left(\mathbb{0}\right)=0,\quad\mu\left(\mathbb{1}\right)=1,\quad\mu\left(\proj{0}\right)=1,\quad\mu\left(\proj{1}\right)=0,\quad\mu\left(\proj{\ps}\right)=1/2,\quad\ldots\end{array}
\]
Note that, similarly to the classical case, the probability of $\mathbb{1}$
is 1 and the probability of collections of orthogonal events (e.g.,
$\proj{0}+\proj{1}$) is the sum of the individual probabilities.
A collection of non-orthogonal events (e.g., $\proj{0}$ and $\proj{\ps}$)
is however not even a valid event. In the classical example, we argued
that each probability measure is uniquely determined by two actual
coins. A similar (but much more subtle) argument is valid also in
the quantum case. By postulates of quantum mechanics and Gleason's
theorem, it turns out that for large enough quantum systems, each
probability measure is uniquely determined by an actual quantum state.
\qed\end{example}

%%%%%


\subsection{Quantum Real-valued Probability Measures}

\begin{definition}[Quantum Probability Measure~\cite{10.2307/2308516,gleason1957,Redhead1987-REDINA,Maassen2010}]\label{def:QuantumProbabilitySpace}
Given a Hilbert space $\Hilb$ with its set of events~$\events$,
a \emph{quantum probability measure} is a function~$\mu:\events\rightarrow[0,1]$
such that:\footnote{It is possible to define a more general space of events consisting
of all operators~$\mathcal{A}$ on $\Hilb$ and to extend $\mu$
linearly to $\mathcal{A}$ and consider $\mu:\mathcal{A}\rightarrow\C$~\cite{Maassen2010,Swart2013}.
When an operator $A\in\mathcal{A}$ is Hermitian, $\mu\left(A\right)$
is the expectation value of $A$ which could bigger than one because
$\mu$ is linear. We does not take this approach because we want to
focus only on probability. } 
\begin{itemize}
\item $\mu(\mathbb{0})=0$. 
\item $\mu(\mathbb{1})=1$. 
\item For any projection $P$, 
\begin{equation}
\mu\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}P\right)=1\tensor*[_{\mathbb{R}}]{-}{}\mu\left(P\right)\textrm{ .}\label{eq:QuantumReal-valuedProbability-Complement}
\end{equation}
\item For a set of mutually orthogonal projections $\left\{ P_{i}\right\} _{i=0}^{N-1}$,
we have $\mu\left(\tensor*[_{\mathscr{O}}]{\sum}{_{i=0}^{N-1}}P_{i}\right)=\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}\mu\left(P_{i}\right)$
. 
\end{itemize}
\qed\end{definition}

Similar to classical definition~\ref{def:ClassicalProbabilitySpace},
$\mu(\mathbb{0})=0$ and equation~(\ref{eq:QuantumReal-valuedProbability-Complement})
can be induced by other conditions. A set of events~$\events$ together
with a quantum probability measure is called a \emph{quantum probability
space}. Comparing to classical probability spaces, the empty set~$\emptyset$
corresponds to the empty projection~$\mathbb{0}$, and the event
of whole space~$\Omega$ corresponds to the identity projection~$\mathbb{1}$.
In contrast, the union~$\cup$ of any two events always gives an
event classically, but the operator addition~$\tensor*[_{\mathscr{O}}]{+}{}$
of two projections may not be a projection. As the result, the classical
condition~$\pmeas(E_{0}\cup E_{1})=\pmeas(E_{0})\tensor*[_{\mathbb{R}}]{+}{}\pmeas(E_{1})$
is always defined, and it is true when $E_{0}$ and $E_{1}$ are disjoint;
however, $\mu\left(P_{0}\tensor*[_{\mathscr{O}}]{+}{}P_{1}\right)=\mu(P_{0})\tensor*[_{\mathbb{R}}]{+}{}\mu(P_{1})$
is always true whenever the left-handed side is defined.

% \yutsung{The definition of independence is different from different
% sources:
% \begin{enumerate}
% \item In Swart's~\cite{Swart2013}, given a $*$-algebra~$\mathscr{A}$,
% two commuting sub-$*$-algebra $\mathscr{A}_{1}$ and $\mathscr{A}_{2}$ are \emph{logically
% independent} if for all probability measure~$\mu_{1}$ on $\mathscr{A}_{1}$ and
% $\mu_{2}$ on $\mathscr{A}_{2}$, there is an unique probability measure~$\mu$
% on the smallest sub-$*$-algebra of $\mathscr{A}$ containing both
% $\mathscr{A}_{1}$ and $\mathscr{A}_{2}$ such that $\mu\left(A_{1}
% A_{2}\right)=\mu_{1}\left(A_{1}\right)\mu_{2}\left(A_{2}\right)$
% for all $A_{1}\in\mathscr{A}_{1}$ and $A_{2}\in\mathscr{A}_{2}$. If two
% sub-$*$-algebra $\mathscr{A}_{1}$ and $\mathscr{A}_{2}$ are logically
% independent, the smallest sub-$*$-algebra containing both $\mathscr{A}_{1}$ and
% $\mathscr{A}_{2}$ is denoted by $\mathscr{A}_{1}\otimes\mathscr{A}_{2}$, and
% its probability measure~$\mu$ is denoted by $\mu_{1}\otimes\mu_{2}$\\
% Then, Swart proved the Bell inequality on logically independence $*$-algebras
% with a product state which gives a clear physical meaning. However, this
% only defines the independence of algebras instead of events.
% \item In Yuan's~\cite{Yuan2012}, given a $\mu$ on a $*$-algebra~$\mathscr{A}$,
% two sub-$*$-algebra $\mathscr{A}_{1}$ and $\mathscr{A}_{2}$ are \emph{independent}
% if $\mu\left(A_{1}A_{2}\right)=\mu\left(A_{1}\right)\mu\left(A_{2}\right)$
% for all $A_{1}\in\mathscr{A}_{1}$ and $A_{2}\in\mathscr{A}_{2}$.
% The same problem as the previous one: this defines the
% independent of algebras, but not events. Morever, I haven't found a clear
% physical meaning of this definition, yet\ldots
% \item In order to understand the definition of independence in~\cite{GuehneKleinmannCabelloEtAl2010},
% we need to first understand their notation because they focused on the
% hidden-variable (HV) models. Let $\lambda$ is the HV, $A_{i}$ is
% the measurement of the observable~$A$ at the position $i$ in the
% sequence. For example, $A_{1}B_{2}C_{3}$ denotes the sequence of
% measuring $A$ first, then $B$, and finally $C$. Given a fixed $\lambda$, we
% assume the outcome of an observable is deterministic. For example, the
% outcome of $B_{2}$ from the preceding sequence is denoted by $v\left(B_{2}\middle|A_{1}B_{2}C_{3}\right)$.
% Then, the outcome of $A$ is \emph{independent} of whether $B$ is measured
% before or after $A$ is $v\left(A_{1}\right)=v\left(A_{2}\middle|B_{1}A_{2}\right)$.\\
% If the outcomes of compatible observables are independent for any
% HV~$\lambda$, the HV model is non-contextual. (Notice that they have their own
% definition of ``compatible observables'' for their HV model\ldots)\\
% Compare to the previous definitions, this definition is appealing
% in a sense that projection operators are special cases of observables,
% and this definition has a physical meaning related to contextuality.
% However, before adopting their definition, we need to reformulate
% the definition to the usual quantum model....
% \item For the definition we used last time, given a quantum probability
% measure~$\mu:\events\rightarrow[0,1]$, two commuting projections
% $P_{1}$ and $P_{2}$ are \emph{independent} if
% $\mu\left(P_{1}P_{2}\right)=\mu\left(P_{1}\right)\mu\left(P_{2}\right)$.
% We have two examples:
% \end{enumerate}
% }
% 
% \yutsung{\begin{example}[Independence of product state] Consider
% the 2-qubit Hilbert space with a probability measure~$\mu\left(P\right)=\melement{\ps\ps}{P}$
% with the following projections:
% \[
% \begin{array}{rcl}
% P_{1}=\proj{00}+\proj{01},\quad P_{2}=\proj{00}+\proj{10},\quad P_{3}=\proj{10}+\proj{11},\quad P_{4}=\proj{00}\end{array}
% \]
% and their probabilities: 
% \[
% \begin{array}{rcl}
% \mu\left(\mathbb{0}\right)=0,\quad\mu\left(P_{1}\right)=1/2,\quad\mu\left(P_{2}\right)=1/2,\quad\mu\left(P_{3}\right)=1/2,\quad\mu\left(P_{4}\right)=1/4\textrm{ .}\end{array}
% \]
% On one hand, $P_{1}$ and $P_{3}$ are complement to each other so
% we have
% \begin{eqnarray*}
% \mu\left(P_{1}\right)+\mu\left(P_{3}\right) & = & \frac{1}{2}+\frac{1}{2}=1\\
% \mu\left(P_{1}\right)\mu\left(P_{3}\right) & = & \frac{1}{4}\ne0=\mu\left(\mathbb{0}\right)=\mu\left(P_{1}P_{3}\right)
% \end{eqnarray*}
% Therefore, $P_{1}$ and $P_{3}$ commute, but are not independent. On
% the other hand, $P_{1}$ also commutes with $P_{2}$, and
% \begin{eqnarray*}
% \mu\left(P_{1}\right)\mu\left(P_{2}\right) & = & \frac{1}{4}=\mu\left(P_{4}\right)=\mu\left(P_{1}P_{2}\right)
% \end{eqnarray*}
% so that $P_{1}$ and $P_{2}$ are independent.\qed\end{example}
% 
% \begin{example}[Independence of any states] In general, for any orthonormal
% basis~$\left\{ \ket{\psi_{j}}\right\} _{j=0}^{3}$, we can always
% pick 
% \[
% \begin{array}{rcl}
% P_{1}=\proj{\psi_{0}}+\proj{\psi_{1}},\quad P_{2}=\proj{\psi_{0}}+\proj{\psi_{2}},\quad P_{3}=\proj{\psi_{0}}\textrm{ .}\end{array}
% \]
% With the probability measure~$\mu\left(P\right)=\melement{\psi}{P}$,
% where $\ket{\psi}=\frac{1}{2}\sum_{j=0}^{3}\ket{\psi_{j}}$, their
% probabilities are
% \[
% \begin{array}{rcl}
% \mu\left(P_{1}\right)=1/2,\quad\mu\left(P_{2}\right)=1/2,\quad\mu\left(P_{3}\right)=1/4\textrm{ ,}\end{array}
% \]
% and we have
% \begin{eqnarray*}
% \mu\left(P_{1}\right)\mu\left(P_{2}\right) & = & \frac{1}{4}=\mu\left(P_{3}\right)=\mu\left(P_{1}P_{2}\right)\textrm{ .}
% \end{eqnarray*}
% In particular, the following four Bell states form an orthonormal
% basis~\cite{CabelloPRL.86.2001}: 
% \[
% \ket{\phi^{\pm}} = \frac{1}{\sqrt{2}}\left(\ket{00}\pm\ket{11}\right),\quad
% \ket{\psi^{\pm}} = \frac{1}{\sqrt{2}}\left(\ket{01}\pm\ket{10}\right)\textrm{ .}
% \]
% Their sum is also a Bell state~
% \[
% \ket{\psi}=\frac{1}{2}\left(\ket{\phi^{+}}+\ket{\phi^{-}}+\ket{\psi^{+}}+\ket{\psi^{-}}\right)=\frac{1}{\sqrt{2}}\left(\ket{00}+\ket{01}\right)\textrm{ .}
% \]
% Thus, $\proj{\phi^{+}}+\proj{\psi^{+}}$ and $\proj{\phi^{+}}+\proj{\psi^{-}}$
% are independent projections with respect to the probability
% measure~$\mu\left(P\right)=\melement{\psi}{P}$.\qed\end{example}}

\footnote{\yutsung{The definition of independence is interesting, and we definitely
need to discuss it when we want to discuss Bell's theorem and the
Kochen-Specker theorem. However, the definition diverges. So we just
leave it so far, and we will go back if we really need it in this
paper. (Maybe when discussing repeating experiments?)}}

Recall that classical probability measure can be constructed from
its Radon-Nikodym derivative. Similarly, a quantum probability measure
can be easily constructed by states according to the Born rule~\cite{Born1983,Mermin2007,RiederSvozil2007}.
For each pure normalized ($\ip{\phi}{\phi}=1$) quantum state $\ket{\phi}$,
the Born rule induces a probability measure $\mu_{\phi}^{B}$ as follows:
\[
\mu_{\phi}^{B}(P)=\ip{\phi}{P\phi}\textrm{ .}
\]

Moreover, the Born rule can be extended to a mixed state. When we
prepare a state, suppose the probability of preparing state~$\ket{\phi_{j}}$
is $q_{j}$, the state of the system can be expressed as a density
matrix~$\rho=\tensor*[_{\mathscr{O}}]{\sum}{_{j=0}^{N-1}}q_{j}\proj{\phi_{j}}$,
where $\tensor*[_{\mathbb{R}}]{\sum}{_{j=0}^{N-1}}q_{j}=1$. Then,
the quantum probability measure introduced by $\rho$ is the combination
of $\mu_{\phi_{j}}^{B}$ with respect to probability $q_{j}$~\cite{peres1995quantum,544199,RiederSvozil2007}:
\begin{eqnarray}
\mu_{\rho}^{B}\left(P\right) & = & \Tr\left(\rho P\right)=\sideset{_{\mathbb{R}}}{_{j=0}^{N-1}}\sum q_{j}\mu_{\phi_{j}}^{B}\left(P\right)\textrm{ .}\label{BornRule.mixed}
\end{eqnarray}

%%%%%


\subsection{Measuring Quantum Probabilities Ideally: From Radon-Nikodym to Gleason's
Theorem}

Similar to the classical case, by applying the law of large numbers,
quantum probabilities can be estimated by relative frequencies. For
example, if we want to know the probability of the spin up in the
Stern-Gerlach experiment~\cite{Stern1988,peres1995quantum,544199,Griffiths2003},
we can put a beam of silver atoms in a highly inhomogeneous magnetic
field, and counting the number of atoms deflects up. Ideally, if the
local field of strength directs to the $z$-axis, and all particles
have the same velocity, the Stern-Gerlach experiment only produces
two spots corresponding to $\ket{0}$ and $\ket{1}$. Notice that
whenever the magnetic field of the Stern-Gerlach is fixed, i.e., an
ideal measurement is picked, measuring the spin is exactly the same
as tossing a coin. In another word, if somebody claims she is tossing
a coin behind a veil, and only shows the resulting heads or tails,
we cannot distinguish whether she has really tossed a coin, or she
has run an Stern-Gerlach experiment, and shows us the head, the tail,
or the side of coin if the silver atoms is spin up, spin down, or
just hit the middle, respectively.

Mathematically, to fix the magnetic field of the Stern-Gerlach experiment
is to pick a orthonormal basis~$\Omega=\left\{ \ket{\psi_{j}}\right\} _{j=0}^{d-1}$
for a given Hilbert space~$\Hilb$ of dimension~$d$, and the state
of the silver atom can be represented by a density matrix~$\rho$
written in the same basis, i.e., $\rho=\tensor*[_{\mathscr{O}}]{\sum}{_{i=0}^{d-1}}\tensor*[_{\mathscr{O}}]{\sum}{_{i=0}^{d-1}}r_{ij}\ket{\psi_{i}}\bra{\psi_{j}}$.
As we discussed in the last paragraph, the Born rule of the mixed
state~$\rho$ induces a classical probability measure~$\pmeas^{\Omega}:2^{\Omega}\rightarrow[0,1]$
on $\Omega$. Let $E\in2^{\Omega}$ be a classical event and $P=\tensor*[_{\mathscr{O}}]{\sum}{_{\ket{\psi_{j}}\in E}}\proj{\psi_{j}}$
be the quantum event corresponding to subspace span by $E$, we can
have
\begin{equation}
\pmeas^{\Omega}\left(E\right)=\mu_{\rho}^{B}\left(P\right)\overset{\left(\ref{BornRule.mixed}\right)}{=}\Tr\left(\rho P\right)=\sideset{_{\mathbb{R}}}{_{\ket{\psi_{j}}\in E}}\sum r_{jj}\textrm{ .}\label{BornRule.mixed-1}
\end{equation}
By equation (\ref{eq:Radon-Nikodym}), the Radon-Nikodym derivative
of $\pmeas'$ is $f\left(\ket{\psi_{j}}\right)=r_{jj}$, which is
just the diagonal elements of $\rho$. In this sense, the Radon-Nikodym
derivative~$f$ is a special case of the density matrix~$\rho$,
and the Radon-Nikodym theorem is a special case of Gleason's theorem~\cite{HollandJr1970,Redhead1987-REDINA,Jaeger2007}.

\begin{thm}[Gleason's theorem~\cite{gleason1957,Redhead1987-REDINA,peres1995quantum}]\label{cor:Gleason's}In
a Hilbert space $\Hilb$ of dimension $d\geq3$, given a quantum probability
measure~$\mu:\events\rightarrow[0,1]$, there exists a unique mixed
state~$\rho$ such that $\mu=\mu_{\rho}^{B}$.\end{thm}

It is instructive to study a counterexample when $d=2$, i.e., the
case of a one-qubit system.

\begin{example}[One-qubit quantum probability measure] Consider
a quantum probability measure~$\mu:\events\rightarrow[0,1]$ defined
as follow: 
\[
\mu(P)=\begin{cases}
1 & \textrm{, if }P=\proj{\ps}\textrm{ ;}\\
0 & \textrm{, if }P=\proj{\ms}\textrm{ ;}\\
\mu_{\ket{0}}^{B}(P) & \textrm{, otherwise.}
\end{cases}
\]
On one hand, $\mu$ is a quantum probability measure. Because $\mu$
is almost the same as the quantum probability measure~$\mu_{\ket{0}}^{B}$,
we only need to check the orthogonal pair~$\proj{\ps}$ and $\proj{\ms}$:
$\mu(\proj{\ps})\tensor*[_{\mathbb{R}}]{+}{}\mu(\proj{\ms})=1\tensor*[_{\mathbb{R}}]{+}{}0=1$.
On the other hand, $\mu$ cannot be induced by any mixed state because
$\mu(\proj{\ps})=\mu(\proj{0})=1$, and $\mu_{\rho}^{B}(P)=1$ if
and only if $\rho$ represents a pure state and $\rho=P$. \qed\end{example}

%%%%%


\subsection{Measuring Quantum Probabilities in Reality: Quantum Interval-valued
Probability Measures}

In reality, a Stern-Gerlach experiment requires a lot of resource
to keep the local field of strength directing to the $z$-axis precisely,
to keep the atoms having almost the same velocity, and to point out
the exact position each particle landed on. If the field of strength
does not perfectly direct to the $z$-axis, we do not really test
the quantum event we want to test. This problem will be handled later
when we introduce the discrete quantum theory. Even if the field of
strength perfectly directs to the $z$-axis, the variant velocity
makes spots broader and more washed out. Together with the precision
limit of the detector, it may sometimes be hard to decide a particle
corresponding to which state\footnote{\yutsung{Add citations to support the idea... Haven't found suitable
ones...}

\amr{preparation fuzzy, device fuzzy, Meyer~\cite{PhysRevLett.83.3751}} }. 

Similar to Buffon's needles, this kind of fuzziness can be taken into
account by associating each quantum event with an interval-valued
probability~$\left[l,r\right]$. Therefore, we should plug in the
definition of interval-valued probability measure into the definition
of quantum probability space for each ideal measurement. More specifically,
for classical probability measures, definition~\ref{def:ClassicalProbabilitySpace}
and lemma~\ref{lem:classicalProbabilityMeasure} are almost the same
except the last condition using the equal sign~$=$ in real-valued
definition~\ref{def:ClassicalProbabilitySpace} and the subset equal
sign~$\subseteq$ in interval-valued lemma~\ref{lem:classicalProbabilityMeasure}.
Therefore, for quantum probability measures, replacing the the equal
sign~$=$ in real-valued definition~\ref{def:QuantumProbabilitySpace}
by the subset equal sign~$\subseteq$ gives the only reasonable definition
for quantum interval-valued probability measures.

\begin{definition}[Quantum Interval-valued Probability Measure]\label{def:QuantumInterval-valuedProbability}
Given a Hilbert space $\Hilb$ with the set of quantum events~$\events$,
and a collection of intervals~$\mathscr{I}$, a \emph{quantum $\mathscr{I}$-interval-valued
probability measure} is a function~$\bar{\mu}:\events\rightarrow\mathscr{I}$
such that: 
\begin{itemize}
\item $\bar{\mu}(\mathbb{0})=\left[0,0\right]$. 
\item $\bar{\mu}(\mathbb{1})=\left[1,1\right]$. 
\item For any projection $P$, 
\begin{equation}
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}P\right)=\left[1,1\right]\tensor*[_{\mathscr{I}}]{-}{}\bar{\mu}\left(P\right)\textrm{ .}\label{eq:QuantumInterval-valuedProbability-Complement}
\end{equation}
\item For a set of mutually orthogonal projections $\left\{ P_{i}\right\} _{i=0}^{N-1}$,
we have 
\begin{equation}
\bar{\mu}\left(\sideset{_{\mathscr{O}}}{_{i=0}^{N-1}}\sum P_{i}\right)\subseteq\sideset{_{\mathscr{I}}}{_{i=0}^{N-1}}\sum\bar{\mu}\left(P_{i}\right)\textrm{ .}\label{eq:QuantumInterval-valuedProbability-Inclusion}
\end{equation}
\end{itemize}
\qed\end{definition}

Similar to the classical interval-valued probability measure, a quantum
interval-valued probability measure can be constructed from a real-valued
one. 

\begin{example}[Quantum three-value interval-valued probability
measure] We consider three intervals\emph{ }$\left[0,0\right]$,
$\left[1,1\right]$ and \emph{$\left[0,1\right]$}, where $\left[0,0\right]$
and $\left[1,1\right]$ are called \imposs~and \necess~as before,
and \emph{$\left[0,1\right]$} is called \unknown~because it provides
no information. For any Hilbert space and any quantum probability
measure~$\mu:\events\rightarrow\left[0,1\right]$, we can define
a quantum interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$
by $\bar{\mu}(P)=\iota\left(\mu(P)\right)$, where $\iota:\left[0,1\right]\rightarrow\mathscr{I}$
is defined by 
\[
\iota(x)=\begin{cases}
\necess & \textrm{, if }x=1\textrm{ ;}\\
\imposs & \textrm{, if }x=0\textrm{ ;}\\
\unknown & \textrm{, otherwise.}
\end{cases}
\]
$\iota$ has two interesting properties $\iota\left(1\tensor*[_{\mathbb{R}}]{-}{}x\right)=\left[1,1\right]\tensor*[_{\mathscr{I}}]{-}{}\iota\left(x\right)$
and $\iota\left(\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}x_{i}\right)\subseteq\tensor*[_{\mathscr{I}}]{\sum}{_{i=0}^{N-1}}\iota\left(x_{i}\right)$,
where $x$ and $\tensor*[_{\mathbb{R}}]{\sum}{_{i=0}^{N-1}}x_{i}\in\left[0,1\right]$.
By applying these two properties, it is easy to verify $\bar{\mu}$
is a quantum \emph{$\mathscr{I}$}-interval-valued probability measure.\qed\end{example}

Similar to classical interval-valued probability measures which have
no easy Radon-Nikodym theorem, quantum interval-valued probability
measures have no easy Gleason's theorem. Although we could define
core and convex for quantum interval-valued probability measures,
in contrast to theorem~\ref{thm:Shapley}, a convex quantum interval-valued
probability measure may have a empty core, i.e., it cannot correspond
to a quantum real-valued probability measure or a mixed state.

\begin{definition}~Given a quantum interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$:
\begin{itemize}
\item A \emph{core} of $\bar{\mu}$ is the set $\mathrm{core}\left(\bar{\mu}\right)=\left\{ \textrm{quantum probability measure }\pmeas:\events\rightarrow[0,1]\middle|\forall E\in\events.\pmeas\left(E\right)\in\bar{\mu}\left(E\right)\right\} $.
\item $\bar{\mu}$ is called convex if 
\begin{equation}
\bar{\mu}\left(P_{0}\tensor*[_{\mathscr{O}}]{\vee}{}P_{1}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(P_{0}P_{1}\right)\subseteq\bar{\mu}\left(P_{0}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(P_{1}\right)\label{eq:QuantumInterval-valuedProbability-Convex}
\end{equation}
 for all commuting $P_{0},P_{1}\in\events$. 
\end{itemize}
\qed\end{definition}

\begin{thm}\label{def:EmptyCoreQuantumInterval-valuedProbability}
There is a convex quantum interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$
such that $\mathrm{core}\left(\bar{\mu}\right)=\emptyset$.\qed\end{thm}

This theorem can be proved by the following example. 

\begin{example}[Three-dimensional quantum three-value interval-valued
probability measure]\label{ex:three-dimensional-three-value} Given
a three dimensional Hilbert space with an orthonormal basis $\left\{ \ket{0},\ket{1},\ket{2}\right\} $.
Consider\emph{ $\mathscr{I}=\left\{ \necess,\imposs,\unknown\right\} $}
and $\ket{\ps}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$, and $\ket{\ps'}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{2})$.
Let 
\begin{eqnarray*}
\bar{\mu}(\mathbb{0})=\bar{\mu}(\proj{0})=\bar{\mu}(\proj{\ps})=\bar{\mu}(\proj{\ps'}) & = & \imposs\textrm{ ,}\\
\bar{\mu}(\mathbb{1})=\bar{\mu}(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{0})=\bar{\mu}(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\ps})=\bar{\mu}(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\ps'}) & = & \necess\textrm{ ,}\\
\bar{\mu}(P) & = & \unknown\textrm{, otherwise. }
\end{eqnarray*}
To verify $\bar{\mu}$ is a quantum interval-valued probability measure,
first notice that $\bar{\mu}\left(P_{0}\tensor*[_{\mathscr{O}}]{+}{}P_{1}\right)\subseteq\bar{\mu}\left(P_{0}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(P_{1}\right)$
implies equation (\ref{eq:QuantumInterval-valuedProbability-Inclusion})
by induction. Second, equation (\ref{eq:QuantumInterval-valuedProbability-Complement})
implies $\bar{\mu}\left(\mathbb{1}\right)\subseteq\bar{\mu}\left(P\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}P\right)$.
Therefore, to prove equation (\ref{eq:QuantumInterval-valuedProbability-Inclusion}),
it is sufficient to check 
\[
\bar{\mu}\left(\proj{\psi_{0}}\tensor*[_{\mathscr{O}}]{+}{}\proj{\psi_{1}}\right)\subseteq\bar{\mu}\left(\proj{\psi_{0}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{\psi_{1}}\right)
\]
for orthogonal $\ket{\psi_{0}}$ and $\ket{\psi_{1}}$. This equation
always holds because we always have $\unknown\subseteq\bar{\mu}\left(\proj{\psi_{0}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{\psi_{1}}\right)$.

Next, we need to verify that $\bar{\mu}$ is convex. First, for orthogonal
projections $P_{0}$ and $P_{1}$, equation (\ref{eq:QuantumInterval-valuedProbability-Inclusion})
implies equation (\ref{eq:QuantumInterval-valuedProbability-Convex}).
Second, when $P_{0}=P_{1}\tensor*[_{\mathscr{O}}]{+}{}P_{2}$, equation
(\ref{eq:QuantumInterval-valuedProbability-Convex}) holds trivially.
Hence, we only need to verify the case which $P_{0}=\proj{\psi_{0}}\tensor*[_{\mathscr{O}}]{+}{}\proj{\psi_{2}}$
and $P_{1}=\proj{\psi_{1}}\tensor*[_{\mathscr{O}}]{+}{}\proj{\psi_{2}}$,
where $\left\{ \ket{\psi_{0}},\ket{\psi_{1}},\ket{\psi_{2}}\right\} $
is an orthonormal basis. Because we have 
\begin{eqnarray*}
\bar{\mu}\left(P_{0}\tensor*[_{\mathscr{O}}]{\vee}{}P_{1}\right) & = & \bar{\mu}\left(\mathbb{1}\right)=\necess\\
\bar{\mu}\left(P_{0}P_{1}\right) & = & \bar{\mu}\left(\proj{\psi_{2}}\right)\subseteq\unknown
\end{eqnarray*}
and $\bar{\mu}\left(P_{0}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(P_{1}\right)$
is either $\necess\tensor*[_{\mathscr{I}}]{+}{}\unknown$ or $\unknown\tensor*[_{\mathscr{I}}]{+}{}\unknown$.
Therefore, we have $\bar{\mu}\left(P_{0}\tensor*[_{\mathscr{O}}]{\vee}{}P_{1}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(P_{0}P_{1}\right)\subseteq\bar{\mu}\left(P_{0}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(P_{1}\right)$.

Finally, we will prove that $\bar{\mu}$ cannot correspond to any
quantum probability measure by contradiction. Suppose $\mu(P)\in\bar{\mu}(P)$
for some quantum probability measure~$\mu:\events\rightarrow\left[0,1\right]$.
We must have 
\begin{equation}
\mu(\proj{0})=\mu(\proj{\ps})=\mu(\proj{\ps'})=0\textrm{ .}\label{eq:probability-zero-on-states}
\end{equation}
By Gleason's theorem, there is a mixed state~$\rho=\tensor*[_{\mathscr{O}}]{\sum}{_{j=0}^{N-1}}q_{j}\proj{\phi_{j}}$
such that $\mu\left(P\right)=\tensor*[_{\mathbb{R}}]{\sum}{_{j=0}^{N-1}}q_{j}\ip{\phi_{j}}{P\phi_{j}}$,
where $\tensor*[_{\mathbb{R}}]{\sum}{_{j=0}^{N-1}}q_{j}=1$ and $q_{j}>0$.
However, no pure state~$\ket{\phi}$ satisfies $\ip{\phi}{0}=\ip{\phi}{\ps}=\ip{\phi}{\ps'}=0$
so that we have $\mu(P)\notin\bar{\mu}(P)$ for all quantum probability
measure~$\mu$.\qed\end{example}

Recall that when approximating the probability classically, the quality
of the experimental equipment cannot be compensated by the number
of independent trials. If the precision of the intervals~$\mathscr{I}$
represents the quality of the quantum experimental equipment, example~\ref{ex:three-dimensional-three-value}
tells us that we cannot identify a particular state if the experimental
equipment reveals too little information. For example, $\bar{\mu}$
might correspond to $\ket{2}$ because $\bar{\mu}(\proj{0})=\bar{\mu}(\proj{\ps})=\imposs$,
$\ket{1}$ because $\bar{\mu}(\proj{0})=\bar{\mu}(\proj{\ps'})=\imposs$,
or the density matrix~$\frac{\mathbb{1}}{3}$ because $\bar{\mu}(\proj{\phi})\ne\necess$
for all $\ket{\phi}$. Also, example~\ref{ex:three-dimensional-three-value}
is no longer valid if our measurement equipment is a little more precise,
say $\mathscr{I}=\left\{ \imposs,\unlikely,\likely,\necess\right\} $,
because example~\ref{ex:three-dimensional-three-value} really use
the property of $\unknown$.

If we consider $\mathscr{I}=\left\{ \imposs,\unlikely,\likely,\necess\right\} $,
example~\ref{ex:three-dimensional-4-value} in appendix~\ref{sec:Examples-for-QuantumIVPM}
provides another example with an empty core. Comparing to example~\ref{ex:three-dimensional-three-value},
example~\ref{ex:three-dimensional-4-value} is closer to the Born
rule in a sense that it might only correspond to $\ket{2}$ or $\frac{\mathbb{1}}{3}$,
but not $\ket{1}$. Example~\ref{ex:three-dimensional-4-value} is
also no longer valid for more precise measurement equipment as well.
In general, we believe if the measurement equipment is more and more
precise, its interval-valued probability measure will be closer and
closer to the the Born rule. In the limit case, $\mathscr{I}=\left\{ \left\{ a\right\} \middle|a\in\left[0,1\right]\right\} $
recovers conventional Gleason's theorem and the Born rule.

\footnote{\yutsung{Notice that Gleason used his conditions to prove quantum
probability measures are continuous first, and then prove they can
be got from the Born rule. Since the current examples are all discontinuous,
maybe ``add continuous'' could give us a interval-valued Gleason's
theorem (?) 

Continuity definitely has physical and computer science meaning. For
example, we could said something like, ``no matter the underlining
physical reality, human doesn't have the ability to distinguish discontinuous
function,`` and cite \cite{Ziegler2007,weihrauch2012computable}.
However, we might need to rewrite all the interpretation if this approach
works...}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\bibliographystyle{plain}
\bibliography{discreteGBKS}
\end{comment}

\printbibliography


\appendix
%%%%%


\section{\label{sec:Examples-for-QuantumIVPM}Examples for Quantum Interval-valued
Probability Measures}

We are going to provide an quantum 4-value interval-valued probability
measures corresponding to no quantum probability measure, i.e., for
all mixed state~$\rho$ there is a $P\in\events$ such that $\mu_{\rho}^{B}(P)\notin\bar{\mu}(P)$.
After the example, we will also justify why this example cannot fit
with more precise intervals.

\begin{example}[Three-dimensional quantum 4-value interval-valued
probability measure]\label{ex:three-dimensional-4-value} \footnote{\yutsung{Need to verify whether this example is convex or not.}}Given
a three dimensional Hilbert space with an orthonormal basis $\left\{ \ket{0},\ket{1},\ket{2}\right\} $.
Consider\emph{ }$\mathscr{I}=\left\{ \imposs,\unlikely,\likely,\necess\right\} $,
$\ket{\ps}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$, $\ket{\ms}=\frac{1}{\sqrt{2}}(\ket{0}-\ket{1})$,
and a quantum $\mathscr{I}$-interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$
defined as follow, where the state vectors corresponding to 1-dimensional
projector are also illustrated in figure~\ref{fig:three-dimensional-4-value}.
\begin{enumerate}
\item Let 
\begin{eqnarray*}
\bar{\mu}(\mathbb{0})=\bar{\mu}(\proj{0})=\bar{\mu}(\proj{\ps}) & = & \imposs\\
\bar{\mu}(\mathbb{1})=\bar{\mu}(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{0})=\bar{\mu}(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\ps}) & = & \necess\textrm{ ,}
\end{eqnarray*}
 where $\ket{0}$ and $\ket{\ps}$ is illustrated as the red and green
dotted vector, respectively.
\item Consider the red and green circles in figure~\ref{fig:three-dimensional-4-value}.
They are the states orthogonal to $\ket{0}$ and $\ket{\ps}$, respectively,
and can be parametrized as $\ket{0_{\theta,\gamma}^{\perp}}=\rme^{\rmi\gamma}\sin\theta\ket{1}+\cos\theta\ket{2}$
and $\ket{\ps_{\theta,\gamma}^{\perp}}=-\rme^{\rmi\gamma}\sin\theta\ket{\ms}+\cos\theta\ket{2}$,
where $0\le\theta\le\frac{\pi}{2}$ and $0\le\gamma<2\pi$. The dotted
half of those states need special treatment, i.e., whenever $0\le\theta<\frac{\pi}{2}$
and $0\le\gamma<\pi$, we define 
\begin{eqnarray*}
\bar{\mu}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)=\bar{\mu}\left(\proj{\ps_{\theta,\gamma}^{\perp}}\right) & = & \likely\\
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{0_{\theta,\gamma}^{\perp}}\right)=\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\ps_{\theta,\gamma}^{\perp}}\right) & = & \unlikely\textrm{ .}
\end{eqnarray*}
\footnote{\yutsung{This case might be simplify if we consider 5 values. Then,
every state orthogonal to $\ket{0}$ and $\ket{\ps}$ can map to $\left[\frac{1}{4},\frac{3}{4}\right]$.
Not sure whether we need to simplify the example or not, because this
example is used in example~\ref{ex:three-dimensional-6-value}. Or
we may just keep example~\ref{ex:three-dimensional-6-value} in the
later version if that example is more important...}}
\item Otherwise, $\bar{\mu}(\proj{\psi})=\unlikely$ and $\bar{\mu}(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi})=\likely$. 
\end{enumerate}
That $\bar{\mu}$ has an empty core can be proved by contradiction.
Assume there is a real-valued probability measure satisfying $\mu_{\rho}^{B}(P)\in\bar{\mu}(P)$
for all $P\in\events$. Because $\mu_{\rho}^{B}(\proj{0})\in\bar{\mu}(\proj{0})=\imposs$
and $\mu_{\rho}^{B}(\proj{\ps})\in\bar{\mu}(\proj{\ps})=\imposs$,
we must have $\mu_{\rho}^{B}(\proj{0})=\mu_{\rho}^{B}(\proj{\ps})=0$
so that $\mu_{\rho}^{B}=\mu_{\ket{2}}^{B}$. However, 
\[
\mu_{\ket{2}}^{B}(\proj{2})=1\notin\unlikely=\bar{\mu}(\proj{2})\textrm{ .}
\]

Before we verify that $\bar{\mu}$ is a quantum interval-valued probability
measure, we need a notation of cross product. For $\ket{\psi}=\alpha_{0}\ket{0}+\alpha_{1}\ket{1}+\alpha_{2}\ket{2}=\begin{pmatrix}\alpha_{0}\\
\alpha_{1}\\
\alpha_{2}
\end{pmatrix}$ and $\ket{\phi}=\beta_{0}\ket{0}+\beta_{1}\ket{1}+\beta_{2}\ket{2}=\begin{pmatrix}\beta_{0}\\
\beta_{1}\\
\beta_{2}
\end{pmatrix}$, their cross product is defined as follow:
\[
\ket{\psi\times\phi}=\ket{\psi}\times\ket{\phi}=\begin{vmatrix}\alpha_{1}^{*} & \beta_{1}^{*}\\
\alpha_{2}^{*} & \beta_{2}^{*}
\end{vmatrix}\ket{0}+\begin{vmatrix}\alpha_{2}^{*} & \beta_{2}^{*}\\
\alpha_{0}^{*} & \beta_{0}^{*}
\end{vmatrix}\ket{1}+\begin{vmatrix}\alpha_{0}^{*} & \beta_{0}^{*}\\
\alpha_{1}^{*} & \beta_{1}^{*}
\end{vmatrix}\ket{2}\textrm{ ,}
\]
where $\begin{vmatrix}\alpha & \beta\\
\alpha' & \beta'
\end{vmatrix}=\alpha\beta'-\alpha'\beta$. Notice that we put the complex conjugate in the cross product, so
that the result of cross product is orthogonal to $\ket{\psi}$ and
$\ket{\phi}$. 
\begin{eqnarray*}
\ip{\psi\times\phi}{\psi} & = & \left(\alpha_{1}\beta_{2}-\alpha_{2}\beta_{1}\right)\alpha_{0}+\left(\alpha_{2}\beta_{0}-\alpha_{0}\beta_{2}\right)\alpha_{1}+\left(\alpha_{0}\beta_{1}-\alpha_{1}\beta_{0}\right)\alpha_{2}=0\textrm{ ,}\\
\ip{\psi\times\phi}{\phi} & = & \left(\alpha_{1}\beta_{2}-\alpha_{2}\beta_{1}\right)\beta_{0}+\left(\alpha_{2}\beta_{0}-\alpha_{0}\beta_{2}\right)\beta_{1}+\left(\alpha_{0}\beta_{1}-\alpha_{1}\beta_{0}\right)\beta_{2}=0\textrm{ .}
\end{eqnarray*}
Also, even if $\ket{\psi}$ and $\ket{\phi}$ are normalized, their
cross product~$\ket{\psi}\times\ket{\phi}$ need not be normalized
as usual.

Similar to example~\ref{ex:three-dimensional-three-value}, to verify
$\bar{\mu}$ is a quantum interval-valued probability measure, we
only need to verify equation~(\ref{eq:QuantumInterval-valuedProbability-Inclusion}),
and it is sufficient to check 
\begin{equation}
\begin{aligned}\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi_{0}}\right) & \subseteq\bar{\mu}\left(\proj{\psi_{1}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{\psi_{2}}\right)\\
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi_{1}}\right) & \subseteq\bar{\mu}\left(\proj{\psi_{2}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{\psi_{0}}\right)\\
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi_{2}}\right) & \subseteq\bar{\mu}\left(\proj{\psi_{0}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{\psi_{1}}\right)
\end{aligned}
\label{eq:non-additive-vectors}
\end{equation}
for every orthonormal basis~$\mathcal{B}=\left\{ \ket{\psi_{0}},\ket{\psi_{1}},\ket{\psi_{2}}\right\} $.
We are going to enumerate all possible orthonormal bases to verify
equation~(\ref{eq:non-additive-vectors}), and case 1 and 2 are illustrated
in figure~\ref{fig:three-dimensional-4-value}. 
\begin{figure}
\includegraphics{\string"Three-dimensional quantum 4-value interval-valued probability measure\string".pdf}\caption{\label{fig:three-dimensional-4-value}This figure illustrates case
1 and 2 in $\mathbb{R}^{3}$ in example~\ref{ex:three-dimensional-4-value}.
The red and green dotted vector are $\ket{0}$ and $\ket{\ps}$ respectively.
All possible vectors of $\ket{0_{\theta,\gamma}^{\perp}}$ and $\ket{\ps_{\theta,\gamma}^{\perp}}$
are drawn in the red and green circles, respectively. Within the circles,
a vector~$\ket{\psi}$ in dotted part means $\bar{\mu}(\proj{\psi})=\likely$;
otherwise, $\bar{\mu}(\proj{\psi})=\unlikely$. The gray vector is
a generic vector~$\ket{0_{\theta,\gamma}^{\perp}}$, and the red
and green solid vectors are normalized $\ket{0}\times\ket{0_{\theta,\gamma}^{\perp}}$
and $\ket{\ps}\times\ket{0_{\theta,\gamma}^{\perp}}$, respectively.}
\end{figure}

\begin{enumerate}
\item When $\ket{\psi_{0}}$ is $\ket{0}$, then either $\ket{\psi_{1}}$
or $\ket{\psi_{2}}$ must be $\ket{0_{\theta,\gamma}^{\perp}}$ for
some $\theta$ and $\gamma$. Because of 
\[
\ket{0}\times\ket{0_{\theta,\gamma}^{\perp}}=\begin{pmatrix}1\\
0\\
0
\end{pmatrix}\times\begin{pmatrix}0\\
\rme^{\rmi\gamma}\sin\theta\\
\cos\theta
\end{pmatrix}=\begin{pmatrix}0\\
-\cos\theta\\
\rme^{-\rmi\gamma}\sin\theta
\end{pmatrix}=\rme^{-\rmi\gamma}\begin{pmatrix}0\\
\rme^{\rmi\left(\pi+\gamma\right)}\sin\left(\frac{\pi}{2}-\theta\right)\\
\cos\left(\frac{\pi}{2}-\theta\right)
\end{pmatrix}=\rme^{-\rmi\gamma}\ket{0_{\frac{\pi}{2}-\theta,\pi+\gamma}^{\perp}}
\]
the pair $\ket{\psi_{1}}=\ket{0_{\theta,\gamma}^{\perp}}$ and $\ket{\psi_{2}}=\ket{0_{\frac{\pi}{2}-\theta,\pi+\gamma}^{\perp}}$
for ($0<\theta<\frac{\pi}{2}$ and $0\le\gamma<\pi$) or $\theta=0$
enumerate all possible orthonormal bases. Equation~(\ref{eq:non-additive-vectors})
can then be verified as follow. 
\begin{equation}
\begin{aligned}\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{0}\right)=\necess & \subseteq\likely\tensor*[_{\mathscr{I}}]{+}{}\unlikely=\bar{\mu}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{0_{\frac{\pi}{2}-\theta,\pi+\gamma}^{\perp}}\right)\\
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{0_{\theta,\gamma}^{\perp}}\right)=\unlikely & =\unlikely\tensor*[_{\mathscr{I}}]{+}{}\imposs=\bar{\mu}\left(\proj{0_{\frac{\pi}{2}-\theta,\pi+\gamma}^{\perp}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{0}\right)\\
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{0_{\frac{\pi}{2}-\theta,\pi+\gamma}^{\perp}}\right)=\likely & =\imposs\tensor*[_{\mathscr{I}}]{+}{}\likely=\bar{\mu}\left(\proj{0}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)
\end{aligned}
\label{eq:case1}
\end{equation}
Similarly, when $\ket{\psi_{0}}$ is $\ket{\ps}$, equation~(\ref{eq:non-additive-vectors})
holds. 
\item When $\ket{0}\notin\mathcal{B}$ and $\ket{\psi_{0}}=\ket{0_{\theta,\gamma}^{\perp}}$
for $0\le\theta<\frac{\pi}{2}$ and $0\le\gamma<\pi$, we want to
prove $\bar{\mu}\left(\proj{\psi_{1}}\right)=\bar{\mu}\left(\proj{\psi_{2}}\right)=\unlikely$.
Then, we have 
\begin{equation}
\begin{aligned}\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{0_{\theta,\gamma}^{\perp}}\right)=\unlikely & \subseteq\unlikely\tensor*[_{\mathscr{I}}]{+}{}\unlikely=\bar{\mu}\left(\proj{\psi_{1}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{\psi_{2}}\right)\\
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi_{1}}\right)=\likely & \subseteq\unlikely\tensor*[_{\mathscr{I}}]{+}{}\likely=\bar{\mu}\left(\proj{\psi_{2}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)
\end{aligned}
\label{eq:case2}
\end{equation}
In order to verify $\bar{\mu}\left(\proj{\psi_{1}}\right)=\bar{\mu}\left(\proj{\psi_{2}}\right)=\unlikely$,
it is sufficient to prove that if $\ket{\ps_{\theta',\gamma'}^{\perp}}\in\mathcal{B}$,
then ($0<\theta'<\frac{\pi}{2}$ and $\pi\le\gamma'<2\pi$) or $\theta'=\frac{\pi}{2}$.
Recall $\ip{\ps}{\ps_{\theta',\gamma'}^{\perp}}=0$. Hence, if $\ket{\ps_{\theta',\gamma'}^{\perp}}\in\mathcal{B}$,
we have $\ket{\ps_{\theta',\gamma'}^{\perp}}$ parallel to $\ket{\ps}\times\ket{0_{\theta,\gamma}^{\perp}}$.
\begin{equation}
\ket{\ps}\times\ket{0_{\theta,\gamma}^{\perp}}=\begin{pmatrix}1\\
1\\
0
\end{pmatrix}\times\begin{pmatrix}0\\
\rme^{\rmi\gamma}\sin\theta\\
\cos\theta
\end{pmatrix}=\begin{pmatrix}\cos\theta\\
-\cos\theta\\
\rme^{-\rmi\gamma}\sin\theta
\end{pmatrix}=\rme^{-\rmi\gamma}\begin{pmatrix}-\rme^{\rmi\left(\pi+\gamma\right)}\sin\left(\frac{\pi}{2}-\theta\right)\\
\rme^{\rmi\left(\pi+\gamma\right)}\sin\left(\frac{\pi}{2}-\theta\right)\\
\cos\left(\frac{\pi}{2}-\theta\right)
\end{pmatrix}=\sqrt{2}\rme^{-\rmi\gamma}\ket{\ps_{\frac{\pi}{2}-\theta,\pi+\gamma}^{\perp}}\label{eq:case2-cross}
\end{equation}
Because of $0\le\theta<\frac{\pi}{2}$ and $0\le\gamma<\pi$, we have
($0<\theta'=\frac{\pi}{2}-\theta<\frac{\pi}{2}$ and $\pi\le\gamma'=\pi+\gamma<2\pi$)
or $\theta'=\frac{\pi}{2}-\theta=\frac{\pi}{2}$. Similarly, when
$\ket{\psi_{0}}$ is $\ket{\ps_{\theta,\gamma}^{\perp}}$ and $\ket{\ps}\notin\mathcal{B}$,
equation~(\ref{eq:non-additive-vectors}) holds. 
\item Finally, when $\ket{0}\notin\mathcal{B}$, $\ket{\ps}\notin\mathcal{B}$,
$\ket{0_{\theta,\gamma}^{\perp}}\notin\mathcal{B}$, and $\ket{\ps_{\theta,\gamma}^{\perp}}\notin\mathcal{B}$
for $0\le\theta<\frac{\pi}{2}$ and $0\le\gamma<\pi$, i.e., the ``otherwise''
case. Then, equation~(\ref{eq:non-additive-vectors}) can easily
be verified. 
\begin{equation}
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi_{0}}\right)=\likely\subseteq\unlikely\tensor*[_{\mathscr{I}}]{+}{}\unlikely=\bar{\mu}\left(\proj{\psi_{1}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{\psi_{2}}\right)\label{eq:case3}
\end{equation}
\end{enumerate}
\footnote{\yutsung{This example is a little bit too long to human-verify, and
we need more examples. Maybe we need to write a program for further
verification.}}\qed\end{example}

The following example verifies example~\ref{ex:three-dimensional-4-value}
cannot hold with more precise intervals.

\begin{example}[Three-dimensional quantum 4-value interval-valued
probability measure (Continue)]\label{ex:three-dimensional-4-value-1}
Consider $\mathscr{I}=\left\{ \imposs,\left[l_{1},r_{1}\right],\left[1\tensor*[_{\mathbb{R}}]{-}{}r_{1},1\tensor*[_{\mathbb{R}}]{-}{}l_{1}\right],\necess\right\} $,
where $\left[l_{1},r_{1}\right]$ and $\left[1\tensor*[_{\mathbb{R}}]{-}{}r_{1},1\tensor*[_{\mathbb{R}}]{-}{}l_{1}\right]$
will be used to replace every $\unlikely$ and $\likely$ in example~\ref{ex:three-dimensional-4-value},
respectively. The bound of $l_{1}$ and $r_{1}$ can then be estimated
by the equations corresponding to equation~(\ref{eq:case1}), (\ref{eq:case2}),
and (\ref{eq:case3}).

When we replace $\unlikely$ and $\likely$ by $\left[l_{1},r_{1}\right]$
and $\left[1\tensor*[_{\mathbb{R}}]{-}{}r_{1},1\tensor*[_{\mathbb{R}}]{-}{}l_{1}\right]$,
equation~(\ref{eq:case1}) will become

\begin{equation}
\begin{aligned}\necess & \subseteq\left[1\tensor*[_{\mathbb{R}}]{-}{}r_{1},1\tensor*[_{\mathbb{R}}]{-}{}l_{1}\right]\tensor*[_{\mathscr{I}}]{+}{}\left[l_{1},r_{1}\right]\\
\left[l_{1},r_{1}\right] & \subseteq\left[l_{1},r_{1}\right]\tensor*[_{\mathscr{I}}]{+}{}\imposs\\
\left[1\tensor*[_{\mathbb{R}}]{-}{}r_{1},1\tensor*[_{\mathbb{R}}]{-}{}l_{1}\right] & \subseteq\imposs\tensor*[_{\mathscr{I}}]{+}{}\left[1\tensor*[_{\mathbb{R}}]{-}{}r_{1},1\tensor*[_{\mathbb{R}}]{-}{}l_{1}\right]
\end{aligned}
\label{eq:case1-1}
\end{equation}
which are all tautologies. Equation~(\ref{eq:case2}) will become
\begin{equation}
\begin{aligned}\left[l_{1},r_{1}\right] & \subseteq\left[l_{1},r_{1}\right]\tensor*[_{\mathscr{I}}]{+}{}\left[l_{1},r_{1}\right]\\
\left[1\tensor*[_{\mathbb{R}}]{-}{}r_{1},1\tensor*[_{\mathbb{R}}]{-}{}l_{1}\right] & \subseteq\left[l_{1},r_{1}\right]\tensor*[_{\mathscr{I}}]{+}{}\left[1\tensor*[_{\mathbb{R}}]{-}{}r_{1},1\tensor*[_{\mathbb{R}}]{-}{}l_{1}\right]
\end{aligned}
\label{eq:case2-1}
\end{equation}
which implies $l_{1}=0$. Finally, equation~(\ref{eq:case3}) will
become
\begin{equation}
\left[1\tensor*[_{\mathbb{R}}]{-}{}r_{1},1\tensor*[_{\mathbb{R}}]{-}{}l_{1}\right]\subseteq\left[l_{1},r_{1}\right]\tensor*[_{\mathscr{I}}]{+}{}\left[l_{1},r_{1}\right]\label{eq:case3-1}
\end{equation}
which implies $\frac{1}{2}\le r_{1}\le1$. Therefore, $l_{1}=0$ and
$r_{1}=\frac{1}{2}$ provides the most precise intervals which make
example~\ref{ex:three-dimensional-4-value} work, and in this case,
the intervals are $\left\{ \imposs,\unlikely,\likely,\necess\right\} $.\qed\end{example}

Although it is easier to deduce an interval-valued probability measure
has an empty core by considering the state mapped to $\imposs$ as
in example~\ref{ex:three-dimensional-three-value} and \ref{ex:three-dimensional-4-value},
it is not the only way to prove an interval-valued probability measure
has an empty core. The following family of interval-valued probability
measures satisfy that $\bar{\mu}\left(P\right)=\imposs$ if and only
if $P=\mathbb{0}$, but we can still prove them have empty cores.

\begin{example}{[}Another three-dimensional quantum 6-value interval-valued
probability measure{]}\label{ex:three-dimensional-6-value} Given
$r_{0}\in\left[\frac{1}{3},\frac{1}{2}\right]$, we are going to construct
an interval-valued probability measures for each $r_{0}$, and it
has an empty core whenever $r_{0}\in\left(\frac{5+2\sqrt{2}}{17},\frac{1}{2}\right]$. 

Consider\emph{ }$\mathscr{I}=\left\{ \imposs,\necess,\unlikely_{0},\likely_{0},\unlikely_{1},\likely_{1}\right\} $,
where $\unlikely_{0}=\left[1-2r_{0},r_{0}\right]$, $\likely_{0}=\left[1-r_{0},2r_{0}\right]$,
$\unlikely_{1}=\left[2-4r_{0},1-r_{0}\right]$, and $\likely_{1}=\left[r_{0},4r_{0}-1\right]$.
This is well-defined since the left-end is less or equal to the right-end
whenever $r_{0}\ge\frac{1}{3}$. If $\ket{0}$, $\ket{1}$, $\ket{2}$,
$\ket{\ps}$, $\ket{\ms}$, $\ket{0_{\theta,\gamma}^{\perp}}$, and
$\ket{\ps_{\theta,\gamma}^{\perp}}$ are defined as in example~\ref{ex:three-dimensional-4-value},
a quantum $\mathscr{I}$-interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$
can be defined as follow: 
\begin{enumerate}
\item $\bar{\mu}(\mathbb{0})=\imposs$ and $\bar{\mu}(\mathbb{1})=\necess.$
\item For the dotted half of the red and green circles in figure~\ref{fig:three-dimensional-4-value},
i.e., whenever $0\le\theta<\frac{\pi}{2}$ and $0\le\gamma<\pi$,
we define 
\begin{eqnarray*}
\bar{\mu}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)=\bar{\mu}\left(\proj{\ps_{\theta,\gamma}^{\perp}}\right) & = & \likely_{1}\\
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{0_{\theta,\gamma}^{\perp}}\right)=\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\ps_{\theta,\gamma}^{\perp}}\right) & = & \unlikely_{1}\textrm{ .}
\end{eqnarray*}
\item Otherwise, $\bar{\mu}(\proj{\psi})=\unlikely_{0}$ and $\bar{\mu}(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi})=\likely_{0}$. 
\end{enumerate}
We now prove $\bar{\mu}$ has an empty core by contradiction. Assume
there is a real-valued probability measure satisfying $\mu_{\rho}^{B}(P)\in\bar{\mu}(P)$
for all $P\in\events$. We first claim that for all $0\le\theta<\frac{\pi}{2}$
and $0\le\gamma<\pi$, we have $\mu_{\rho}^{B}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)=r_{0}$.
On one hand, $\mu_{\rho}^{B}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)\in\bar{\mu}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)=\left[r_{0},4r_{0}-1\right]$
implies $\mu_{\rho}^{B}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)\ge r_{0}$.
On the other hand, by figure~\ref{fig:three-dimensional-4-value},
it is easy to see for any neighborhood of $\ket{0_{\theta,\gamma}^{\perp}}$,
these is always a state~$\ket{\psi}$ such that $\mu_{\rho}^{B}\left(\proj{\psi}\right)\in\bar{\mu}\left(\proj{\psi}\right)=\unlikely_{0}$,
i.e., $\mu_{\rho}^{B}\left(\proj{\psi}\right)\le r_{0}$. Because
$\mu_{\rho}^{B}$ is continuous, we have $\mu_{\rho}^{B}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)=r_{0}$.
Similarly, $\mu_{\rho}^{B}\left(\proj{\ps_{\theta,\gamma}^{\perp}}\right)=r_{0}$.

Consider a general density matrix~ $\rho=\tensor*[_{\mathscr{O}}]{\sum}{_{i=0}^{2}}\tensor*[_{\mathscr{O}}]{\sum}{_{j=0}^{2}}r_{ij}\ket{i}\bra{j}$.
Then, we have 
\begin{eqnarray*}
r_{0} & = & \mu_{\rho}^{B}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)\\
 & \stackrel{\left(\ref{BornRule.mixed}\right)}{=} & \Tr\left(\rho\proj{0_{\theta,\gamma}^{\perp}}\right)\\
 & = & \Tr\left(\left(\sideset{_{\mathscr{O}}}{_{i=0}^{2}}\sum\sideset{_{\mathscr{O}}}{_{j=0}^{2}}\sum r_{ij}\ket{i}\bra{j}\right)\left(\rme^{\rmi\gamma}\sin\theta\ket{1}+\cos\theta\ket{2}\right)\left(\rme^{\rmi\gamma}\sin\theta\ket{1}+\cos\theta\ket{2}\right)^{\dagger}\right)\\
 & = & \Tr\left(\sideset{_{\mathscr{O}}}{_{i=0}^{2}}\sum\left(r_{i1}\rme^{\rmi\gamma}\sin\theta+r_{i2}\cos\theta\right)\ket{i}\left(\rme^{-\rmi\gamma}\sin\theta\bra{1}+\cos\theta\bra{2}\right)\right)\\
 & = & r_{11}\sin^{2}\theta+r_{12}\cos\theta\rme^{-\rmi\gamma}\sin\theta+r_{21}\rme^{\rmi\gamma}\sin\theta\cos\theta+r_{22}\cos^{2}\theta
\end{eqnarray*}
Because the above equation holds for all $0\le\theta<\frac{\pi}{2}$
and $0\le\gamma<\pi$, we must have $r_{12}=r_{21}=0$ and $r_{11}=r_{22}=r_{0}$.
We also have
\begin{eqnarray*}
r_{0} & = & \mu_{\rho}^{B}\left(\proj{\ps_{\theta,\gamma}^{\perp}}\right)\\
 & \stackrel{\left(\ref{BornRule.mixed}\right)}{=} & \Tr\left(\rho\proj{\ps_{\theta,\gamma}^{\perp}}\right)\\
 & = & \Tr\left(\left(\sideset{_{\mathscr{O}}}{_{i=0}^{2}}\sum\sideset{_{\mathscr{O}}}{_{j=0}^{2}}\sum r_{ij}\ket{i}\bra{j}\right)\left(-\rme^{\rmi\gamma}\sin\theta\ket{\ms}+\cos\theta\ket{2}\right)\left(-\rme^{\rmi\gamma}\sin\theta\ket{\ms}+\cos\theta\ket{2}\right)^{\dagger}\right)\\
 & = & \Tr\left(\sideset{_{\mathscr{O}}}{_{i=0}^{2}}\sum\left(-\frac{r_{i0}\rme^{\rmi\gamma}\sin\theta}{\sqrt{2}}+\frac{r_{i1}\rme^{\rmi\gamma}\sin\theta}{\sqrt{2}}+r_{i2}\cos\theta\right)\ket{i}\left(-\frac{\rme^{-\rmi\gamma}\sin\theta}{\sqrt{2}}\bra{0}+\frac{\rme^{-\rmi\gamma}\sin\theta}{\sqrt{2}}\bra{1}+\cos\theta\bra{2}\right)\right)\\
 & = & -\frac{\rme^{-\rmi\gamma}\sin\theta}{\sqrt{2}}\left(-\frac{r_{00}\rme^{\rmi\gamma}\sin\theta}{\sqrt{2}}+\frac{r_{01}\rme^{\rmi\gamma}\sin\theta}{\sqrt{2}}+r_{02}\cos\theta\right)\\
 &  & +\frac{\rme^{-\rmi\gamma}\sin\theta}{\sqrt{2}}\left(-\frac{r_{10}\rme^{\rmi\gamma}\sin\theta}{\sqrt{2}}+\frac{r_{0}\rme^{\rmi\gamma}\sin\theta}{\sqrt{2}}\right)+\cos\theta\left(-\frac{r_{20}\rme^{\rmi\gamma}\sin\theta}{\sqrt{2}}+r_{0}\cos\theta\right)\\
 & = & \left(\frac{r_{00}\sin^{2}\theta}{2}-\frac{r_{01}\sin^{2}\theta}{2}-\frac{r_{02}\rme^{-\rmi\gamma}\sin\theta\cos\theta}{\sqrt{2}}\right)+\left(-\frac{r_{10}\sin^{2}\theta}{2}+\frac{r_{0}\sin^{2}\theta}{2}\right)+\left(-\frac{r_{20}\rme^{\rmi\gamma}\cos\theta\sin\theta}{\sqrt{2}}+r_{0}\cos^{2}\theta\right)\\
 & = & \left(\frac{r_{00}}{2}-\frac{r_{01}}{2}-\frac{r_{10}}{2}+\frac{r_{0}}{2}\right)\sin^{2}\theta+r_{0}\cos^{2}\theta-\left(r_{02}\rme^{-\rmi\gamma}+r_{20}\rme^{\rmi\gamma}\right)\frac{\cos\theta\sin\theta}{\sqrt{2}}
\end{eqnarray*}
Because of the same reason, we have $r_{02}=r_{20}=0$ and $r_{00}=r_{0}+r_{01}+r_{10}$.
Moreover, because $\rho$ is a density matrix, we should have $\rho^{\dagger}=\rho$,
$\Tr\left(\rho\right)=1$, and $\rho$ is a positive operator~\cite{544199}.
These implies that we can express $r_{01}=a+\rmi b$ and $r_{10}=a-\rmi b$
for some $a$, $b\in\mathbb{R}$. After plugging into $\Tr\left(\rho\right)=1$,
we have $r_{00}=1-2r_{0}$, $r_{01}=\frac{1}{2}-\frac{3}{2}r_{0}+\rmi b$,
and $r_{01}=\frac{1}{2}-\frac{3}{2}r_{0}-\rmi b$. Next, since $\rho$
is a positive operator, $\rho$ should have all non-negative eigenvalues,
which are the solutions of the characteristic polynomial
\begin{eqnarray*}
0 & = & \det\left(\rho-\lambda\mathbb{1}\right)\\
 & = & \begin{vmatrix}1-2r_{0}-\lambda & \frac{1}{2}-\frac{3}{2}r_{0}+\rmi b & 0\\
\frac{1}{2}-\frac{3}{2}r_{0}-\rmi b & r_{0}-\lambda & 0\\
0 & 0 & r_{0}-\lambda
\end{vmatrix}\\
 & = & \left(r_{0}-\lambda\right)\left[\left(1-2r_{0}-\lambda\right)\left(r_{0}-\lambda\right)-\left(\frac{1}{2}-\frac{3}{2}r_{0}+\rmi b\right)\left(\frac{1}{2}-\frac{3}{2}r_{0}-\rmi b\right)\right]\\
 & = & \left(r_{0}-\lambda\right)\left[\lambda^{2}-\left(1-r_{0}\right)\lambda+r_{0}-2r_{0}^{2}-\frac{1}{4}+\frac{3}{2}r_{0}-\frac{9}{4}r_{0}^{2}-b^{2}\right]\\
 & = & \left(r_{0}-\lambda\right)\left[\lambda^{2}-\left(1-r_{0}\right)\lambda-\frac{1}{4}+\frac{5}{2}r_{0}-\frac{17}{4}r_{0}^{2}-b^{2}\right]
\end{eqnarray*}
Hence, the solutions of $\lambda$ are $r_{0}$ and $1-r_{0}\pm\sqrt{\left(1-r_{0}\right)^{2}-4\cdot1\cdot\left(-\frac{1}{4}+\frac{5}{2}r_{0}-\frac{17}{4}r_{0}^{2}-b^{2}\right)}$.
If they are all non-negative, we want 
\begin{eqnarray*}
0 & \ge & -4\cdot1\cdot\left(-\frac{1}{4}+\frac{5}{2}r_{0}-\frac{17}{4}r_{0}^{2}-b^{2}\right)\\
 & = & 1-10r_{0}+17r_{0}^{2}+4b^{2}\\
 & = & 17\left(r_{0}-\frac{5+\sqrt{25-17\cdot\left(4b^{2}+1\right)}}{17}\right)\left(r_{0}-\frac{5-\sqrt{25-17\cdot\left(4b^{2}+1\right)}}{17}\right)
\end{eqnarray*}
Therefore, when $r_{0}>\frac{5+2\sqrt{2}}{17}$ or $r_{0}<\frac{5-2\sqrt{2}}{17}$,
$\rho$ always has a negative eigenvalue for all~$b\in\mathbb{R}$.
This contradiction implies $\bar{\mu}$ has an empty core when $r_{0}\in\left(\frac{5+2\sqrt{2}}{17},\frac{1}{2}\right]$.

Similar to example~\ref{ex:three-dimensional-4-value}, to verify
$\bar{\mu}$ is a quantum interval-valued probability measure for
$r_{0}\in\left[\frac{1}{3},\frac{1}{2}\right]$, we are going to enumerate
all possible orthonormal bases~$\mathcal{B}=\left\{ \ket{\psi_{0}},\ket{\psi_{1}},\ket{\psi_{2}}\right\} $
to verify equation~(\ref{eq:non-additive-vectors}).
\begin{enumerate}
\item When $\ket{\psi_{0}}=\ket{0_{\theta,\gamma}^{\perp}}$ for $0\le\theta<\frac{\pi}{2}$
and $0\le\gamma<\pi$, we want to prove $\bar{\mu}\left(\proj{\psi_{1}}\right)=\bar{\mu}\left(\proj{\psi_{2}}\right)=\unlikely_{0}$.
Then, we have 
\begin{equation}
\begin{aligned} & \bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{0_{\theta,\gamma}^{\perp}}\right)=\unlikely_{1}=\left[2-4r_{0},1-r_{0}\right]\\
\subseteq & \left[2-4r_{0},2r_{0}\right]=\unlikely_{0}\tensor*[_{\mathscr{I}}]{+}{}\unlikely_{0}=\bar{\mu}\left(\proj{\psi_{1}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{\psi_{2}}\right)\\
 & \bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi_{1}}\right)=\likely_{0}=\left[1-r_{0},2r_{0}\right]\\
\subseteq & \left[1-r_{0},5r_{0}-1\right]=\unlikely_{0}\tensor*[_{\mathscr{I}}]{+}{}\likely_{1}=\bar{\mu}\left(\proj{\psi_{2}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{0_{\theta,\gamma}^{\perp}}\right)
\end{aligned}
\label{eq:case2-2}
\end{equation}
In order to verify $\bar{\mu}\left(\proj{\psi_{1}}\right)=\bar{\mu}\left(\proj{\psi_{2}}\right)=\unlikely$,
it is sufficient to prove that if $\ket{\ps_{\theta',\gamma'}^{\perp}}\in\mathcal{B}$,
then ($0<\theta'<\frac{\pi}{2}$ and $\pi\le\gamma'<2\pi$) or $\theta'=\frac{\pi}{2}$.
This is easy because of equation~(\ref{eq:case2-cross}). Similarly,
when $\ket{\psi_{0}}$ is $\ket{\ps_{\theta,\gamma}^{\perp}}$, equation~(\ref{eq:non-additive-vectors})
holds. 
\item Otherwise, when $\ket{0_{\theta,\gamma}^{\perp}}\notin\mathcal{B}$,
and $\ket{\ps_{\theta,\gamma}^{\perp}}\notin\mathcal{B}$ for $0\le\theta<\frac{\pi}{2}$
and $0\le\gamma<\pi$, equation~(\ref{eq:non-additive-vectors})
can easily be verified. 
\begin{equation}
\begin{aligned} & \bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi_{0}}\right)=\likely_{0}=\left[1-r_{0},2r_{0}\right]\\
\subseteq & \left[2-4r_{0},2r_{0}\right]=\unlikely_{0}\tensor*[_{\mathscr{I}}]{+}{}\unlikely_{0}=\bar{\mu}\left(\proj{\psi_{1}}\right)\tensor*[_{\mathscr{I}}]{+}{}\bar{\mu}\left(\proj{\psi_{2}}\right)
\end{aligned}
\label{eq:case3-2}
\end{equation}
\end{enumerate}
\qed\end{example}

\yutsung{Since we have a family of interval-valued probability measures
with empty cores, we need to find a more systematically way to how
close these interval-valued probability measures to the Born rule.
Need to check whether people defined the distance between games or
not... Possible candidates are sup-norm or total variation distance(?)}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Probability space (restricted to finite sample spaces)

* Sample space Ω (arbitrary finite non-empty set)
* Set of events F: pick 2^Ω
* Probability measure (real-valued): P : F → [0,1] such that:
  * For any collection of pairwise disjoint Aᵢ ∈ F, we have P ( ∪ᵢ Aᵢ) = ∑ᵢ P(Aᵢ) 
  * P(Ω) = 1


Set-value probability measures. Change the last bullet to:
* P : F → 2^(ℝⁿ) such that: ...

%%%%%
\subsection{Sample Space $\Omega$} 

In this paper, we will only consider \textbf{finite} sample spaces. We
therefore define a sample space $\Omega$ as a non-empty finite set.

\begin{example}[A Classical Sample Space.]
Consider an experiment that tosses three coins. A possible outcome of
the experiment is $HHT$ which means that the first and second coins
landed with ``heads'' as the face-up side and that the third coin
landed with ``tails'' as the face-up side. There are clearly a total
of eight possible outcomes, and this collection constitutes the sample
space:
\[
\Omega_C = \{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT \}
\]
\end{example}

\begin{example}[A Quantum Sample Space.]
Consider a quantum system composed of three electrons. By the
postulates of quantum mechanics, an experiment designed to measure
whether the spin of each electron along the $x$ axis is left ($L$) or
right ($R$) can only result in one of eight outcomes:
\[
\Omega_H = \{ LLL, LLR, LRL, LRR, RLL, RLR, RRL, RRR \}
\]
\end{example}

%%%%%
\subsection{Events $\mathcal{F}$} 

The space of events $\mathcal{F}$ associated with a sample space
$\Omega$ is $2^\Omega$, the powerset of $\Omega$. In other words,
every subset of $\Omega$ is a possible event.

\begin{example}[Some classical events.] 
The following are events associated with $\Omega_C$:
\begin{itemize}
\item $E_0$, exactly zero coins are $H$, is the set $\{ TTT \}$.
\item $E_1$, exactly one coin is $H$, is the set $\{ HTT, THT, TTH \}$. 
\item $E_2$, exactly two coins are $H$, is the set $\{ HHT, HTH, THH \}$.
\item $E_3$, exactly three coins are $H$, is the set $\{ HHH \}$. 
\item $E_{>0}$, at least one coin is $H$, is the set $\{ HHH, HHT, HTH, HTT, THH, THT, TTH \}$. 
\end{itemize}
As the examples illustrate, events are \emph{indirect} questions built from elementary elements of the sample space using logical connectives. Also
note that some events may be disjoint and that some events may be
expressed as combinations of other events. For example, we have
$E_{>0} = E_1 \cup E_2 \cup E_3$ and each of these four events is
disjoint from event $E_0$.
\end{example}

\begin{example}[Some quantum events.] 
The following are events associated with $\Omega_H$:
\begin{itemize}
\item $F_0$, exactly zero electrons are spinning $L$, is the set $\{ RRR \}$.
\item $F_1$, exactly one electron is spinning $L$, is the set $\{ LRR, RLR, RRL \}$. 
\item $F_2$, exactly two electrons are spinning $L$, is the set $\{ LLR, LRL, RLL \}$.
\item $F_3$, exactly three electrons are spinning $L$, is the set $\{ LLL \}$. 
\item $F_{>0}$, at least one electron is spinning $L$, is the set $\{ LLL, LLR, LRL, LRR, RLL, RLR, RRL \}$. 
\end{itemize}
As the examples illustrate, quantum events are, at first glance,
similar to classical events. There are however some subtle
differences that we point out in the next section.
\end{example}

%%%%%
\subsection{Measures $\mathbb{P}$} 

The last ingredient of a probability space is a probability measure
$\mathbb{P} : \mathcal{F} \rightarrow [0,1]$ that assigns to each
event a real number in the closed interval $[0,1]$ subject to the
following conditions:
\begin{itemize}
\item $\mathbb{P}(\Omega) = 1$, and 
\item For any collection of pairwise disjoint events $A_i$, we have 
$\mathbb{P}(\bigcup_i A_i) = \Sigma_i ~\mathbb{P}(A_i)$.
\end{itemize}

\begin{example}[Classical probability measure]
There are $2^8$ events associated with $\Omega_C$. A possible probability measure for
these events is:
\[\begin{array}{c}
\mathbb{P}(E) = \left\{ \begin{array}{ll} 
  1 & \mbox{if}~E = \Omega \\
  0 & \mbox{otherwise} 
  \end{array}\right.
\end{array}\]
\yutsung{ Actually, the above $\mathbb{P}$ is not a probability
measure because 
\[
\mathbb{P}\left(\Omega\right)=1\ne0+0=\mathbb{P}\left(E_{0}\right)+\mathbb{P}\left(E_{>0}\right)
\]
} \\
A more interesting measure is defined recursively as follows:
\renewcommand\arraystretch{1.4}
\[\begin{array}{rcl}
\mathbb{P}(\emptyset) &=& 0 \\
\mathbb{P}(\{ HHH \} \cup E) &=& \frac{1}{5} + \mathbb{P}(E) \\
\mathbb{P}(\{ HHT \} \cup E) &=& \mathbb{P}(E) \\
\mathbb{P}(\{ HTH \} \cup E) &=& \frac{3}{10} + \mathbb{P}(E) \\
\mathbb{P}(\{ HTT \} \cup E) &=& \mathbb{P}(E) \\
\mathbb{P}(\{ THH \} \cup E) &=& \frac{1}{5} + \mathbb{P}(E) \\
\mathbb{P}(\{ THT \} \cup E) &=& \mathbb{P}(E) \\
\mathbb{P}(\{ TTH \} \cup E) &=& \frac{3}{10} + \mathbb{P}(E) \\
\mathbb{P}(\{ TTT \} \cup E) &=& \mathbb{P}(E) 
\end{array}\]
\yutsung{Because $\mathbb{P}(\bigcup_{i}A_{i})=\Sigma_{i}~\mathbb{P}(A_{i})$
requires disjoint events, the above formula should write like:
\[
\begin{array}{rcl}
\mathbb{P}(\emptyset) & = & 0\\
\mathbb{P}(\{HHH\}\cup E) & = & \frac{1}{5}+\mathbb{P}(E)\textrm{, if }HHH\notin E\\
\mathbb{P}(\{HHT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }HHT\notin E\\
\mathbb{P}(\{HTH\}\cup E) & = & \frac{3}{10}+\mathbb{P}(E)\textrm{, if }HTH\notin E\\
\mathbb{P}(\{HTT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }HTT\notin E\\
\mathbb{P}(\{THH\}\cup E) & = & \frac{1}{5}+\mathbb{P}(E)\textrm{, if }THH\notin E\\
\mathbb{P}(\{THT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }THT\notin E\\
\mathbb{P}(\{TTH\}\cup E) & = & \frac{3}{10}+\mathbb{P}(E)\textrm{, if }TTH\notin E\\
\mathbb{P}(\{TTT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }TTT\notin E
\end{array}
\]
Or add a sentence ``where the element in the singleton set is not
belong to $E$ for each equation.'' Or write like: 
\[
\begin{array}{rcl}
\mathbb{P}(\{HHH\}) & = & \frac{1}{5}\\
\mathbb{P}(\{HHT\}) & = & 0\\
\mathbb{P}(\{HTH\}) & = & \frac{3}{10}\\
\mathbb{P}(\{HTT\}) & = & 0\\
\mathbb{P}(\{THH\}) & = & \frac{1}{5}\\
\mathbb{P}(\{THT\}) & = & 0\\
\mathbb{P}(\{TTH\}) & = & \frac{3}{10}\\
\mathbb{P}(\{TTT\}) & = & 0\\
\mathbb{P}(E) & = & \sum_{\omega\in E}\mathbb{P}(\left\{ \omega\right\} )
\end{array}
\]
} \\
Because this is a \emph{classical} situation, the probability
assignments can be understood \emph{locally} and
\emph{non-contextually}. In other words, we can reason about each coin
separately and perform experiments on it ignoring the rest of the
context. If we were to perform such experiments we may find that for
the first coin, the probability of either outcome
$H$ or $T$ is $\frac{1}{2}$; for coin two, the probabilities are
skewed a little with the probability of outcome $H$ being
$\frac{2}{5}$ and the probability of outcome $T$ being $\frac{3}{5}$; and that
coin 3 is a fake double-headed coin where the probability of
outcome $H$ is 1 and the probability of outcome $T$ is 0. The reader
may check that these local observations are consistent with the
probability measure above. 
\end{example}

\begin{example}[Quantum probability measure] Like in the classical
case, there are $2^{8}$ events. But as Mermin explains in a simple
example~\cite{MerminPRL1990}, here is a possible probability measure:
\[
\begin{array}{rcl}
\mathbb{P}_{xxx}(\{LLL\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{LLR\}) & = & 0\\
\mathbb{P}_{xxx}(\{LRL\}) & = & 0\\
\mathbb{P}_{xxx}(\{LRR\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{RLL\}) & = & 0\\
\mathbb{P}_{xxx}(\{RLR\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{RRL\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{RRR\}) & = & 0\\
\mathbb{P}_{xxx}(E) & = & \sum_{\omega\in E}\mathbb{P}_{xxx}(\left\{ \omega\right\} )
\end{array}
\]
In contrast with the previous classical example, the event of different
electrons are not independent. More precisely, consider the event
for each electron separately:
\begin{eqnarray*}
F_{1,L} & = & \left\{ LLL,LLR,LRL,LRR\right\} \\
F_{2,L} & = & \left\{ LLL,LLR,RLL,RLR\right\} \\
F_{3,L} & = & \left\{ LLL,LRL,RLL,RRL\right\} 
\end{eqnarray*}
They are not independent means 
\[
\mathbb{P}_{xxx}(F_{1,L}\cap F_{2,L}\cap F_{3,L})=\mathbb{P}_{xxx}(\{LLL\})=\frac{1}{4}\ne\frac{1}{8}=\mathbb{P}_{xxx}(F_{1,L})\mathbb{P}_{xxx}(F_{2,L})\mathbb{P}_{xxx}(F_{3,L})\textrm{ .}
\]


Classical events may also not be independent even if they seems unrelated.
For example, events defined by the temperature is usually not independent
to ones defined by how much Coca-Cola is sold. Another example can
be formulated by tossing three coins as we discussed previously. However,
this time the coins are tossed behind a veil where someone tosses
the coins for you. Because we cannot see how she tosses the coins,
she might actually roll a four-sided tetrahedral die with $\{HHH,HTT,THT,TTH\}$
in its four faces. If $HTT$ is on the downward face, she places $H$,
$T$, and $T$ as the face-up sides of of the three coins by hand,
respectively. Then, she uncovers the veil, and claims she has tossed
the coins. If the coins are tossed in this way, the result of coin-tossing
is correlated, and we will never see $TTT$ no matter how many times
we toss these coins. 

Because we do not know how the spin of an electron is decided, Einstein,
Podolsky, and Rosen (EPR)~\cite{EPR1935} suggested the nature might
give us the probability measure~$\mathbb{P}_{xxx}$ because she rolled
a tetrahedral die or performed other classical and deterministic process
behind the veil. This claim may be convincing if $\mathbb{P}_{xxx}$
is the only probability measure we have, but will lead to a contradiction
if we consider other probability measures as well. Notice that after
the coins are placed by hand and before uncovering the veil, which
side up has already been decided although we do not know. This would
be also true for the quantum probability measure. Because the three
electrons can be spatially separated, and each electron can be measured
along the $x$ axis separately, if the nature rolled a tetrahedral
die, this die should be rolled before the electrons are separated
and measured, and she should know the result of measurement before
we measure the electrons. Let the result of the $j$-th electron measured
along the $x$ axis be $w\left(\sigma_{x}^{j}\right)$. Because 
\[
\mathbb{P}_{xxx}(\{LLR\})=\mathbb{P}_{xxx}(\{LRL\})=\mathbb{P}_{xxx}(\{RLL\})=\mathbb{P}_{xxx}(\{RRR\})=0\textrm{ ,}
\]
we have $w\left(\sigma_{x}^{1}\right)w\left(\sigma_{x}^{2}\right)w\left(\sigma_{x}^{3}\right)\in\{LLL,LRR,RLR,RRL\}$,
i.e., the number of $L$ in $w\left(\sigma_{x}^{1}\right)$, $w\left(\sigma_{x}^{2}\right)$,
and $w\left(\sigma_{x}^{3}\right)$ should be odd. 

The three electrons cannot be measured the spin only along the $x$
axis, but also along the $y$ axis with the result down ($D$) or
up ($U$). We only consider to measure even number of electrons along
the $y$ axis, and the probability measures could be defined by 
\begin{eqnarray*}
\begin{array}{rcl}
\mathbb{P}_{xyy}(\{LDD\}) & = & 0\\
\mathbb{P}_{xyy}(\{LDU\}) & = & \frac{1}{4}\\
\mathbb{P}_{xyy}(\{LUD\}) & = & \frac{1}{4}\\
\mathbb{P}_{xyy}(\{LUU\}) & = & 0\\
\mathbb{P}_{xyy}(\{RDD\}) & = & \frac{1}{4}\\
\mathbb{P}_{xyy}(\{RDU\}) & = & 0\\
\mathbb{P}_{xyy}(\{RUD\}) & = & 0\\
\mathbb{P}_{xyy}(\{RUU\}) & = & \frac{1}{4}
\end{array} & \begin{array}{rcl}
\mathbb{P}_{yxy}(\{DLD\}) & = & 0\\
\mathbb{P}_{yxy}(\{DLU\}) & = & \frac{1}{4}\\
\mathbb{P}_{yxy}(\{DRD\}) & = & \frac{1}{4}\\
\mathbb{P}_{yxy}(\{DRU\}) & = & 0\\
\mathbb{P}_{yxy}(\{ULD\}) & = & \frac{1}{4}\\
\mathbb{P}_{yxy}(\{ULU\}) & = & 0\\
\mathbb{P}_{yxy}(\{URD\}) & = & 0\\
\mathbb{P}_{yxy}(\{URU\}) & = & \frac{1}{4}
\end{array} & \begin{array}{rcl}
\mathbb{P}_{yyx}(\{DDL\}) & = & 0\\
\mathbb{P}_{yyx}(\{DDR\}) & = & \frac{1}{4}\\
\mathbb{P}_{yyx}(\{DUL\}) & = & \frac{1}{4}\\
\mathbb{P}_{yyx}(\{DUR\}) & = & 0\\
\mathbb{P}_{yyx}(\{UDL\}) & = & \frac{1}{4}\\
\mathbb{P}_{yyx}(\{UDR\}) & = & 0\\
\mathbb{P}_{yyx}(\{UUL\}) & = & 0\\
\mathbb{P}_{yyx}(\{UUR\}) & = & \frac{1}{4}
\end{array}
\end{eqnarray*}
with $\mathbb{P}_{ijk}(E)=\sum_{\omega\in E}\mathbb{P}_{ijk}(\left\{ \omega\right\} )$.
Similarly, the nature should predetermine $w\left(\sigma_{x}^{j}\right)$
and $w\left(\sigma_{y}^{j}\right)$ for them. Furthermore, because
she do not know along which axis we are going to measure, she should
predetermine the same $w\left(\sigma_{x}^{j}\right)$ and $w\left(\sigma_{y}^{j}\right)$
for all different probability measures. By the same reason as above,
the number of $L$ or $D$ in $\left\{ w\left(\sigma_{x}^{1}\right),w\left(\sigma_{y}^{2}\right),w\left(\sigma_{y}^{3}\right)\right\} $,
$\left\{ w\left(\sigma_{y}^{1}\right),w\left(\sigma_{x}^{2}\right),w\left(\sigma_{y}^{3}\right)\right\} $,
and $\left\{ w\left(\sigma_{y}^{1}\right),w\left(\sigma_{y}^{2}\right),w\left(\sigma_{x}^{3}\right)\right\} $
should be even. If we look these 9 letters carefully, we can find
that every $w\left(\sigma_{x}^{j}\right)$ appears once and every
$w\left(\sigma_{y}^{j}\right)$ appears twice. Hence, the number of
$L$ in $w\left(\sigma_{x}^{1}\right)$, $w\left(\sigma_{x}^{2}\right)$,
and $w\left(\sigma_{x}^{3}\right)$ should be even. This contradict
to the conclusion in our last paragraph. Therefore, EPR's assumption
is wrong, and it is not always true that the nature can predetermine
the measurement result before we perform the measurement. \end{example}


%%%%
\subsection{Finite Precision of Measurements}

In a laboratory setting or a computational setting, there are neither
uncountable entities nor uncomputable entities. We are thus looking at
alternative probability spaces which do not depend on the real numbers
and revisit the mysteries of quantum mechanics in that
setting. In other words, is it possible that at least part of the quantum mysteries related to probability and measurement are due to the reliance on uncomputable probability values? 

Following previous work on probability, we will replace the
closed interval $[0,1]$ by the \emph{finite set} $S = \{
\textbf{possible}, \textbf{impossible} \}$ and adapt the definition of
probability measure as follows.

A set-valued probability measure $\mathbb{P} : \mathcal{F} \rightarrow
S$ assigns to each event either the tag \textbf{possible} or the tag
\textbf{impossible} subject to the following conditions:
\begin{itemize}
\item $\mathbb{P}(\Omega) = \textbf{possible}$, and 
\item For any collection of pairwise disjoint events $A_i$, we have 
$\mathbb{P}(\bigcup_i A_i) = \textbf{possible}$ if any event $A_i$ is
\textbf{possible} and \textbf{impossible} otherwise. 
\end{itemize}

We begin by reviewing the conventional presentation of classical
probability spaces and then give an alternative formulation that is
``quantum-like'' but still classical. We conclude this section with a
definition of quantum probability spaces given as a modest
generalization of the alternative classical definition. 

%%%%%
\subsection{Conventional Classical Probability Spaces}

Textbook probability
theory~\cite{inun.425605319950101,GrahamKnuthPatashnik1994,rohatgi2011introduction}
is defined using the notions of a \emph{sample space} $\Omega$, a
space of \emph{events}~$\events$, and a \emph{probability
  measure}~$\pmeas$. In this paper, we will only consider
\emph{finite} sample spaces: we therefore define a sample space
$\Omega$ as an arbitrary non-empty finite set and the space of events
$\events$ as, $2^\Omega$, the powerset of $\Omega$. A \emph{probability
measure} is a function $\pmeas : \events \rightarrow [0,1]$ such that:
\begin{itemize}
\item $\pmeas(\Omega) = 1$, and 
\item for a collection of pairwise disjoint events $E_i$, we have
  $\pmeas(\bigcup E_i) = \sum \pmeas(E_i)$. 
\end{itemize}

\begin{example}[Two coin experiment] Consider an experiment that
  tosses two coins. We have four possible outcomes that constitute the
  sample space $\Omega = \{ HH, HT, TH, TT \}$. The event that the
  first coin lands heads up is $\{ HH, HT \}$; the event that the two
  coins land on opposite sides is $\{ HT, TH \}$; the event that at
  least one coin lands tails up is $\{ HT, TH, TT\}$. Depending on the
  assumptions regarding the coins, we can define several probability
  measures. Here is a possible one:
\[\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) &=& 0 \\
\pmeas(\{ HH \}) &=& 1/3 \\
\pmeas(\{ HT \}) &=& 0 \\
\pmeas(\{ TH \}) &=& 2/3 \\
\pmeas(\{ TT \}) &=& 0 \\
\pmeas(\{  HH, HT \}) &=& 1/3 \\
\pmeas(\{  HH, TH \}) &=& 1 \\
\pmeas(\{  HH , TT \}) &=& 1/3 
\end{array} & \begin{array}{rcl}
\pmeas(\{  HT, TH \}) &=& 2/3 \\
\pmeas(\{  HT , TT \}) &=& 0 \\
\pmeas(\{  TH , TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH \}) &=& 1 \\
\pmeas(\{  HH, HT, TT \}) &=& 1/3 \\
\pmeas(\{  HH, TH, TT \}) &=& 1 \\
\pmeas(\{  HT, TH, TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH, TT \}) &=& 1
\end{array}
\end{array}\]
\end{example}

%%%%%
\subsection{Alternative Definition of Classical Probability Spaces}

In the conventional presentation, we have viewed the space of events
$2^\Omega$ are the powerset of $\Omega$. We can equivalently view
$2^\Omega$ as the space of functions from $\Omega$ to the set
$2 = \{0,1\}$. For example, the event $\{HT,TH\}$ is the function $e$
such that:
\[
e (HH) = 0, \quad e (HT) = 1, \quad e (TH) = 1, \quad e (TT) = 0 
\]
We will in fact do a sweeping generalization and view events as
functions from $\Omega$ to $\mathbb{C}$, the set of complex
numbers. This accommodates the previous events such as $e$ and allows
many more events such as event $e'$ below:
\[
e' (HH) = \sqrt{2} + i \sqrt{3}, \quad e' (HT) = 1, \quad e' (TH) = \pi, \quad e' (TT) = 0 
\]
The events are not going to be completely arbitrary functions,
however. We will insist on some conditions:...

This generalization

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conventional Quantum Mechanics}

Attempting to modify the probability measure to be set-valued, while keeping the rest of the mathematical framework of quantum mechanics intact leads to a contradiction. More precisely, it is not possible to maintain infinite precision probability amplitudes in the presence of set-valued probabilities without violating essential aspects of quantum theory. 

\ldots explain and give theorem

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete Quantum Theory}

The next question to ask is therefore whether the infinite precision of probability amplitudes is itself justified. If all measurements are finite and all probabilities are computable, then it is plausible that the internal mathematical representation of quantum states should also be based on countable computable entities. 
