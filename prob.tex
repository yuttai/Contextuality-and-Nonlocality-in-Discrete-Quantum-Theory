\documentclass{article}
\usepackage{fullpage}
\usepackage{url}
\usepackage{bbm}
\usepackage{bbold}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{array}
\usepackage{mathrsfs}
\usepackage[all]{xy}
\usepackage{amsmath}
\usepackage{wesa}
\usepackage[T1]{fontenc}

% \makeatletter
% \newtheoremstyle{indented}
%   {3pt}% space before
%   {3pt}% space after
%   {\addtolength{\@totalleftmargin}{3.5em}
%    \addtolength{\linewidth}{-3.5em}
%    \parshape 1 3.5em \linewidth}% body font
%   {}% indent
%   {\bfseries}% header font
%   {.}% punctuation
%   {.5em}% after theorem header
%   {}% header specification (empty for default)
% \makeatother
% \theoremstyle{indented}
\theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{lemma}{Lemma}
\newcommand{\events}{\ensuremath{\mathcal{E}}}
\newcommand{\qevents}{\ensuremath{\mathcal{E}}}
\newcommand{\pmeas}{\ensuremath{\mu}}
\newcommand{\Hilb}{\mathcal{H}}
\newcommand{\pr}[2]{\langle #1, #2 \rangle}
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle#1|}
\newcommand{\ip}[2]{\langle #1 | #2 \rangle}
\newcommand{\proj}[1]{|#1 \rangle\langle #1 |}
\newcommand{\ps}{\texttt{+}}
\newcommand{\ms}{\texttt{-}}
\newcommand{\poss}{{\mbox{\wesa{possible}}}}
\newcommand{\imposs}{{\mbox{\wesa{impossible}}}}
\newcommand{\likely}{{\mbox{\wesa{likely}}}}
\newcommand{\unlikely}{{\mbox{\wesa{unlikely}}}}
\newcommand{\necess}{{\mbox{\wesa{certain}}}}
\newcommand{\overflow}{{\mbox{\wesa{overflow}}}}
\newcommand{\unknown}{{\mbox{\wesa{unknown}}}}

\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\yutsung}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{purple}{Yu-Tsung says: #1}\end{minipage}}}
\newcommand{\amr}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{green}{Amr says: #1}\end{minipage}}}
\newcommand{\ffzd}[1]{{\mathbb{F}^{d\;*}_{#1}}}
\def\C{{\mathbb{C}}}
\newcommand{\ff}[1]{\mathbb{F}_{#1}}
\newcommand{\melement}[2]{ \langle #1 | #2 | #1 \rangle}
\newcommand{\expect}[2]{ \langle #1 | #2 | #1 \rangle}
\newcommand{\Tr}{\mathop{\mathrm{Tr}}\nolimits}
\newcommand{\gerardo}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{OliveGreen}{Gerardo says: #1}\end{minipage}}}
\newcommand{\andy}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{blue}{Andy says: #1}\end{minipage}}}
\usepackage{tensor}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\rmi}{\mathrm{i}}

\begin{document}
\title{Interval Probability for Fuzzy Quantum Theories}
\author{}
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
  
Fuzzy quantum mechanics:
\begin{itemize}
\item \url{http://cds.cern.ch/record/518511/files/0107054.pdf}
\item \url{http://link.springer.com/chapter/10.1007%2F978-3-642-35644-5_18#page-1}
\item \url{http://link.springer.com/chapter/10.1007%2F978-3-540-93802-6_20#page-1}
\item \url{http://www.du.edu/nsm/departments/mathematics/media/documents/preprints/m0412.pdf}
\item \url{http://www.space-lab.ru/files/pages/PIRT_VII-XII/pages/text/PIRT_X/Bobola.pdf}
\item \url{http://www.vub.ac.be/CLEA/aerts/publications/1993LiptovskyJan.pdf}
\end{itemize}

\noindent Pseudo-randomness:
\begin{itemize}
\item
  \url{https://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Pseudo%20Randomness/How_To_Generate_Cryptographically_Strong_Sequences_Of_Pseudo-Random_Bits.pdf}:
  ``the randomness of an event is relative to a specific model of
  computation with a specified amount of computing resources.''
\item Another version \url{https://pdfs.semanticscholar.org/3e9c/5f6f48d9ef426655dc799e9b287d754e86c1.pdf}
\end{itemize}

%%%%%
\subsection{Plan}

In the remainder of the paper, we consider variations of quantum
probability spaces motivated by computation of numerical quantities in
a world with limited resources:
\begin{itemize}
\item Instead of the Hilbert space $\Hilb$ (constructed over the
  uncountable and uncomputable complex numbers $\mathbb{C}$), we will
  consider variants constructed over finite
  fields~\cite{HansonOrtizSabryEtAl2015,DQT2014,geometry2013}.
\item Instead of real-valued probability measures producing results in
  the uncountable and uncomputable interval $[0,1]$, we will consider
  finite set-valued probability measures~\cite{PuriRalescu1983}.
\end{itemize}
We will then ask if it is possible to construct variants of quantum
probability spaces under these conditions. The main question is
related to the definition of probability measures: is it possible to
still define a probability measure as a function that depends on a
single state? Specifically,
\begin{itemize}
\item given a state $\ket{\psi}$, is there a probability measure
  mapping events to probabilities that only depends on $\ket{\psi}$?
  In the conventional quantum probability space, the answer is yes by
  the Born rule~\cite{Born1983,Mermin2007} and the map is given by:
  $P \mapsto \ip{\psi}{P\psi}$.
\item given a probability measure $\mu$
  mapping each event $P$
  to a probability, is there a \emph{unique} state $\psi$
  such that $\mu(P)
  =
  \ip{\psi}{P\psi}$? In the conventional case, the answer is yes by
  Gleason's
  theorem~\cite{gleason1957,peres1995quantum,Redhead1987-REDINA}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classical Probability Spaces}
  
A \emph{probability space} specifies the necessary conditions for
reasoning coherently about collections of uncertain events.  We review
the conventional presentation of probability spaces and then discuss
the computational resources needed to estimate probabilities.

%%%
\subsection{Real-Valued Probability Spaces}

The conventional definition of a probability
space~\cite{Kolmogorov1950,Shafer1976,Griffiths2003,Swart2013}
builds upon the real numbers. In more detail, a probability space
consists of a \emph{sample space} $\Omega$, a space of
\emph{events}~$\events$, and a \emph{probability measure}~$\pmeas$
mapping events in $\events$ to the real interval $[0,1]$. In this
paper, we will only consider \emph{finite} sets of events: we
therefore restrict our attention to non-empty finite sets $\Omega$ as
the sample space. The space of events $\events$ includes every
possible subset of $\Omega$: it is the powerset~$2^\Omega$. Given the
set of events $\events$, a \emph{probability measure} is a function
$\pmeas : \events \rightarrow [0,1]$ such that:
\begin{itemize}
\item $\pmeas(\Omega) = 1$, and 
\item for a collection $E_{i}$ of pairwise disjoint events, $\pmeas(\bigcup_{i}E_{i})=\tensor*[_{\mathbb{R}}]{\sum}{_{i}}\pmeas(E_{i})$,
where $\tensor*[_{\mathbb{R}}]{\sum}{_{i}}\pmeas(E_{i})$
explicitly specifies $\pmeas(E_{i})\in\mathbb{R}$. Besides $\mathbb{R}$, we will
prepose other symbols later to specify the type of operations, and they may be
dropped when there is no ambiguity.
\end{itemize}

\begin{example}[Two-coins experiment]\label{ex1}
  Consider an experiment that tosses two coins. We have four possible
  outcomes that constitute the sample space
  $\Omega = \{ HH, HT, TH, TT \}$. There are 16 total events including
  for example the event $\{ HH, HT \}$ that the first coin lands heads
  up, the event $\{ HT, TH \}$ that the two coins land on opposite
  sides, and the event $\{ HT, TH, TT\}$ that at least one coin lands
  tails up. Here is a possible probability measure for these events:
\[\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) &=& 0 \\
\pmeas(\{ HH \}) &=& 1/3 \\
\pmeas(\{ HT \}) &=& 0 \\
\pmeas(\{ TH \}) &=& 2/3 \\
\pmeas(\{ TT \}) &=& 0 \\
\pmeas(\{  HH, HT \}) &=& 1/3 \\
\pmeas(\{  HH, TH \}) &=& 1 \\
\pmeas(\{  HH , TT \}) &=& 1/3 
\end{array} & \begin{array}{rcl}
\pmeas(\{  HT, TH \}) &=& 2/3 \\
\pmeas(\{  HT , TT \}) &=& 0 \\
\pmeas(\{  TH , TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH \}) &=& 1 \\
\pmeas(\{  HH, HT, TT \}) &=& 1/3 \\
\pmeas(\{  HH, TH, TT \}) &=& 1 \\
\pmeas(\{  HT, TH, TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH, TT \}) &=& 1
\end{array}
  \end{array}\]
  The assignment satisfies the two constraints for probability measures:
  the probability of the entire sample space is 1, and the probability
  of every collection of disjoint events (e.g.,
  $\{ HT \} \cup \{ TH \} = \{ HT, TH \}$) is the sum of the individual
  probabilities. The probability of collections of non-disjoint events
  (e.g., $\{ HT, TH \} \cup \{ TH , TT \} = \{ HT, TH, TT \}$) may add
  to something different than the probabilities of the individual
  events. It is useful to think that this probability measure is
  completely induced by the two coins in question and their
  characteristics in the sense that each pair of coins induces a
  measure, and each measure must correspond to some pair of coins.  The
  measure above is induced by two coins such that the first coin is
  twice as likely to land tails up than heads up and the second coin is
  double-headed.  
  \qed\end{example}

Although specifying a probability for every event looks complex, a
probability measure can be simply constructed by $\pmeas(E)=\tensor*[_{\mathbb{R}}]{\sum}{_{\omega\in E}}\rho(\omega)$,
where $\rho:\Omega\rightarrow[0,1]$ and $\tensor*[_{\mathbb{R}}]{\sum}{_{\omega\in\Omega}}\rho(\omega)=1$.
Furthermore, every probability measure~$\pmeas$ can be written in
this form for an unique $\rho$~\cite{Kolmogorov1950,Swart2013}
which is called the probability distribution~\cite{GrahamKnuthPatashnik1994}
or the Radon-Nikodym derivative of $\pmeas$ with respect to the counting
measure~\cite{Nikodym1930,Torchinsky1994}. 

In a strict computational or experimental setting, one may question
the reliance of the definition of probability space on the uncountable
and uncomputable real interval $[0,1]$. This interval includes numbers
like $0.h_1h_2h_3\ldots$ where $h_i$ is 1 or 0 depending on whether
Turing machine $M_i$ halts or not. Such numbers cannot be
computed. This interval also includes numbers like $\frac{\pi}{4}$
which can only be computed with increasingly large resources as the
precision increases.
% \yutsung{Check the meaning of the ``computing $\frac{\pi}{4}$'', because
% Bailey Borwein Plouffe formula can computing the $n$th binary digit of $\pi$ 
% using base 16 directly.}
Therefore, in a resource-aware computational or experimental setting,
it is more appropriate to consider probability measures that map
events to a set of elements computable with a fixed set of
resources. We expand on this observation and then consider
%% two approaches from the literature: set-valued probability
%% measures~\cite{Artstein1972,PuriRalescu1983} and 
interval-valued probability
measures~\cite{Dempster1967,Shafer1976,JamisonLodwick2004}
in detail.\footnote{There is another possible approach that can be
  used to split the real interval $[0,1]$ into a collection of
  subsets~\cite{PuriRalescu1983} \amr{need to explain the
    connection and why we are not using it.}}

%%%%%
\subsection{Measuring Probabilities: Buffon's Needle
Problem\label{subsec:Measuring-Probabilities:-Buffon}}
 
In previous section, the probability of each event is known
a priori. In reality, we seldom know much about events, but we could still assume
each event~$E$ has a probability~$\pmeas(E)$. If we want to know the
probability~$\pmeas(E)$, we could run $N$ independent trials. Let $x_{i}$ denote
whether the event~$E$ occurs in the $i$-th trial or not, then $\pmeas(E)$ could
be approximated to given accuracy by the relative
frequency~$\frac{1}{N}\sum_{i}x_{i}$ with the probability converging to one as
$N$ goes to infinity. This fact is called the law of large
numbers~\cite{Bernoulli2006,Kolmogorov1950,Uspensky1937,Shafer1976,544199}.

Suppose we drop a needle of length $\ell$ onto a floor made of equally
spaced parallel lines a distance $h$ apart. It is a known fact that
the probability of the needle crossing a line is
$\frac{2\ell}{\pi h}$~\cite{Buffon1777,DeMorgan1872,Hall1873,Uspensky1937}.
We analyze this situation in the mathematical framework of probability
spaces paying special attention to the resources needed to estimate
the probability computationally or experimentally.

To formalize the experiment, we consider an experimental setup
consisting of a collection of $N$ identical needles of length
$\ell$. We throw the $N$ needles one needle at a time, and observe the number
$X$ of needles that cross a line. The sample space can be expressed as the set
$\{X,-\}^N$
of sequences of characters of length $N$ where each character is
either $X$ to indicate a needle crossing a line or $-$ to indicate a
needle not crossing a line. If $N=3$, the probability of the event that exactly
2 needles cross lines~$\{-XX,X{-}X,XX-\}$ can be 
estimated by the relative frequency~$\frac{2}{3}$. Generally, the probability of
the event that exactly $M$ needles out of the $N$ total needles cross lines can
be estimated by $\frac{M}{N}$.

In an actual experiment with $500$ needles and the ratio
$\frac{\ell}{h}=0.75$~\cite{Hall1873}, it was found that $236$ crossed
a line so the relative frequency is $0.472$ whereas the idealized
mathematical probability is $0.4774\ldots$. In a larger experiment
with $5000$ needles and the ratio
$\frac{\ell}{h}=0.8$~\cite{Uspensky1937}, the relative frequency was
calculated to be $0.5064$ whereas the idealized mathematical
probability is $0.5092\ldots$. We see that the observed probability
approaches $\frac{2\ell}{\pi h}$ but only if \emph{larger and larger
  resources} are expended. These resource considerations suggest that
it is possible to replace the real interval $[0,1]$ with rational
numbers up to a certain precision related to the particular experiment
in question.  There is clearly a connection between the number of
needles and the achievable precision: in the hypothetical experiment
with 3 needles, it is not sensible to retain 100 digits in the
expansion of $\frac{2\ell}{\pi h}$.

There is however another more subtle assumption of unbounded
computational power in the experiment. We are assuming that we can
always determine with certainty whether a needle is crossing a
line. But ``lines'' on the the floor have thickness, their distance
apart is not exactly $h$, and the needles lengths 
are not all absolutely equal to $\ell$. These perturbations make
the events ``fuzzy.'' Thus, in an experiment with limited resources,
it is not possible to talk about the idealized event that exactly $M$
needles cross lines as this would require the most expensive needles
built to the most precise accuracy, laser precision for drawing lines
on the floor, and the most powerful microscopes to determine if a
needle does cross a line. Instead we might talk about the event that
$M-\delta$ needles evidently cross lines and $M+\delta'$ needles
plausibly cross lines where $\delta$ and $\delta'$ are
resource-dependent approximations. This fuzzy notion of events leads
to probabilities being only calculable within intervals of confidence
reflecting the certainty of events and their plausibility. This is
indeed consistent with published experiments: in an experiment with
$3204$ needles and the ratio $\frac{\ell}{h}=0.6$~\cite{DeMorgan1872},
$1213$ needles clearly crossed a line and $11$ needles were close
enough to plausibly be considered as crossing the line: we would
express the probability in this case as the interval
$\left[\frac{1213}{3204},\frac{1224}{3204}\right]$ expressing that we
are certain that the event has probability at least
$\frac{1213}{3204}$ but it is possible that it would have probability
$\frac{1224}{3204}$.

Recall that the relative frequency will approximate the probability
of an event if the event has a probability. What if the event doesn't
have infinity precise probability because of the experimental limit?
In this case, two sequences of independent copies of experimental
results can have their relative frequencies converge almost surely
to different limits~\cite{Marinacci1999,Teran2014}. In another word,
to get a better approximation of the probability, the quality of experimental
equipment cannot be compensated by the number of independent trials.

% %%%%%
% \subsection{Set-valued Probability Measures}
 
% Instead of using every point in the real interval $[0,1]$ as a
% potential value for a probability measure, we can lump points together
% in sets and consider probability measures up to set membership. The
% simplest such situation is to partition the interval $[0,1]$ into two
% sets: the set $\{0\}$ (which we will call $\imposs$) and the set
% containing all the points in the half-open interval $(0,1]$ (which we
% will call $\poss$). We will call the resulting collection
% $\left\{ \imposs,\poss\right\} $, the collection $\mathscr{L}_{2}$. To
% completely specify a probability space, we also need to specify how to
% ``add'' the probabilities in $\mathscr{L}_2$. Given two subsets of the
% real interval, $I_1$ and $I_2$, the general formula for adding them
% is:\footnote{Notice that the set-valued measure
%   in~\cite{Artstein1972,PuriRalescu1983}, only require
%   $I_{1}+I_{2}=\left\{ a_{1}+a_{2}\middle|a_{1}\in I_{1},a_{2}\in
%     I_{2}\right\} $
%   because they focused on measure theory. In probability theory,
%   $2\in\pmeas\left(E\right)$ doesn't really make sense so that we
%   consider intersect $[0,1]$ in our definition.}
% \[
% I_{1} \vee I_{2}=\left\{ x+y~\middle|~ x \in I_{1}, y \in I_{2}\right\} ~\cap~ [0,1]\textrm{ .}
% \]
% In the case of $\mathscr{L}_2$, this reduces to $x \vee y = \imposs$
% iff $x = y = \imposs$.

% \amr{
% From Puri and Ralescu's paper: Fix $n=1$ (only 1-dimension). Conditions on $\mu$:
% \begin{itemize}
% \item $\mu(E) \neq \emptyset$ for every $E \in \events$, 
% \item $1 \in \mu(\Omega)$, 
% \item for every disjoint family $A_i$ we have, $\mu(\bigcup_i A_i) =
%   \sum_i \mu(A_i)$
% \end{itemize}
% where the sum of subsets of $\mathbb{R}$ (not $\mathbb{R}^n$ as in the
% paper) is as follows. Let $X$ and $Y$ be subsets of $\mathbb{R}$, then
% $X + Y = \{ x + y ~|~ x \in X, y \in Y \}$
% }\\
% \yutsung{Actually, using $X+Y=\{x+y~|~x\in X,y\in Y\}$ directly
% might be more confusing than I thought... Because our example will
% be follows:

% For $\{0\}=\imposs$ and $(0,\infty)=\poss$, it will become
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \{0\}\\
% \pmeas(\{HH\}) & = & (0,\infty)\\
% \pmeas(\{HT\}) & = & \{0\}\\
% \pmeas(\{TH\}) & = & (0,\infty)\\
% \pmeas(\{TT\}) & = & \{0\}\\
% \pmeas(\{HH,HT\}) & = & (0,\infty)\\
% \pmeas(\{HH,TH\}) & = & (0,\infty)\\
% \pmeas(\{HH,TT\}) & = & (0,\infty)
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & (0,\infty)\\
% \pmeas(\{HT,TT\}) & = & \{0\}\\
% \pmeas(\{TH,TT\}) & = & (0,\infty)\\
% \pmeas(\{HH,HT,TH\}) & = & (0,\infty)\\
% \pmeas(\{HH,HT,TT\}) & = & (0,\infty)\\
% \pmeas(\{HH,TH,TT\}) & = & (0,\infty)\\
% \pmeas(\{HT,TH,TT\}) & = & (0,\infty)\\
% \pmeas(\{HH,HT,TH,TT\}) & = & (0,\infty)
% \end{array}\end{array}
% \]

% For $\{0\}=\imposs$ and $(0,1]=\poss$, it will become the following
% and gives an extra-value~$(0,2]$.
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \{0\}\\
% \pmeas(\{HH\}) & = & (0,1]\\
% \pmeas(\{HT\}) & = & \{0\}\\
% \pmeas(\{TH\}) & = & (0,1]\\
% \pmeas(\{TT\}) & = & \{0\}\\
% \pmeas(\{HH,HT\}) & = & (0,1]\\
% \pmeas(\{HH,TH\}) & = & (0,2]\\
% \pmeas(\{HH,TT\}) & = & (0,1]
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & (0,1]\\
% \pmeas(\{HT,TT\}) & = & \{0\}\\
% \pmeas(\{TH,TT\}) & = & (0,1]\\
% \pmeas(\{HH,HT,TH\}) & = & (0,2]\\
% \pmeas(\{HH,HT,TT\}) & = & (0,1]\\
% \pmeas(\{HH,TH,TT\}) & = & (0,2]\\
% \pmeas(\{HT,TH,TT\}) & = & (0,1]\\
% \pmeas(\{HH,HT,TH,TT\}) & = & (0,2]
% \end{array}\end{array}
% \]

% For $\{0\}=\imposs$, $[0,\frac{1}{2}]=\unlikely$, and $[\frac{1}{2},1]=\likely$,
% it will become the following and gives an extra-value~$[\frac{1}{2},\frac{3}{2}]$.
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \{0\}\\
% \pmeas(\{HH\}) & = & [0,\frac{1}{2}]\\
% \pmeas(\{HT\}) & = & \{0\}\\
% \pmeas(\{TH\}) & = & [\frac{1}{2},1]\\
% \pmeas(\{TT\}) & = & \{0\}\\
% \pmeas(\{HH,HT\}) & = & [0,\frac{1}{2}]\\
% \pmeas(\{HH,TH\}) & = & [\frac{1}{2},\frac{3}{2}]\\
% \pmeas(\{HH,TT\}) & = & [0,\frac{1}{2}]
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & [\frac{1}{2},1]\\
% \pmeas(\{HT,TT\}) & = & \{0\}\\
% \pmeas(\{TH,TT\}) & = & [\frac{1}{2},1]\\
% \pmeas(\{HH,HT,TH\}) & = & [\frac{1}{2},\frac{3}{2}]\\
% \pmeas(\{HH,HT,TT\}) & = & [0,\frac{1}{2}]\\
% \pmeas(\{HH,TH,TT\}) & = & [\frac{1}{2},\frac{3}{2}]\\
% \pmeas(\{HT,TH,TT\}) & = & [\frac{1}{2},1]\\
% \pmeas(\{HH,HT,TH,TT\}) & = & [\frac{1}{2},\frac{3}{2}]
% \end{array}\end{array}
% \]

% Actually, this approach is more natural to me when I think in measure
% theory, but I guess the readers might be confused if write in this
% way?}




% To summarize, an $\mathscr{L}$-valued probability is a function
% $\pmeas:\events\rightarrow\mathscr{L}$ such that:
% \begin{itemize}
% \item $1\in\pmeas(\Omega)$, and 
% \item for a collection $E_{i}$ of pairwise disjoint events, $\pmeas(\bigcup_{i}E_{i})=\bigvee_i\pmeas(E_{i})$. 
% \end{itemize}

% \begin{example}{[}Two-coin probability space with finite set-valued
% probability measure{]} \label{ex2} Under the new set-valued requirement,
% the probability measure in the first example becomes: 
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \imposs\\
% \pmeas(\{HH\}) & = & \poss\\
% \pmeas(\{HT\}) & = & \imposs\\
% \pmeas(\{TH\}) & = & \poss\\
% \pmeas(\{TT\}) & = & \imposs\\
% \pmeas(\{HH,HT\}) & = & \poss\\
% \pmeas(\{HH,TH\}) & = & \poss\\
% \pmeas(\{HH,TT\}) & = & \poss
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & \poss\\
% \pmeas(\{HT,TT\}) & = & \imposs\\
% \pmeas(\{TH,TT\}) & = & \poss\\
% \pmeas(\{HH,HT,TH\}) & = & \poss\\
% \pmeas(\{HH,HT,TT\}) & = & \poss\\
% \pmeas(\{HH,TH,TT\}) & = & \poss\\
% \pmeas(\{HT,TH,TT\}) & = & \poss\\
% \pmeas(\{HH,HT,TH,TT\}) & = & \poss
% \end{array}\end{array}
% \]
% Despite the fact that we have lost all numeric information, the probability
% measure still reveals that the second coin is double-headed. We have
% however lost the information regarding the bias in the first coin.
% This information can be recovered with a more refined probability
% measure as we show next. \qed\end{example}

% Although $\mathscr{L}_{2}$-valued probability measure is quite intuitive,
% set-valued probability measure is not that intuitive if we extend
% to more values. For example, consider the following three \emph{overlapping}
% closed sub-intervals: $[0,0]$ is still call \imposs, $[0,\frac{1}{2}]$
% which we call \unlikely, ad $[\frac{1}{2},1]$ which we call \likely.
% The following example is a set-valued probability measure corresponding
% to the probability measure in the first example.

% \begin{example}\label{Two-coin-set-valued-again}{[}Two-coin probability
% space with set-valued probability measure, again{]} 
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas(\emptyset) & = & \imposs\\
% \pmeas(\{HH\}) & = & \unlikely\\
% \pmeas(\{HT\}) & = & \imposs\\
% \pmeas(\{TH\}) & = & \likely\\
% \pmeas(\{TT\}) & = & \imposs\\
% \pmeas(\{HH,HT\}) & = & \unlikely\\
% \pmeas(\{HH,TH\}) & = & \likely\\
% \pmeas(\{HH,TT\}) & = & \unlikely
% \end{array} & \begin{array}{rcl}
% \pmeas(\{HT,TH\}) & = & \likely\\
% \pmeas(\{HT,TT\}) & = & \imposs\\
% \pmeas(\{TH,TT\}) & = & \likely\\
% \pmeas(\{HH,HT,TH\}) & = & \likely\\
% \pmeas(\{HH,HT,TT\}) & = & \unlikely\\
% \pmeas(\{HH,TH,TT\}) & = & \likely\\
% \pmeas(\{HT,TH,TT\}) & = & \likely\\
% \pmeas(\{HH,HT,TH,TT\}) & = & \likely
% \end{array}\end{array}
% \]
% In this example, we can get the information that the first coin is
% weighted and the second coin is double-headed. However, we may notice
% that the set-valued probability of the whole space~$\{HH,HT,TH,TT\}$
% are different between these two examples. This problem can be fixed
% when we moved on to the next section. \qed\end{example}

% We will return to finite set-valued probability measures in Sec.~\ref{sec:?}.

%%%%%
\subsection{Interval-valued probability measures}

As motivated above, an event $E_1$ may have an interval of probability
$[l_1,r_1]$. Assume that another disjoint event $E_2$ has interval
probability $[l_2,r_2]$, what is the interval probability of the event
$E_1 \cup E_2$? The answer is somewhat subtle: although it is possible
to use the sum of the intervals $[l_1+l_2,r_1+r_2]$ as the combined
probability, one can do find a much tighter interval if information
\emph{against} the event (i.e., information about the complement
event) is also taken into consideration. Formally, for a general event
$E$ with probability $[l,r]$, the evidence that
contradicts $E$ is an evidence supporting the complement of $E$. The
complement of $E$ must therefore have probability
$\left[1-r,1-l\right]$ which we abbreviate $1\tensor*[_{\mathscr{I}}]{-}{}\left[l,r\right]$,
where the preposing $\mathscr{I}$ specifies we subtracts intervals. Given
a collection of intervals $\mathscr{I}$, an
$\mathscr{I}$--interval-valued probability measure is a function
$\bar{\mu} : \events \rightarrow \mathscr{I}$ such
that~\cite{JamisonLodwick2004}:\footnote{Notice that the belief function defined by Dempster~\cite{Dempster1967}
and Shafer~\cite{Shafer1976} is a special case of the interval-valued
probability measures. Depending on how quantum interval probability goes, this
section may be specialized to Dempster and Shafer only.}
\begin{itemize}
\item $\bar{\mu}(\emptyset)=[0,0]$, 
\item $\bar{\mu}(\Omega)=[1,1]$, 
\item for any event $E$, $\bar{\mu}\left(\Omega\backslash
E\right)=1\tensor*[_{\mathscr{I}}]{-}{}\bar{\mu}\left(E\right)$, and 
\item for a collection $E_{i}$ of pairwise disjoint events, we have $\bar{\mu}\left(\bigcup_{i}E_{i}\right)\subseteq\tensor*[_{\mathscr{I}}]{\sum}{_{i}}\bar{\mu}\left(E_{i}\right)$,
where $\tensor*[_{\mathscr{I}}]{\sum}{_{i}}[l_{i},r_{i}]=\left[\tensor*[_{\mathbb{R}}]{\sum}{_{i}}l_{i},\tensor*[_{\mathbb{R}}]{\sum}{_{i}}r_{i}\right]$.
We may drop the preposing $\mathscr{I}$ when summands are clearly intervals. 
\end{itemize}

% The singleton closed interval $[0,0]$ is a required special value
% for empty set, and a natural generalization is to consider another
% special closed interval $[1,1]$ which we call \necess\ for the whole
% space. In general, if the probability of an event $E$ is $[a,b]$,
% we think of the left-endpoint~$a$ as representing the strength of
% the evidence that supports $E$, and the right-endpoint~$b$ as the
% strength of the evidence that contradicts $E$.

% Thus if we have an event $E$ with probability $[a,b]$ where $a=0.1$
% and $b=0.7$, we have that: 
% \begin{itemize}
% \item the strength of evidence supporting $E$ is 0.1; since either $E$
% or its complement must happen, we conclude that there is 0.9 evidence
% supporting the complement of $E$; 
% \item the strength of evidence contradicting $E$ is 0.7; again since either
% $E$ or its complement must happen, we conclude that there is 0.3
% evidence contradicting the complement of $E$. 
% \end{itemize}
%% \yutsung{Do we use the law of excluded middle here? You remind me
%% Agda : ) Did Homotopy Type Theory people said anything about the probability?}\\

% Turning things around, the strength of evidence that contradicts $E$
% is evidence supporting the complement of $E$. The complement of $E$
% must therefore have probability $\left[1-b,1-a\right]$ which we
% abbreviate $1-\left[a,b\right]$, so the probability measure $\bar{\mu}$
% should satisfy:
% \begin{itemize}
% \item $\bar{\mu}(\emptyset)=[0,0]$, 
% \item $\bar{\mu}(\Omega)=[1,1]$, and 
% \item $\bar{\mu}\left(\Omega\backslash E\right)=1-\bar{\mu}\left(E\right)$
% \end{itemize}
% However, if we want $\bar{\mu}(\Omega)=[1,1]$, and the probability assignment
% for singleton sets $\bar{\mu}(\{HH\})=\unlikely$, $\bar{\mu}(\{HT\})=\imposs$,
% $\bar{\mu}(\{TH\})=\likely$, and $\bar{\mu}(\{TT\})=\imposs$ as in example~\ref{Two-coin-set-valued-again},
% we only have $\bar{\mu}(\Omega)\subsetneq\bar{\mu}(\{HH\})+\bar{\mu}(\{HT\})+\bar{\mu}(\{TH\})+\bar{\mu}(\{TT\})$.
% In general, we only require
% \begin{itemize}
% \item for a collection $E_{i}$ of pairwise disjoint events, we have $\bar{\mu}\left(\bigcup_{i}E_{i}\right)\subseteq\sum_{i}\bar{\mu}\left(E_{i}\right)$. 
% \end{itemize}
% This statement means that the evidences of $\bigcup_{i}E_{i}$ is
% at least as strong as putting all the evidences of $E_{i}$ together,
% but some evidence may only be acquired for $\bigcup_{i}E_{i}$ as
% the whole. Therefore, $\bar{\mu}\left(\bigcup_{i}E_{i}\right)$ is a
% subset of $\sum_{i}\bar{\mu}\left(E_{i}\right)$, but may not equal.
% In our example, 
% \begin{eqnarray*}
%  &  & \bar{\mu}(\{HH\})+\bar{\mu}(\{HT\})+\bar{\mu}(\{TH\})+\bar{\mu}(\{TT\})\\
%  & = & \imposs+\unlikely+\imposs+\likely\\
%  & = & \left[0,0\right]+\left[0,\frac{1}{2}\right]+\left[0,0\right]+\left[\frac{1}{2},1\right]=\left[\frac{1}{2},\frac{3}{2}\right]
% \end{eqnarray*}
% The above equation told us $\bar{\mu}(\Omega)\subseteq\left[\frac{1}{2},\frac{3}{2}\right]$.
% However, because of $\bar{\mu}(\emptyset)=\imposs$, we have $\bar{\mu}\left(\Omega\right)=1-\bar{\mu}\left(\emptyset\right)=\necess$,
% but $\bar{\mu}(\emptyset)=\imposs$ cannot be used to reasoning the probability
% of $\{HH\}$, $\{HT\}$, $\{TH\}$ and $\{TT\}$ individually. The
% full probability assignment is shown in the following example. 

\noindent We will explain why the last condition is expressed using
$\subseteq$ by a small example.

\begin{example}[Two-coin experiment with interval probability]
  \label{ex3} We split the unit interval $[0,1]$ in the following four
  closed sub-intervals: $[0,0]$ which we call \imposs,
  $[0,\frac{1}{2}]$ which we call \unlikely, $[\frac{1}{2},1]$ which
  we call \likely, and $[1,1]$ which we call \necess. Using these new
  values, we can modify the probability measure of Ex.~\ref{ex1} by
  mapping each numeric value to the smallest sub-interval containing
  it to get the following:
\[
\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\bar{\mu}(\emptyset) & = & \imposs\\
\bar{\mu}(\{HH\}) & = & \unlikely\\
\bar{\mu}(\{HT\}) & = & \imposs\\
\bar{\mu}(\{TH\}) & = & \likely\\
\bar{\mu}(\{TT\}) & = & \imposs\\
\bar{\mu}(\{HH,HT\}) & = & \unlikely\\
\bar{\mu}(\{HH,TH\}) & = & \necess\\
\bar{\mu}(\{HH,TT\}) & = & \unlikely
\end{array} & \begin{array}{rcl}
\bar{\mu}(\{HT,TH\}) & = & \likely\\
\bar{\mu}(\{HT,TT\}) & = & \imposs\\
\bar{\mu}(\{TH,TT\}) & = & \likely\\
\bar{\mu}(\{HH,HT,TH\}) & = & \necess\\
\bar{\mu}(\{HH,HT,TT\}) & = & \unlikely\\
\bar{\mu}(\{HH,TH,TT\}) & = & \necess\\
\bar{\mu}(\{HT,TH,TT\}) & = & \likely\\
\bar{\mu}(\{HH,HT,TH,TT\}) & = & \necess
\end{array}\end{array}
\]
Despite the absence of any numeric information, the probability
measure is quite informative: it reveals that the second coin is
double-headed and that the first coin is biased. To understand the
$\subseteq$-condition, consider the following calculation:
\begin{eqnarray*}
 &  & \bar{\mu}(\{HH\})+\bar{\mu}(\{HT\})+\bar{\mu}(\{TH\})+\bar{\mu}(\{TT\})\\
 & = & \imposs+\unlikely+\imposs+\likely\\
 & = & \left[0,0\right]+\left[0,\frac{1}{2}\right]+\left[0,0\right]+\left[\frac{1}{2},1\right]=\left[\frac{1}{2},\frac{3}{2}\right]
\end{eqnarray*}
If we were to equate $\bar{\mu}(\Omega)$ with the sum of the individual
probabilities we would get that
$\bar{\mu}(\Omega) = \left[\frac{1}{2},\frac{3}{2}\right]$. However,
using the fact that $\bar{\mu}(\emptyset)=\imposs$, we have
$\bar{\mu}\left(\Omega\right)=1-\bar{\mu}\left(\emptyset\right)=\necess=[1,1]$. This
interval is tighter and a better estimate for the probability of the
event $\Omega$ and of course it is contained in
$[\frac{1}{2},\frac{3}{2}]$. However it is only possible to exploit
the information about the complement when all four events are
combined. Thus the $\subseteq$-condition allows us to get an estimate
for the combined event from each of its constituents and then gather
more evidence knowing the aggregate event.\qed\end{example}

% \gerardo{Set-valued is a particular case?}

% \yutsung{Set-valued is not a particular case of interval-valued probability
% because their rules are different. A set-valued probability measure
% should satisfy: 
% \begin{enumerate}
% \item $1\in\bar{\mu}(\Omega)$, and 
% \item for a collection $E_{i}$ of pairwise disjoint events, $\bar{\mu}(\bigcup_{i}E_{i})=\sum_{i}\bar{\mu}(E_{i})$. 
% \end{enumerate}
% which are completely opposite to the interval-valued probability measures. 

% It seems hard to make a consistent story with the general set-valued
% probability measure framework (maybe except possible/impossible?),
% so we just comment out set-valued probability measure here...}

% to think of the above combination in the context of the evidence
% against the events established by the dual events. Writing in terms of
% the explicit intervals we see:
% \[\begin{array}{rcl@{\qquad}rcl}
% \bar{\mu}(\{HH\}) & = & [0,\frac{1}{2}] & \bar{\mu}(\{HT\}) & = & [0,0] \\
% \bar{\mu}(\{TH\}) & = & [\frac{1}{2},1] &  \bar{\mu}(\{TT\}) & = & [0,0]
% \end{array}\]
% The probability for $\bar{\mu}(\{HH,TH\})$ is $[X,Y]$ where $X$ comes
% from the positive evidence for $\{HH\}$ and $\{TH\}$ and from the
% evidence against  is $[X,Y]$ where $X$ comes
% \[
% [\max(1,\frac{1}{2}),\min(1,\frac{3}{2})]
% \]

% must be at least as strong as the
% evidence for $\{HH\}$ and $\{TH\}$ individually and hence we have the
% tentative bounds on $\bar{\mu}(\{HH,TH\})$ as
% $[\frac{1}{2},\frac{3}{2}]$. The dual events, however, establish
% evidence against $\{HH,TH\}$ with bounds $[

% A=HH
% B=TH
% C=HT
% D=TT

% To calculate the probability of $\{HH\}\cup\{TH\}$ we gather 
% evidence not only from the events $\{HH\}$ and $\{TH\}$, but also from 
% the dual events $\{HT,TH,TT\}$ and $\{HH,HT,TT\}$. The minimal
% probability for $\{HH\}\cup\{TH\}$ is\max(

% To understand this mystery, we look at
% another more general example and then give the formal mathematical
% definition of interval-valued probabilities. 

% \begin{example}[Dempster-Shafer Theory of Evidence] We have three
%   employees whose precise ages $A_1$, $A_2$, and $A_3$ are not
%   known. All is given is a range of ages for each employee:
%   $A_1 \in \{ 23,24 \}$, $A_2 \in \{ 20,21,22\}$, and
%   $A_3 \in \{ 21,22 \}$. The sample space in this case is
%   $A_1 \times A_2 \times A_3$ which represents all the possible
%   combinations of ages for the three employees:
% \[\begin{array}{rcl}
% \Omega &=& \{ 
%         (23,20,21), (23,20,22), (23,21,21), (23,21,22), (23,22,21), (23,22,22), \\
% && ~(24,20,21), (24,20,22), (24,21,21), (24,21,22), (24,22,21), (24,22,22) \}
% \end{array}\]
% Subsets of $\Omega$ represent events as usual. Consider the event
% $\Omega$ that \emph{some} employee's age is in the range
% $\{20,21,22\}$: since that event covers the entire sample space its
% probability must be 1. We can however produce a more informative
% answer by reasoning as follows: it is \emph{impossible} for the first
% employee's age to be in the range $\{20,21,22\}$; it is \emph{certain}
% that the second employee's age is in that range; and it is
% \emph{possible} that the third's employee age is in that
% range. Aggregating the results, we see that it is \emph{necessary} for
% one out of three employees to have their age in the required range,
% and it is \emph{possible} for an additional employee to have their age
% in the required range. We summarize this information by reporting that
% the probability of this event is $[\frac{1}{3},\frac{2}{3}]$ where the
% first number reports the \emph{certainty} of the event and the second
% reports the \emph{possibility} of the event. We could also have
% reasoned about the dual event that \emph{no} employee's age is in the
% range $\{20,21,22\}$ to get the probability $[\frac{1}{3},\frac{2}{3}]$

%  dually
% about the evidence \emph{against} the event. The probability in this
% case 


% Now consider the event
% that some employee's age is in the range $\{23,24\}$. Reasoning as
% above, the probability of this event is $[\frac{1}{3},\frac{1}{3}]$ as
% only the first employee qualifies. Clearly if we were to ask the
% probability that some employee's age is in the range
% $\{20,21,22,23,24\}$ the result should be $[1,1]$ as this range covers
% all the possibilities. The way to calculate this result from the two
% previous ones is as follows: the evidence for the combined event to be
% necessary is at least as strong as the evidence that each disjoint
% event that contributes to it is necessary. So the lower bound
% probability for the combined event is at least
% $\frac{1}{3}$. Similarly the upper bound probability is at least
% $1$. But now we can reason using the complements of the events about
% the necessity and possibility of refuting each event. For the event
% that some employee's age is \emph{not} in the range $\{20,21,22\}$ we
% have a probability $[\frac{1}{3},\frac{1}{3}]$ because it is necessary
% that $A_1$ is not in that range. Similarly, the event that some employee's age is
% \emph{not} in the range $\{23,24\}$ is
% $[\frac{2}{3},\frac{2}{3}]$. Thus although the positive evidence for
% necessity 

% \qed\end{example}

% \newpage

% \begin{example}[Dempster-Shafer Theory of Evidence] We have five
%   employees whose precise age is not known. All is given is a range for
%   each employee:
% \[\begin{array}{rcl}
% \textrm{Age}(M_{1}) & \in & \{ 23,24 \}=D_{1}\\
% \textrm{Age}(M_{2}) & \in & \{ 20,21,22\}=D_{2}\\
% \textrm{Age}(M_{3}) & \in & \{ 20,21 \}=D_{3}
% \end{array}\]
% What is the probability that an employee's age is in the range
% $\{22,23,24\}$? Looking at the data, it is \emph{possible} for $M_2$'s age
% to be within the range, it is \emph{not possible} for $M_3$'s age to
% be within the range, and it is \emph{certain} or \emph{necessary} that
% $M_1$'s age is in the range. Clearly saying that an event is
% \emph{necessary} is equivalent to saying that its complement is
% \emph{not possible}. Aggregating the results for the three employees,
% we can calculate that is necessary that 1 employees have their ages
% within the range, and it is possible that 2 employees have their ages
% within the range. We express this formally as saying that the
% probability of the event is $\left[\frac{1}{3},\frac{2}{3}\right]$. Now consider
% another event asking whether the ages are some other disjoint range
% $\{20,21\}$. Reasoning in a similar way we calculate that the
% probability for this event is $\left[\frac{1}{3},\frac{2}{3}\right]$. Now let's
% take the two events together and ask about the possibility of the ages to be in the range
% $\{20,21,22,23,24\}$. Clearly that probability must be $[1,1]$ as every
% employee's age is in that range. This problem
% looks puzzle if we want to attack it directly, but will be more clear
% if we think in terms of conditional probability. We will compute the
% conditional probability for the usual real-valued probability first
% and for the interval-valued probability later.
% \begin{itemize}
% \item We randomly draw an employee among the three, so each employee has
% probability $\frac{1}{3}$ to be drawn, i.e.,
% \[
% \bar{\mu}\left(\left\{ i=1\right\} \right)=\bar{\mu}\left(\left\{ i=2\right\} \right)=\bar{\mu}\left(\left\{ i=3\right\} \right)=\frac{1}{3}
% \]
% In order to compute the usual real-valued probability, we need a probability
% distribution within the possible range $D_{1}$, $D_{2}$, and $D_{3}$.
% Let's assume they are equally probable, i.e., 
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{1})=23\right\} \right) & = & \frac{1}{2}\\
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{1})=24\right\} \right) & = & \frac{1}{2}
% \end{array} & \begin{array}{rcl}
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{2})=20\right\} \right) & = & \frac{1}{3}\\
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{2})=21\right\} \right) & = & \frac{1}{3}\\
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{2})=22\right\} \right) & = & \frac{1}{3}
% \end{array}\end{array}\begin{array}{rcl}
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{3})=20\right\} \right) & = & \frac{1}{2}\\
% \bar{\mu}\left(\left\{ \textrm{Age}(M_{3})=21\right\} \right) & = & \frac{1}{2}
% \end{array}
% \]
% Then, the probability that an employee's age is in the range $\{22,23,24\}$
% can be computed as follow:
% \begin{eqnarray*}
%  &  & \bar{\mu}\left(\left\{ i\middle|\textrm{Age}(M_{i})\in\{22,23,24\}\right\} \right)\\
%  & = & \bar{\mu}\left(\left\{ i=1\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{1})\in\{22,23,24\}\right\} \right)\\
%  &  & +\bar{\mu}\left(\left\{ i=2\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{2})\in\{22,23,24\}\right\} \right)\\
%  &  & +\bar{\mu}\left(\left\{ i=3\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{3})\in\{22,23,24\}\right\} \right)\\
%  & = & \frac{1}{3}\cdot1+\frac{1}{3}\cdot\frac{1}{3}+\frac{1}{3}\cdot0=\frac{4}{9}
% \end{eqnarray*}
% \item Assume we don't know the probability distributions within the possible
% range $D_{1}$, $D_{2}$, and $D_{3}$. All we know is whether they
% are \emph{possible} or \emph{necessary}. Then we replace the above
% computation from exact value to interval and consider
% \begin{eqnarray*}
%  &  & \bar{\mu}\left(\left\{ i\middle|\textrm{Age}(M_{i})\in\{22,23,24\}\right\} \right)\\
%  & = & \bar{\mu}\left(\left\{ i=1\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{1})\in\{22,23,24\}\right\} \right)\\
%  &  & +\bar{\mu}\left(\left\{ i=2\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{2})\in\{22,23,24\}\right\} \right)\\
%  &  & +\bar{\mu}\left(\left\{ i=3\right\} \right)\bar{\mu}\left(\left\{ \textrm{Age}(M_{3})\in\{22,23,24\}\right\} \right)\\
%  & = & \frac{1}{3}\cdot[1,1]+\frac{1}{3}\cdot[0,1]+\frac{1}{3}\cdot[0,0]=\left[\frac{1}{3},\frac{2}{3}\right]
% \end{eqnarray*}
% \yutsung{Check the conditional probability rule in the Dempster-Shafer
% Theory!}
% \end{itemize}
% \qed\end{example}

% In interval-valued probability measures, if the probability of an
% event $E$ is $[a,b]$, we think of the left-endpoint~$a$ as
% representing the strength of the evidence that supports $E$, and the
% right-endpoint~$b$ as the strength of the evidence that contradicts
% $E$. 

% Thus if we have an event $E$ with probability $[a,b]$ where
% $a=0.1$ and $b=0.7$, we have that:
% \begin{itemize}
% \item the strength of evidence supporting $E$ is 0.1; since either $E$
%   or its complement must happen, we conclude that there is 0.9
%   evidence supporting the complement of $E$; 
% \item the strength of evidence contradicting $E$ is 0.7; again since
%   either $E$ or its complement must happen, we conclude that there is
%   0.3 evidence contradicting the complement of $E$.
% \end{itemize}

% \yutsung{Do we use the law of excluded middle here? You remind me Agda : ) Did
% Homotopy Type Theory people said anything about the probability?}

% Turning things around, the strength of evidence that contradicts
% $E$ is evidence supporting the complement of $E$. The complement of $E$ must
% therefore have probability $\left[1-b,1-a\right]$ which we abbreviate $1-\left[a,b\right]$:

% \begin{eqnarray*}
% \bar{\mu}\left(\emptyset\right) & = & \imposs\\
% \bar{\mu}\left(\Omega\right) & = & \necess\\
% \bar{\mu}\left(\Omega\backslash E\right) & = & 1-\bar{\mu}\left(E\right)
% \end{eqnarray*}
% Next, if we define $\sum_{i}\left[a_{i},b_{i}\right]=\left[\sum_{i}a_{i},\sum_{i}b_{i}\right]$,
% then 
% \begin{itemize}
% \item for a collection $E_{i}$ of pairwise disjoint events, we have $\bar{\mu}\left(\bigcup_{i}E_{i}\right)\subseteq\sum_{i}\bar{\mu}\left(E_{i}\right)$. 
% \end{itemize}
% Notice that the equality may not hold in general. This statement says
% that the evidences of $\bigcup_{i}E_{i}$ is at least as strong as
% putting all the evidences of $E_{i}$ together, but some evidence
% may only be acquired for $\bigcup_{i}E_{i}$ as the whole. Therefore,
% $\bar{\mu}\left(\bigcup_{i}E_{i}\right)$ is a subset of $\sum_{i}\bar{\mu}\left(E_{i}\right)$,
% but may not equal. In our example, 
% \begin{eqnarray*}
%  &  & \bar{\mu}(\{HH,TH\})=\necess=[1,1]\\
%  & \subseteq & \left[\frac{1}{2},\frac{3}{2}\right]=\left[0,\frac{1}{2}\right]+\left[\frac{1}{2},1\right]=\unlikely+\likely=\bar{\mu}(\{HH\})+\bar{\mu}(\{TH\})\textrm{ .}
% \end{eqnarray*}
% However, $\bar{\mu}(\{HH,TH\})$ is a proper subset of $\bar{\mu}(\{HH\})+\bar{\mu}(\{TH\})$
% because if we check the complement of $\{HH,TH\}$, we have 
% \begin{eqnarray*}
%  &  & \bar{\mu}(\{HH,TH\})=1-\bar{\mu}(\{HT,TT\})=1-\imposs=\necess\textrm{ ,}
% \end{eqnarray*}
% and $\bar{\mu}(\{HT,TT\})=\imposs$ cannot be used to reasoning the probability
% of $\{HH\}$ and $\{TH\}$ individually. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quantum Probability Spaces}

The mathematical framework above assumes that there exists a
predetermined set of events that are independent of the particular
experiment. However, in many practical situations, the structure of
the event space is only partially known and the precise dependence of
two events on each other cannot, a priori, be determined with
certainty. In the quantum framework, this partial knowledge is
compounded by the fact that there exist non-commuting events which
cannot happen simultaneously. To accommodate these more complex
situations, we abandon the sample space~$\Omega$ and reason directly
about events. A quantum probability space therefore consists of just
two components: a set of events $\qevents$ and a probability measure
$\mu : \qevents \rightarrow [0,1]$. We give an example before giving
the formal definition.

\begin{example}[One-qubit quantum probability space] 
  Consider a one-qubit Hilbert space with states
  $\alpha \ket{0} + \beta \ket{1}$ such that
  $|\alpha|^2 + |\beta|^2 = 1$, $\alpha ,\beta \in \C$. The set of events
  associated with this Hilbert space consists of all projection operators. Each event is
  interpreted as a possible post-measurement state of a quantum system
  in current state $\ket{\phi}$. For example, the event $\proj{0}$
  indicates that the post-measurement state will be~$\ket{0}$; the
  event $\proj{1}$ indicates that the post-measurement state will
  be~$\ket{1}$; the event $\proj{\ps}$ where
  $\ket{\ps} = \frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$ indicates that the
  post-measurement state will be $\ket{\ps}$; the event
  $\mathbb{1} = \proj{0}+\proj{1}$ indicates that the post-measurement
  state will be a linear combination of $\ket{0}$ and $\ket{1}$; and
  the empty event $\mathbb{0}$ states that the post-measurement state
  will be the empty state. As in the classical case, a probability
  measure is a function that maps events to $[0,1]$: here is a partial
  specification of a possible probability measure:
\[\begin{array}{rcl}
\mu\left(\mathbb{0}\right) = 0, \quad
\mu\left(\mathbb{1}\right) =  1, \quad
\mu\left(\proj{0}\right) = 1, \quad
\mu\left(\proj{1}\right) = 0, \quad
\mu\left(\proj{\ps}\right) = 1/2, \quad \ldots
\end{array}\]
Note that, similarly to the classical case, the probability of
$\mathbb{1}$ is 1 and the probability of collections of orthogonal
events (e.g., $\proj{0}+\proj{1}$) is the sum of the individual
probabilities. A collection of non-orthogonal events
(e.g., $\proj{0}$ and $\proj{\ps}$) is however not even a valid event.
In the classical example, we argued that each probability measure is
uniquely determined by two actual coins. A similar (but much more
subtle) argument is valid also in the quantum case. By postulates of
quantum mechanics and Gleason's theorem, it turns out that for large
enough quantum systems, each probability measure is uniquely
determined by an actual quantum state.
\qed\end{example}

To properly explain the previous example and generalize to arbitrary
quantum systems, we formally discuss projection operators, 
define quantum probability measures, and extend to quantum interval-valued
probability.

\subsection{Quantum Events}

\begin{definition}[Projection Operators;
Orthogonality~\cite{10.2307/2308516,Redhead1987-REDINA,peres1995quantum,Griffiths2003,Swart2013}]
\label{def:Projection} Given a Hilbert space $\Hilb$, an event\footnote{An event
is formally called an experimental proposition~\cite{BirkhoffVonNeumann1936}, a question~\cite{10.2307/2308516,DBLP:journals/corr/abs-0910-2393}, or an
elementary quantum test~\cite{peres1995quantum}.} mathematically is represented as a projection operator $P:\Hilb\rightarrow\Hilb$ onto a linear subspace~$S$ of $\Hilb$. The set of all events can be defined recursively as follow:
\footnote {``Projection'' is sometimes called
``orthogonal projection" or ``self-adjoint projection" to
emphasize $P^{\dagger} = P$~\cite{Griffiths2003,Maassen2010}.}
\begin{itemize}
\item $\mathbb{0}$ is a projection. 
\item For any pure state~$\ket{\psi}$, $\proj{\psi}$ is a projection
operator. 
\item Projection operators $P_1$ and $P_2$ are \emph{orthogonal} if $P_1P_2 =
  P_2P_1 = \mathbb{0}$. The sum of two projection operators~$P_{1}\tensor*[_{\mathscr{O}}]{+}{}P_{2}$
is also a projection operator if and only if they are orthogonal, 
where the preposing subscript $\mathscr{O}$
means $\tensor*[_{\mathscr{O}}]{+}{}$ is an operation between operators. 
\item Conversely, every projection~$P$ can be expressed as
$\tensor*[_{\mathscr{O}}]{\sum}{_j}\proj{\psi_{j}}$,
where $P$ actually projects onto the linear subspace~$S$ which
has an orthonormal basis~$\{\ket{\psi_{j}}\}$. 
\end{itemize}
\qed\end{definition}

Based on quantum events are projection operators, operations and properties of
quntum events can be written in terms of those of operators.

\begin{definition}[Ideal Measurement; Complement;
Commutativity~\cite{Swart2013,peres1995quantum,Griffiths2003}]~
\begin{itemize}
\item A set of mutually orthogonal projections $P_{i}$ is called an \emph{ideal
measurement} if it is a partition of the identity, i.e.,
$\tensor*[_{\mathscr{O}}]{\sum}{_{i}}P_{i}=\mathbb{1}$.
\item If $P$ is a projection operator, then $\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}P$
is also a projection operator, called \emph{complement}. It is orthogonal to $P$,
and corresponds to the complement event~$\Omega\backslash E$
in classical probability.
\item Projection operators $P_{1}$ and $P_{2}$ \emph{commute} if $P_{1}P_{2}=P_{2}P_{1}$.
The product of two projection operators~$P_{1}P_{2}$ is also a projection
operator if and only if they commute. This corresponds to the classical intersection
between events. 
\end{itemize}
\qed\end{definition}

%%%%%
\subsection{Quantum Probability Measures}

\begin{definition}[Quantum Probability
Measure~\cite{10.2307/2308516,gleason1957,Redhead1987-REDINA,Maassen2010}]\label{def:QuantumProbabilitySpace}
Given a Hilbert space $\Hilb$ with its set of events~$\events$, a
\emph{quantum probability measure} is a function~$\mu : \events \rightarrow
[0,1]$ such that:\footnote {It is possible to define a more general space of events consisting of
    all operators~$\mathcal{A}$ on $\Hilb$ and consider
    $\mu:\mathcal{A}\rightarrow\C$~\cite{Maassen2010,Swart2013}.  When
    an operator $A\in\mathcal{A}$ is Hermitian, $\mu\left(A\right)$ is
    the expectation value of $A$. We does not take this approach
    because we want to focus only on probability. }
\begin{itemize}
\item $\mu(\mathbb{1})=1$, and 
\item for mutually orthogonal projections $P_{i}$, we have 
$\mu\left(\tensor*[_{\mathscr{O}}]{\sum}{_{i}}P_{i}\right)=\tensor*[_{\mathbb{R}}]{\sum}{_{i}}\mu\left(P_{i}\right)$.
\end{itemize}
\qed\end{definition}

In order to motivate the definition of quantum interval-valued probability
measures later, we provide an equivalent definition of quantum probability
measures.

\begin{lemma}\label{lem:quantumProbabilityMeasure}Given a Hilbert
space $\Hilb$, a function~$\mu:\events\rightarrow[0,1]$ is a quantum probability
measure if and only if $\mu$ satisfies the following conditions:
\begin{itemize}
\item $\mu(\mathbb{0})=0$, 
\item $\mu(\mathbb{1})=1$, 
\item for any projection $P$,
$\mu\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}P\right)=1\tensor*[_{\mathbb{R}}]{-}{}\mu\left(P\right)$, and 
\item for a set of mutually orthogonal projections $P_{i}$, we have
$\mu\left(\tensor*[_{\mathscr{O}}]{\sum}{_{i}}P_{i}\right)=\tensor*[_{\mathbb{R}}]{\sum}{_{i}}\mu\left(P_{i}\right)$.
\end{itemize}
\qed\end{lemma}

A set of events~$\events$ together with quantum probability measure is called a
\emph{quantum probability space}. 
Comparing to the classical probability space, the empty set~$\emptyset$
corresponds to the empty projection~$\mathbb{0}$ and the event of
whole space~$\Omega$ corresponds to the identity projection~$\mathbb{1}$.
In contrast, the union~$\cup$ of any two events always gives an
event classically, but the operator addition~$\tensor*[_{\mathscr{O}}]{+}{}$
of two projections may not be a projection. As the result, the classical
condition~$\pmeas(E_{1}\cup E_{2})=\pmeas(E_{1})\tensor*[_{\mathbb{R}}]{+}{}\pmeas(E_{2})$
is always defined, and it is true when $E_{1}$ and $E_{2}$ are disjoint;
however, $\mu\left(P_{1}\tensor*[_{\mathscr{O}}]{+}{}P_{2}\right)=\mu(P_{1})\tensor*[_{\mathbb{R}}]{+}{}\mu(P_{2})$
is always true whenever the left-handed side is defined.

% \yutsung{The definition of independence is different from different
% sources:
% \begin{enumerate}
% \item In Swart's~\cite{Swart2013}, given a $*$-algebra~$\mathscr{A}$,
% two commuting sub-$*$-algebra $\mathscr{A}_{1}$ and $\mathscr{A}_{2}$ are \emph{logically
% independent} if for all probability measure~$\mu_{1}$ on $\mathscr{A}_{1}$ and
% $\mu_{2}$ on $\mathscr{A}_{2}$, there is an unique probability measure~$\mu$
% on the smallest sub-$*$-algebra of $\mathscr{A}$ containing both
% $\mathscr{A}_{1}$ and $\mathscr{A}_{2}$ such that $\mu\left(A_{1}
% A_{2}\right)=\mu_{1}\left(A_{1}\right)\mu_{2}\left(A_{2}\right)$
% for all $A_{1}\in\mathscr{A}_{1}$ and $A_{2}\in\mathscr{A}_{2}$. If two
% sub-$*$-algebra $\mathscr{A}_{1}$ and $\mathscr{A}_{2}$ are logically
% independent, the smallest sub-$*$-algebra containing both $\mathscr{A}_{1}$ and
% $\mathscr{A}_{2}$ is denoted by $\mathscr{A}_{1}\otimes\mathscr{A}_{2}$, and
% its probability measure~$\mu$ is denoted by $\mu_{1}\otimes\mu_{2}$\\
% Then, Swart proved the Bell inequality on logically independence $*$-algebras
% with a product state which gives a clear physical meaning. However, this
% only defines the independence of algebras instead of events.
% \item In Yuan's~\cite{Yuan2012}, given a $\mu$ on a $*$-algebra~$\mathscr{A}$,
% two sub-$*$-algebra $\mathscr{A}_{1}$ and $\mathscr{A}_{2}$ are \emph{independent}
% if $\mu\left(A_{1}A_{2}\right)=\mu\left(A_{1}\right)\mu\left(A_{2}\right)$
% for all $A_{1}\in\mathscr{A}_{1}$ and $A_{2}\in\mathscr{A}_{2}$.
% The same problem as the previous one: this defines the
% independent of algebras, but not events. Morever, I haven't found a clear
% physical meaning of this definition, yet\ldots
% \item In order to understand the definition of independence in~\cite{GuehneKleinmannCabelloEtAl2010},
% we need to first understand their notation because they focused on the
% hidden-variable (HV) models. Let $\lambda$ is the HV, $A_{i}$ is
% the measurement of the observable~$A$ at the position $i$ in the
% sequence. For example, $A_{1}B_{2}C_{3}$ denotes the sequence of
% measuring $A$ first, then $B$, and finally $C$. Given a fixed $\lambda$, we
% assume the outcome of an observable is deterministic. For example, the
% outcome of $B_{2}$ from the preceding sequence is denoted by $v\left(B_{2}\middle|A_{1}B_{2}C_{3}\right)$.
% Then, the outcome of $A$ is \emph{independent} of whether $B$ is measured
% before or after $A$ is $v\left(A_{1}\right)=v\left(A_{2}\middle|B_{1}A_{2}\right)$.\\
% If the outcomes of compatible observables are independent for any
% HV~$\lambda$, the HV model is non-contextual. (Notice that they have their own
% definition of ``compatible observables'' for their HV model\ldots)\\
% Compare to the previous definitions, this definition is appealing
% in a sense that projection operators are special cases of observables,
% and this definition has a physical meaning related to contextuality.
% However, before adopting their definition, we need to reformulate
% the definition to the usual quantum model....
% \item For the definition we used last time, given a quantum probability
% measure~$\mu:\events\rightarrow[0,1]$, two commuting projections
% $P_{1}$ and $P_{2}$ are \emph{independent} if
% $\mu\left(P_{1}P_{2}\right)=\mu\left(P_{1}\right)\mu\left(P_{2}\right)$.
% We have two examples:
% \end{enumerate}
% }
% 
% \yutsung{\begin{example}[Independence of product state] Consider
% the 2-qubit Hilbert space with a probability measure~$\mu\left(P\right)=\melement{\ps\ps}{P}$
% with the following projections:
% \[
% \begin{array}{rcl}
% P_{1}=\proj{00}+\proj{01},\quad P_{2}=\proj{00}+\proj{10},\quad P_{3}=\proj{10}+\proj{11},\quad P_{4}=\proj{00}\end{array}
% \]
% and their probabilities: 
% \[
% \begin{array}{rcl}
% \mu\left(\mathbb{0}\right)=0,\quad\mu\left(P_{1}\right)=1/2,\quad\mu\left(P_{2}\right)=1/2,\quad\mu\left(P_{3}\right)=1/2,\quad\mu\left(P_{4}\right)=1/4\textrm{ .}\end{array}
% \]
% On one hand, $P_{1}$ and $P_{3}$ are complement to each other so
% we have
% \begin{eqnarray*}
% \mu\left(P_{1}\right)+\mu\left(P_{3}\right) & = & \frac{1}{2}+\frac{1}{2}=1\\
% \mu\left(P_{1}\right)\mu\left(P_{3}\right) & = & \frac{1}{4}\ne0=\mu\left(\mathbb{0}\right)=\mu\left(P_{1}P_{3}\right)
% \end{eqnarray*}
% Therefore, $P_{1}$ and $P_{3}$ commute, but are not independent. On
% the other hand, $P_{1}$ also commutes with $P_{2}$, and
% \begin{eqnarray*}
% \mu\left(P_{1}\right)\mu\left(P_{2}\right) & = & \frac{1}{4}=\mu\left(P_{4}\right)=\mu\left(P_{1}P_{2}\right)
% \end{eqnarray*}
% so that $P_{1}$ and $P_{2}$ are independent.\qed\end{example}
% 
% \begin{example}[Independence of any states] In general, for any orthonormal
% basis~$\left\{ \ket{\psi_{j}}\right\} _{j=0}^{3}$, we can always
% pick 
% \[
% \begin{array}{rcl}
% P_{1}=\proj{\psi_{0}}+\proj{\psi_{1}},\quad P_{2}=\proj{\psi_{0}}+\proj{\psi_{2}},\quad P_{3}=\proj{\psi_{0}}\textrm{ .}\end{array}
% \]
% With the probability measure~$\mu\left(P\right)=\melement{\psi}{P}$,
% where $\ket{\psi}=\frac{1}{2}\sum_{j=0}^{3}\ket{\psi_{j}}$, their
% probabilities are
% \[
% \begin{array}{rcl}
% \mu\left(P_{1}\right)=1/2,\quad\mu\left(P_{2}\right)=1/2,\quad\mu\left(P_{3}\right)=1/4\textrm{ ,}\end{array}
% \]
% and we have
% \begin{eqnarray*}
% \mu\left(P_{1}\right)\mu\left(P_{2}\right) & = & \frac{1}{4}=\mu\left(P_{3}\right)=\mu\left(P_{1}P_{2}\right)\textrm{ .}
% \end{eqnarray*}
% In particular, the following four Bell states form an orthonormal
% basis~\cite{CabelloPRL.86.2001}: 
% \[
% \ket{\phi^{\pm}} = \frac{1}{\sqrt{2}}\left(\ket{00}\pm\ket{11}\right),\quad
% \ket{\psi^{\pm}} = \frac{1}{\sqrt{2}}\left(\ket{01}\pm\ket{10}\right)\textrm{ .}
% \]
% Their sum is also a Bell state~
% \[
% \ket{\psi}=\frac{1}{2}\left(\ket{\phi^{+}}+\ket{\phi^{-}}+\ket{\psi^{+}}+\ket{\psi^{-}}\right)=\frac{1}{\sqrt{2}}\left(\ket{00}+\ket{01}\right)\textrm{ .}
% \]
% Thus, $\proj{\phi^{+}}+\proj{\psi^{+}}$ and $\proj{\phi^{+}}+\proj{\psi^{-}}$
% are independent projections with respect to the probability
% measure~$\mu\left(P\right)=\melement{\psi}{P}$.\qed\end{example}}

\footnote{\yutsung{The definition of independence is interesting, and we
definitely need to discuss it when we want to discuss Bell's theorem and the Kochen-Specker
theorem. However, the definition diverges. So we just leave it so far, and we
will go back if we really need it in this paper. (Maybe when discussing repeating
experiments?)}}

Another difference between classical and quantum probability space. For a
given set of events $\events$, there are many possible probability measures $\mu:\events\rightarrow[0,1]$. The Born rule~\cite{Born1983,Mermin2007,RiederSvozil2007},
a postulate of quantum mechanics, states that each pure normalized
($\ip{\phi}{\phi}=1$) quantum state $\ket{\phi}$ induces a probability measure
$\mu^B_{\phi}$ as follows: 
\[
\mu^B_{\phi}(P)=\ip{\phi}{P\phi}
\]

Moreover, the Born rule can be extended to a mixed state. Suppose
we want to prepare a quantum system. As we discussed in classical
probability, our ability of preparing a state might not be perfect.
If we want to prepare $\ket{\phi}$, we may turn out preparing a set
of state~$\ket{\phi_{j}}$ each with probability~$q_{j}$. Then,
the state of the system can be expressed as a density matrix~$\rho=\sum_{j}q_{j}\proj{\phi_{j}}$,
where $\sum_{j}q_{j}=1$. It is natural that the quantum probability
measure introduced by $\rho$ is the combination of $\mu^B_{\phi_{j}}$
with respect to probability $q_{j}$~\cite{peres1995quantum,544199,RiederSvozil2007}:
\begin{eqnarray}
\mu^B_{\rho}\left(P\right) & = & \Tr\left(\rho P\right)=\sum_{j=1}^{N}q_{j}\mu^B_{\phi_{j}}\left(P\right)\textrm{ .}\label{BornRule.mixed}
\end{eqnarray}

Conversely, in Hilbert spaces of dimension $d\geq3$, Gleason's theorem
states that given a probability measure $\mu$, there exist a mixed
state~$\rho$ that induces such a measure using the Born rule~\cite{gleason1957,Redhead1987-REDINA,peres1995quantum}.
By applying Gleason's theorem, we can extend lemma~\ref{lem:quantumProbabilityMeasure}
to the following corollary.

\begin{cor}\label{cor:Gleason's}Given a Hilbert space $\Hilb$ of dimension
$d\geq3$, for any function~$\mu:\events\rightarrow[0,1]$ satisfying the conditions
listed in lemma~\ref{lem:quantumProbabilityMeasure}, there exist
an unique mixed state~$\rho$ such that $\mu=\mu^B_{\rho}$.\end{cor}

It is instructive to study counterexamples when $d=2$, i.e., the
case of a one-qubit system.

\begin{example}[One-qubit quantum probability measure] Consider
a quantum probability measure~$\mu:\events\rightarrow[0,1]$ defined
as follow: 
\[
\mu(P)=\begin{cases}
1 & \textrm{, if }P=\proj{\ps}\textrm{ ;}\\
0 & \textrm{, if }P=\proj{\ms}\textrm{ ;}\\
\mu^B_{\ket{0}}(P) & \textrm{, otherwise.}
\end{cases}
\]
On one hand, $\mu$ is a quantum probability measure. Because $\mu$ is almost
the same as a quantum probability measure~$\mu^B_{\ket{0}}$, we only
need to check the orthogonal pair~$\proj{\ps}$ and $\proj{\ms}$:
\[
\mu(\proj{\ps})+\mu(\proj{\ms})=1+0=1\textrm{ .}
\]
On the other hand, $\mu$ cannot be induced by any mixed state because
\[
\mu(\proj{\ps})=\mu(\proj{0})=1\textrm{ .}
\]
However, $\mu^B_{\rho}(P)=1$ if and only if $\rho$ represents
a pure state and $\rho=P$. \qed\end{example}

%%%%%
\subsection{Measuring Quantum Probabilities}

Similar to the classical case, by applying the law of large number,
quantum probabilities can be estimated by relative frequencies. For
example, if we want to know the probability of the spin up in the
Stern-Gerlach experiment~\cite{Stern1988,peres1995quantum,544199,Griffiths2003},
we can put a beam of silver atoms in a highly inhomogeneous magnetic
field, and counting the number of atoms deflects up. Ideally, if the
local field strength directs to the $z$-axis, and all particles have
the same velocity, the Stern-Gerlach experiment only produces two
spots corresponding to $\ket{0}$ and $\ket{1}$. In reality, we need
a lot of resource to keep the local field of strength directing to
the $z$-axis precisely, to keep the atoms having almost the same velocity,
and to point out the exact position each particle landed on. 

If the field of strength does not perfectly direct to the $z$-axis,
we does not really test the quantum event we want to test. This problem
will be handled later when we introduce the discrete quantum theory.
Even if the field of strength is perfectly direct to the $z$-axis,
the variant velocity makes spots broader and more washed out. Together
with the precision limit of the detector, it may sometimes be hard
to decide a particle corresponding to which state\footnote{\yutsung{Add citations to support the idea... Haven't found suitable
ones...}}. Similar to Buffon's needles, this kind of fuzziness can be taken
into account by associating each quantum event with an interval-valued
probability~$\left[l,r\right]$. 

\amr{preparation fuzzy, device fuzzy, Meyer~\cite{PhysRevLett.83.3751}}

%%%%%


\subsection{Quantum Interval-valued Probability Measures}

As mentioned above, each quantum event~$P$ will be associated with
an interval-valued probability~$\left[l,r\right]$. Notice that whenever
the magnetic field of the Stern-Gerlach is fixed, i.e., an ideal measurement
is picked, measuring the spin is exactly the same as tossing a coin.
In another word, if somebody claimed she is tossing a coin behind
a veil, and only show the resulting heads or tails, we cannot distinguish
whether she has really tossed a coin, or she has run an Stern-Gerlach
experiment, and show us the head, the tail, or the side of coin if
the silver atoms is spin up, spin down, or just hit the middle, respectively.
Therefore, we should plug in the definition of interval-valued probability
measure into the definition of quantum probability space for each
ideal measurement, and get the following definition.

\begin{definition}[Quantum Interval-valued Probability Measure]\label{def:QuantumInterval-valuedProbability}
Given a Hilbert space $\Hilb$ with the set of quantum events~$\events$,
and a collection of intervals~$\mathscr{I}$, a \emph{quantum $\mathscr{I}$-interval-valued
probability measure} is a function~$\bar{\mu}:\events\rightarrow\mathscr{I}$
such that: 
\begin{itemize}
\item $\bar{\mu}(\mathbb{0})=\left[0,0\right]$, 
\item $\bar{\mu}(\mathbb{1})=\left[1,1\right]$, 
\item for any projection $P$,
$\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}P\right)=1\tensor*[_{\mathscr{I}}]{-}{}\bar{\mu}\left(P\right)$, and
\item for a set of mutually orthogonal projections $P_{i}$, we have
$\bar{\mu}\left(\tensor*[_{\mathscr{O}}]{\sum}{_{i}}P_{i}\right)\subseteq\tensor*[_{\mathscr{I}}]{\sum}{_{i}}\bar{\mu}\left(P_{i}\right)$.
\end{itemize}
As before, we will only define a quantum interval-valued probability measure
with preposing $\mathscr{I}$ when we want to compare a quantum probability
measure and a quantum interval-valued probability measure.
\qed\end{definition}

Notice that if the last condition is replaced by the equal sign, these
conditions will be exactly the same as the conditions in lemma~\ref{lem:quantumProbabilityMeasure}
which is equivalent to the Born rule as in corollary~\ref{cor:Gleason's}.
Therefore, it is reasonable to believe these conditions should be
equivalent to a interval-valued Born rule. 

When we pick $\mathscr{I}=\left\{ \left[x,x\right]\middle|x\in\left[0,1\right]\right\} $,
for any quantum probability measure~$\mu:\events\rightarrow\left[0,1\right]$,
we can define a corresponding quantum \emph{$\mathscr{I}$}-interval-valued
probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$
by $\bar{\mu}\left(P\right)=\left[\mu\left(P\right),\mu\left(P\right)\right]$.
In this sense, every usual quantum probability measure can be considered
as a quantum interval-valued probability measure. There are more interesting
examples as we now show.

\begin{example}[Quantum three-value interval-valued probability
measure] We consider three intervals\emph{ }$\left[0,0\right]$,
$\left[1,1\right]$ and \emph{$\left[0,1\right]$}, where $\left[0,0\right]$
and $\left[1,1\right]$ are called \imposs~and \necess~as before,
and \emph{$\left[0,1\right]$} is called \unknown~because it provides
no information. For any Hilbert space and any quantum probability
measure~$\mu:\events\rightarrow\left[0,1\right]$,
we can define a quantum interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$
by 
\[
\bar{\mu}(P)=\iota\left(\mu(P)\right)\textrm{ ,}
\]
where $\iota:\left[0,1\right]\rightarrow\mathscr{I}$ is defined by
\[
\iota(x)=\begin{cases}
\necess & \textrm{, if }x=1\textrm{ ;}\\
\imposs & \textrm{, if }x=0\textrm{ ;}\\
\unknown & \textrm{, otherwise.}
\end{cases}
\]
$\iota$ has two interesting properties 
\begin{eqnarray*}
\iota\left(\mathbb{1}\tensor*[_{\mathbb{R}}]{-}{}x\right) & = & 1\tensor*[_{\mathscr{I}}]{-}{}\iota\left(x\right)\\
\iota\left(\sideset{_{\mathbb{R}}}{_{i}}\sum x_{i}\right) & \subseteq & \sideset{_{\mathscr{I}}}{_{i}}\sum\iota\left(x_{i}\right)\textrm{ ,}
\end{eqnarray*}
where $x$ and $\sideset{_{\mathbb{R}}}{_{i}}\sum x_{i}\in\left[0,1\right]$.
By applying these two properties, it is easy to verify $\bar{\mu}$
is a quantum \emph{$\mathscr{I}$}-interval-valued probability measure.\qed\end{example}

Notice that in the above two examples, quantum interval-valued probability
measures are all come from quantum probability measures because the
numbers in $\left[0,1\right]$ can be mapped to the chosen intervals~$\mathscr{I}$
naturally. This is not always the case. For example, if $\mathscr{I}=\left\{ \imposs,\unlikely,\likely,\necess\right\} $,
there is no natural way to map from $\frac{1}{2}$, as both $\frac{1}{2}\in\left[0,\frac{1}{2}\right]=\unlikely$
and $\frac{1}{2}\in\left[\frac{1}{2},1\right]=\likely$. However,
we still can find a quantum interval-valued probability measure corresponding
to a quantum probability measure as in the following example. 

\begin{example}[One-qubit quantum interval-valued probability measure]
Given a two dimensional Hilbert space, a vector $\ket{\psi}$ can
be normalized and written in $\left(\begin{array}{c}
\cos\theta\\
\rme^{\rmi\gamma}\sin\theta
\end{array}\right)$, where $0\le\theta\le\frac{\pi}{2}$ and $0\le\gamma\le2\pi$. Consider
a quantum interval-valued probability measure~$\bar{\mu}:\events\rightarrow\mathscr{I}$
defined by 
\begin{eqnarray*}
\bar{\mu}(\mathbb{0}) & = & \imposs\textrm{ ,}\\
\bar{\mu}(\mathbb{1}) & = & \necess\textrm{ ,}\\
\bar{\mu}(\proj{\psi}) & = & \begin{cases}
\necess & \textrm{, if }\theta=0\textrm{ ;}\\
\likely & \textrm{, if }0<\theta<\frac{\pi}{4}\textrm{ ;}\\
\likely & \textrm{, if }\theta=\frac{\pi}{4}\textrm{ and }0\le\gamma<\pi\textrm{ ;}\\
\unlikely & \textrm{, if }\theta=\frac{\pi}{4}\textrm{ and }\pi\le\gamma<2\pi\textrm{ ;}\\
\unlikely & \textrm{, if }\frac{\pi}{4}<\theta<\frac{\pi}{2}\textrm{ ;}\\
\imposs & \textrm{, if }\theta=\frac{\pi}{2}\textrm{ .}
\end{cases}
\end{eqnarray*}
Let $\ket{\psi^{\perp}}$ be the state perpendicular to $\ket{\psi}$.
Then, the vector representation of $\ket{\psi^{\perp}}$ can be simplified
as 
\[
\left(\begin{array}{c}
-\rme^{\rmi\gamma}\sin\theta\\
\cos\theta
\end{array}\right)=-\rme^{\rmi\gamma}\left(\begin{array}{c}
\cos\left(\frac{\pi}{2}-\theta\right)\\
\rme^{\rmi\left(\pi-\gamma\right)}\sin\left(\frac{\pi}{2}-\theta\right)
\end{array}\right)
\]
so we have
\[
\bar{\mu}\left(\mathbb{1}\tensor*[_{\mathscr{O}}]{-}{}\proj{\psi}\right)=\bar{\mu}\left(\proj{\psi^{\perp}}\right)=1\tensor*[_{\mathscr{I}}]{-}{}\bar{\mu}\left(\proj{\psi}\right)\textrm{ .}
\]
Hence, $\bar{\mu}$ is a quantum $\mathscr{I}$-interval-valued
probability measure. Notice that for all projection~$P$, $\mu^B_{\ket{0}}(P)\in\bar{\mu}(P)$
implies that $\bar{\mu}(P)$ is one of quantum
$\mathscr{I}$-interval-valued probability measures corresponding
to $\mu^B_{\ket{0}}(P)$. However,
$\mu^B_{\ket{0}}(P)$ can correspond
to infinity many quantum $\mathscr{I}$-interval-valued probability
measures by twisting the definition involving $\gamma$.\qed\end{example}

We have enough examples for quantum interval-valued probability measure,
it is instructive to understand the conditions by studying a measure
which is not a quantum interval-valued probability measure.

\begin{example}[Not three-dimensional quantum interval-valued probability
measure] Given a three dimensional Hilbert space with an orthonormal
basis $\left\{ \ket{0},\ket{1},\ket{2}\right\} $. Consider\emph{
$\mathscr{I}=\left\{ \necess,\imposs,\unknown\right\} $}, and $\ket{\ps}=\frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$
as before. Let
\begin{eqnarray*}
\bar{\mu}(\mathbb{0}) & = & \imposs\textrm{ ,}\\
\bar{\mu}(\mathbb{1}) & = & \necess\textrm{ ,}\\
\bar{\mu}(\proj{0}) & = & \necess\textrm{ ,}\\
\bar{\mu}(\proj{\ps}) & = & \necess\textrm{ ,}\\
\bar{\mu}(\proj{\psi}) & = & \imposs\textrm{, for }\ip{\psi}{0}=0\textrm{ or }\ip{\psi}{\ps}=0\textrm{ ,}\\
\bar{\mu}(1-\proj{0}) & = & \imposs\textrm{ ,}\\
\bar{\mu}(1-\proj{\ps}) & = & \imposs\textrm{ ,}\\
\bar{\mu}(1-\proj{\psi}) & = & \necess\textrm{, for }\ip{\psi}{0}=0\textrm{ or }\ip{\psi}{\ps}=0\textrm{ ,}\\
\bar{\mu}(P) & = & \unknown\textrm{, otherwise. }
\end{eqnarray*}
$\bar{\mu}$ is not a quantum interval-valued probability measure. To verify
this fact, we first consider an orthonormal basis
\begin{eqnarray*}
\ket{\psi_{0}} & = & \frac{1}{\sqrt{2}}\ket{1}+\frac{1}{\sqrt{2}}\ket{2}\\
\ket{\psi_{1}} & = & \frac{1}{\sqrt{3}}\ket{0}-\frac{1}{\sqrt{3}}\ket{1}+\frac{1}{\sqrt{3}}\ket{2}\\
\ket{\psi_{2}} & = & \sqrt{\frac{2}{3}}\ket{0}+\frac{1}{\sqrt{6}}\ket{1}-\frac{1}{\sqrt{6}}\ket{2}
\end{eqnarray*}
However, 

\begin{eqnarray*}
 &  & \bar{\mu}\left(\proj{\psi_{0}}+\proj{\psi_{1}}\right)=\bar{\mu}\left(1-\proj{\psi_{2}}\right)=\unknown\\
 & \nsubseteq & \imposs+\imposs=\bar{\mu}\left(\proj{\psi_{1}}\right)+\bar{\mu}\left(\proj{\psi_{2}}\right)\textrm{ .}
\end{eqnarray*}
\qed\end{example}

\yutsung{For \emph{$\mathscr{I}=\left\{ \necess,\imposs,\unknown\right\} $},
are there any quantum interval-valued probability measures do not
come from quantum probability measures?}

\amr{
We can use DQC if we have some kind of topology (distances). The idea
will be that we want to prepare state PSI but because of errors etc we
prepare a close state. Well the next closest state will be the next
state in our discrete grid. I am sure that a state that's very close
to PSI can involve some wrapping around.
}

\yutsung{About continuity, one of the confusion comes from the subtle
correspondence between the measurement processes and observables.
Consider the following observables and their measurement processes:
\begin{itemize}
\item For any $x\in\mathbb{R}$, consider the observable operator~$\mathbf{O}_{x}=\proj{1}+x\proj{-1}$.
Except $x=1$, given the system in the state~$\ket{\phi}$, we can
denote the measurement process for $\mathbf{O}_{x}$ by ${\cal O}_{x}$:
\begin{enumerate}
\item With probability~$\mu_{\phi}\left(\proj{1}\right)$, return the measurement
result~$1$ with the post-measurement state~$\ket{1}$;
\item With probability~$\mu_{\phi}\left(\proj{0}\right)$, return the measurement
result~$0$ with the post-measurement state~$\ket{0}$;
\item With probability~$\mu_{\phi}\left(\proj{-1}\right)$, return the
measurement result~$x$ with the post-measurement state~$\ket{-1}$.
\end{enumerate}
\item The observable operator~$\mathbf{J}_{z}^{2}=\proj{1}+\proj{-1}$,
and its measurement process~${\cal J}_{z}^{2}$:
\begin{enumerate}
\item With probability~$\mu_{\phi}\left(\proj{1}+\proj{-1}\right)$, return
the measurement result~$1$ with the post-measurement state~$\ket{1}\ip{1}{\phi}+\ket{-1}\ip{-1}{\phi}$;
\item With probability~$\mu_{\phi}\left(\proj{0}\right)$, return the measurement
result~$0$ with the post-measurement state~$\ket{0}$.
\end{enumerate}
\end{itemize}
As an operator, it is clear that $\lim_{x\rightarrow1}\mathbf{O}_{x}=\mathbf{J}_{z}^{2}$,
and we should have $\lim_{x\rightarrow1}{\cal O}_{x}={\cal O}_{1}$
for the measurement process. However, it is not obvious why ${\cal O}_{1}={\cal J}_{z}^{2}$.
Although they have the same probability for the measurement results,
but their post-measurement states are different. This might be a defect
of representing a measurement process as an observable. As $\mathbf{O}_{1}=\mathbf{J}_{z}^{2}$,
there is no observable corresponding to the measurement process~${\cal O}_{1}$.}

\amr{the rest needs cleaning up and perhaps does not even belong in
  this section}

Although it seems that we need an infinite long table to specify the
quantum probability measure~$\mu$, our $\mu$ is actually
given by a simple formula~$\melement{0}{P}$. In general, Born discovered
each quantum state $\ket{\psi}\in\Hilb\backslash\left\{ 0\right\} $
induces a probability measure $\mu^B_{\psi}:\qevents\rightarrow[0,1]$
on the space of events defined for any event $P\in\qevents$ as follows~\cite{Born1983,Mermin2007}:
\begin{equation}
\mu^B_{\psi}(P)=\frac{\melement{\psi}{P}}{\ip{\psi}{\psi}}\label{eq:Born}
\end{equation}
The Born rule satisfies the following properties:
\begin{itemize}
\item It can be extend to mixed states. Given a mixed state represented
by a density matrix $\rho=\sum_{j=1}^{N}q_{j}\frac{\proj{\psi_{j}}}{\ip{\psi_{j}}{\psi_{j}}}$,
where $\sum_{j=1}^{N}q_{j}=1$, i.e., $\Tr\left(\rho\right)=1$, then
the Born rule can be extended to $\rho$ by 
\begin{eqnarray}
\mu^B_{\rho}\left(P\right) & = & \Tr\left(\rho P\right)=\sum_{j=1}^{N}q_{j}\mu^B_{\Psi_{j}}\left(P\right)\textrm{ .}\label{BornRule.mixed}
\end{eqnarray}
Notice that $\left(\left\{ 1,\ldots,N\right\} ,2^{\left\{ 1,\ldots,N\right\} },\pmeas\left(J\right)=\sum_{j\in J}q_{j}\right)$
is a classical probability space. Therefore, when we discretize the
Hilbert space later, we may need to discretize this probability space
as well.
\item $\mu^B_{\rho}$ is a probability measure for all mixed state~$\rho$.
\item $\ip{\psi}{\phi}=0\Leftrightarrow\mu^B_{\psi}\left(\proj{\phi}\right)=0$.
\item $\mu^B_{\psi}\left(P\right)=\mu^B_{\mathbf{U}\ket{\psi}}\left(\mathbf{U}P\mathbf{U}^{\dagger}\right)\textrm{ ,}$where
$\mathbf{U}$ is any unitary map, i.e., $\mathbf{U}^{\dagger}\mathbf{U}=\mathbb{1}$. 
\end{itemize}

Naturally, we may ask: is every probability measure induced from a
state by the Born rule? The answer is yes by Gleason's theorem when
the dimension~$\ge3$~\cite{gleason1957,peres1995quantum,Redhead1987-REDINA}.
Furthermore, a simple corollary of Gleason's theorem can show the
Born rule is the unique function satisfying conditions 1. to 3.
\begin{cor}
The Born rule is the unique function satisfying conditions 1. to 3.
\end{cor}
\begin{proof}
Assume there is another function $\mu^{\prime B}$ such that $\mu^{\prime B}_{\rho}$
is a quantum probability measure for all mixed state~$\rho$. We
are going to prove $\mu^{\prime B}=\mu^B$.

Fix a pure normalized state $\phi$, $\mu^{\prime B}_{\phi}$ is a quantum
probability measure by condition 2. By Gleason's theorem, there is
a mixed state ~$\rho'$, such that $\mu^{\prime B}_{\phi}\left(P\right)=\Tr\left(\rho'P\right)=\sum_{j=1}^{N}q_{j}\mu^B_{\psi_{j}}\left(P\right)$
for all event $P$. 

Consider the event $P'=\mathbb{1}-\proj{\phi}$, we have 
\begin{eqnarray*}
0 & \overset{\textrm{Condition 3}}{=} & \mu^B_{\phi}\left(P'\right)\\
 & = & \sum_{j=1}^{N}q_{j}\mu^B_{\psi_{j}}\left(P'\right)
\end{eqnarray*}
Because $q_{j}>0$, we have $\mu^B_{\psi_{j}}\left(P\right)=0$,
i.e., $\psi_{j}$ is orthogonal to a co-dimension-$1$ subspace $P'$.
However, the only subspace orthogonal to $P'$ is span by $\ket{\phi}$.
Hence, $\mu^{\prime B}_{\phi}=\mu^B_{\phi}$.
\end{proof}



% If there may be more than one probability measure, we will discuss
% whether we will keep using the Born rule (\ref{eq:Born}) or there
% is another formula $\mu^B$ such that $\mu^B_{\psi}$
% is a probability measure for all $\ket{\psi}\in\Hilb\backslash\left\{ 0\right\} $.

% Then, Gleason's theorem states given a probability measure $\mu$
% there is a mixed state $\ket{\psi}$ such that $\mu=\mu_{\psi}$,
% i.e., the Born rule is surjective\footnote{If we extend the domain of $\mu_{\psi}$ including the mixed states.}.
% For any other formula~$\mu^B$, we can ask whether $\mu^B$
% is surjective as well.

% Obviously, we don't want arbitrarily assign a state~$\ket{\psi}\in\Hilb\backslash\left\{ 0\right\} $
% with a probability measure $\mu^B_{\psi}$, for example, assigning
% every state to the same probability measure. We want $\mu^B_{\psi}$
% satisfying the following properties with some physical meaning:
% \begin{itemize}
% \item $\ip{\psi}{\phi}=0\Leftrightarrow\mu^B_{\psi}\left(\proj{\phi}\right)=\tilde{0}$,
% where $\tilde{0}$ is $0$ for $\left[0,1\right]$ and $\tilde{0}$
% is impossible for $\mathscr{L}_{2}=\left\{ \imposs,\poss\right\} $. 
% \item $\mu^B_{\psi}\left(\proj{\phi}\right)=\mu^B_{\mathbf{U}\ket{\psi}}\left(\mathbf{U}\proj{\phi}\mathbf{U}^{\dagger}\right)\textrm{ ,}$where
% $\ket{\psi},\ket{\phi}$ are states and $\mathbf{U}$ is any unitary
% map, i.e., $\mathbf{U}^{\dagger}\mathbf{U}=\mathbb{1}$. 
% \end{itemize}
% And the results can be summarized in the following table:\\
% \center{ %
% \begin{tabular}{>{\raggedright}m{0.2\columnwidth}>{\raggedright}m{0.2\columnwidth}>{\raggedright}m{0.2\columnwidth}>{\raggedright}m{0.2\columnwidth}}
% \hline 
% State space $\Hilb$  & Probability values  & Is there a $\mu^B$ satisfying the given conditions?  & Is the $\mu^B$ surjective?\tabularnewline
% \hline 
% \hline 
% $\C^{d}$ for $d\ge3$  & $\left[0,1\right]$  & Yes  & Yes\tabularnewline
% \hline 
% $\C^{d}$  & $\mathscr{L}_{2}$  & Yes  & No\tabularnewline
% \hline 
% $\ffzd{p^{2}}$for $d\ge3$ except $d=p=3$  & $\left[0,1\right]$  & No  & \tabularnewline
% \hline 
% $\ffzd{p^{2}}$  & $\mathscr{L}_{2}$  & Yes & No\tabularnewline
% \hline 
% \end{tabular}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{All Continuous or All Discrete}

Before we turn to the main part of the paper, we quickly dismiss the
possibility of having one but not the other of the discrete
variations. Specifically, it is impossible to maintain the Hilbert
space and have a finite set-valued probability measure and it is also
impossible to have a vector space constructed over a finite field with
a real-valued probability measure. 

%%%
\subsection{Hilbert Space with Finite Set-Valued Probability Measure}

However, there is a $\mathscr{L}_{2}$-valued probability measure
\[
\hat{\mu}_{1}\left(P\right)=\begin{cases}
\imposs & \textrm{, if }P=\proj{+};\\
\bar{\mu}(P) & \textrm{, otherwise.}
\end{cases}
\]
such that $\hat{\mu}_{1}\ne\bar{\mu}_{\psi}$ for all mixed state
$\ket{\psi}$.

%%%
\subsection{Discrete Vector Space with Real-Valued Probability Measure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrt}
\bibliography{discreteGBKS}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Probability space (restricted to finite sample spaces)

* Sample space Ω (arbitrary finite non-empty set)
* Set of events F: pick 2^Ω
* Probability measure (real-valued): P : F → [0,1] such that:
  * For any collection of pairwise disjoint Aᵢ ∈ F, we have P ( ∪ᵢ Aᵢ) = ∑ᵢ P(Aᵢ) 
  * P(Ω) = 1


Set-value probability measures. Change the last bullet to:
* P : F → 2^(ℝⁿ) such that: ...

%%%%%
\subsection{Sample Space $\Omega$} 

In this paper, we will only consider \textbf{finite} sample spaces. We
therefore define a sample space $\Omega$ as a non-empty finite set.

\begin{example}[A Classical Sample Space.]
Consider an experiment that tosses three coins. A possible outcome of
the experiment is $HHT$ which means that the first and second coins
landed with ``heads'' as the face-up side and that the third coin
landed with ``tails'' as the face-up side. There are clearly a total
of eight possible outcomes, and this collection constitutes the sample
space:
\[
\Omega_C = \{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT \}
\]
\end{example}

\begin{example}[A Quantum Sample Space.]
Consider a quantum system composed of three electrons. By the
postulates of quantum mechanics, an experiment designed to measure
whether the spin of each electron along the $x$ axis is left ($L$) or
right ($R$) can only result in one of eight outcomes:
\[
\Omega_H = \{ LLL, LLR, LRL, LRR, RLL, RLR, RRL, RRR \}
\]
\end{example}

%%%%%
\subsection{Events $\mathcal{F}$} 

The space of events $\mathcal{F}$ associated with a sample space
$\Omega$ is $2^\Omega$, the powerset of $\Omega$. In other words,
every subset of $\Omega$ is a possible event.

\begin{example}[Some classical events.] 
The following are events associated with $\Omega_C$:
\begin{itemize}
\item $E_0$, exactly zero coins are $H$, is the set $\{ TTT \}$.
\item $E_1$, exactly one coin is $H$, is the set $\{ HTT, THT, TTH \}$. 
\item $E_2$, exactly two coins are $H$, is the set $\{ HHT, HTH, THH \}$.
\item $E_3$, exactly three coins are $H$, is the set $\{ HHH \}$. 
\item $E_{>0}$, at least one coin is $H$, is the set $\{ HHH, HHT, HTH, HTT, THH, THT, TTH \}$. 
\end{itemize}
As the examples illustrate, events are \emph{indirect} questions built from elementary elements of the sample space using logical connectives. Also
note that some events may be disjoint and that some events may be
expressed as combinations of other events. For example, we have
$E_{>0} = E_1 \cup E_2 \cup E_3$ and each of these four events is
disjoint from event $E_0$.
\end{example}

\begin{example}[Some quantum events.] 
The following are events associated with $\Omega_H$:
\begin{itemize}
\item $F_0$, exactly zero electrons are spinning $L$, is the set $\{ RRR \}$.
\item $F_1$, exactly one electron is spinning $L$, is the set $\{ LRR, RLR, RRL \}$. 
\item $F_2$, exactly two electrons are spinning $L$, is the set $\{ LLR, LRL, RLL \}$.
\item $F_3$, exactly three electrons are spinning $L$, is the set $\{ LLL \}$. 
\item $F_{>0}$, at least one electron is spinning $L$, is the set $\{ LLL, LLR, LRL, LRR, RLL, RLR, RRL \}$. 
\end{itemize}
As the examples illustrate, quantum events are, at first glance,
similar to classical events. There are however some subtle
differences that we point out in the next section.
\end{example}

%%%%%
\subsection{Measures $\mathbb{P}$} 

The last ingredient of a probability space is a probability measure
$\mathbb{P} : \mathcal{F} \rightarrow [0,1]$ that assigns to each
event a real number in the closed interval $[0,1]$ subject to the
following conditions:
\begin{itemize}
\item $\mathbb{P}(\Omega) = 1$, and 
\item For any collection of pairwise disjoint events $A_i$, we have 
$\mathbb{P}(\bigcup_i A_i) = \Sigma_i ~\mathbb{P}(A_i)$.
\end{itemize}

\begin{example}[Classical probability measure]
There are $2^8$ events associated with $\Omega_C$. A possible probability measure for
these events is:
\[\begin{array}{c}
\mathbb{P}(E) = \left\{ \begin{array}{ll} 
  1 & \mbox{if}~E = \Omega \\
  0 & \mbox{otherwise} 
  \end{array}\right.
\end{array}\]
\yutsung{ Actually, the above $\mathbb{P}$ is not a probability
measure because 
\[
\mathbb{P}\left(\Omega\right)=1\ne0+0=\mathbb{P}\left(E_{0}\right)+\mathbb{P}\left(E_{>0}\right)
\]
} \\
A more interesting measure is defined recursively as follows:
\renewcommand\arraystretch{1.4}
\[\begin{array}{rcl}
\mathbb{P}(\emptyset) &=& 0 \\
\mathbb{P}(\{ HHH \} \cup E) &=& \frac{1}{5} + \mathbb{P}(E) \\
\mathbb{P}(\{ HHT \} \cup E) &=& \mathbb{P}(E) \\
\mathbb{P}(\{ HTH \} \cup E) &=& \frac{3}{10} + \mathbb{P}(E) \\
\mathbb{P}(\{ HTT \} \cup E) &=& \mathbb{P}(E) \\
\mathbb{P}(\{ THH \} \cup E) &=& \frac{1}{5} + \mathbb{P}(E) \\
\mathbb{P}(\{ THT \} \cup E) &=& \mathbb{P}(E) \\
\mathbb{P}(\{ TTH \} \cup E) &=& \frac{3}{10} + \mathbb{P}(E) \\
\mathbb{P}(\{ TTT \} \cup E) &=& \mathbb{P}(E) 
\end{array}\]
\yutsung{Because $\mathbb{P}(\bigcup_{i}A_{i})=\Sigma_{i}~\mathbb{P}(A_{i})$
requires disjoint events, the above formula should write like:
\[
\begin{array}{rcl}
\mathbb{P}(\emptyset) & = & 0\\
\mathbb{P}(\{HHH\}\cup E) & = & \frac{1}{5}+\mathbb{P}(E)\textrm{, if }HHH\notin E\\
\mathbb{P}(\{HHT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }HHT\notin E\\
\mathbb{P}(\{HTH\}\cup E) & = & \frac{3}{10}+\mathbb{P}(E)\textrm{, if }HTH\notin E\\
\mathbb{P}(\{HTT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }HTT\notin E\\
\mathbb{P}(\{THH\}\cup E) & = & \frac{1}{5}+\mathbb{P}(E)\textrm{, if }THH\notin E\\
\mathbb{P}(\{THT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }THT\notin E\\
\mathbb{P}(\{TTH\}\cup E) & = & \frac{3}{10}+\mathbb{P}(E)\textrm{, if }TTH\notin E\\
\mathbb{P}(\{TTT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }TTT\notin E
\end{array}
\]
Or add a sentence ``where the element in the singleton set is not
belong to $E$ for each equation.'' Or write like: 
\[
\begin{array}{rcl}
\mathbb{P}(\{HHH\}) & = & \frac{1}{5}\\
\mathbb{P}(\{HHT\}) & = & 0\\
\mathbb{P}(\{HTH\}) & = & \frac{3}{10}\\
\mathbb{P}(\{HTT\}) & = & 0\\
\mathbb{P}(\{THH\}) & = & \frac{1}{5}\\
\mathbb{P}(\{THT\}) & = & 0\\
\mathbb{P}(\{TTH\}) & = & \frac{3}{10}\\
\mathbb{P}(\{TTT\}) & = & 0\\
\mathbb{P}(E) & = & \sum_{\omega\in E}\mathbb{P}(\left\{ \omega\right\} )
\end{array}
\]
} \\
Because this is a \emph{classical} situation, the probability
assignments can be understood \emph{locally} and
\emph{non-contextually}. In other words, we can reason about each coin
separately and perform experiments on it ignoring the rest of the
context. If we were to perform such experiments we may find that for
the first coin, the probability of either outcome
$H$ or $T$ is $\frac{1}{2}$; for coin two, the probabilities are
skewed a little with the probability of outcome $H$ being
$\frac{2}{5}$ and the probability of outcome $T$ being $\frac{3}{5}$; and that
coin 3 is a fake double-headed coin where the probability of
outcome $H$ is 1 and the probability of outcome $T$ is 0. The reader
may check that these local observations are consistent with the
probability measure above. 
\end{example}

\begin{example}[Quantum probability measure] Like in the classical
case, there are $2^{8}$ events. But as Mermin explains in a simple
example~\cite{MerminPRL1990}, here is a possible probability measure:
\[
\begin{array}{rcl}
\mathbb{P}_{xxx}(\{LLL\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{LLR\}) & = & 0\\
\mathbb{P}_{xxx}(\{LRL\}) & = & 0\\
\mathbb{P}_{xxx}(\{LRR\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{RLL\}) & = & 0\\
\mathbb{P}_{xxx}(\{RLR\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{RRL\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{RRR\}) & = & 0\\
\mathbb{P}_{xxx}(E) & = & \sum_{\omega\in E}\mathbb{P}_{xxx}(\left\{ \omega\right\} )
\end{array}
\]
In contrast with the previous classical example, the event of different
electrons are not independent. More precisely, consider the event
for each electron separately:
\begin{eqnarray*}
F_{1,L} & = & \left\{ LLL,LLR,LRL,LRR\right\} \\
F_{2,L} & = & \left\{ LLL,LLR,RLL,RLR\right\} \\
F_{3,L} & = & \left\{ LLL,LRL,RLL,RRL\right\} 
\end{eqnarray*}
They are not independent means 
\[
\mathbb{P}_{xxx}(F_{1,L}\cap F_{2,L}\cap F_{3,L})=\mathbb{P}_{xxx}(\{LLL\})=\frac{1}{4}\ne\frac{1}{8}=\mathbb{P}_{xxx}(F_{1,L})\mathbb{P}_{xxx}(F_{2,L})\mathbb{P}_{xxx}(F_{3,L})\textrm{ .}
\]


Classical events may also not be independent even if they seems unrelated.
For example, events defined by the temperature is usually not independent
to ones defined by how much Coca-Cola is sold. Another example can
be formulated by tossing three coins as we discussed previously. However,
this time the coins are tossed behind a veil where someone tosses
the coins for you. Because we cannot see how she tosses the coins,
she might actually roll a four-sided tetrahedral die with $\{HHH,HTT,THT,TTH\}$
in its four faces. If $HTT$ is on the downward face, she places $H$,
$T$, and $T$ as the face-up sides of of the three coins by hand,
respectively. Then, she uncovers the veil, and claims she has tossed
the coins. If the coins are tossed in this way, the result of coin-tossing
is correlated, and we will never see $TTT$ no matter how many times
we toss these coins. 

Because we do not know how the spin of an electron is decided, Einstein,
Podolsky, and Rosen (EPR)~\cite{EPR1935} suggested the nature might
give us the probability measure~$\mathbb{P}_{xxx}$ because she rolled
a tetrahedral die or performed other classical and deterministic process
behind the veil. This claim may be convincing if $\mathbb{P}_{xxx}$
is the only probability measure we have, but will lead to a contradiction
if we consider other probability measures as well. Notice that after
the coins are placed by hand and before uncovering the veil, which
side up has already been decided although we do not know. This would
be also true for the quantum probability measure. Because the three
electrons can be spatially separated, and each electron can be measured
along the $x$ axis separately, if the nature rolled a tetrahedral
die, this die should be rolled before the electrons are separated
and measured, and she should know the result of measurement before
we measure the electrons. Let the result of the $j$-th electron measured
along the $x$ axis be $w\left(\sigma_{x}^{j}\right)$. Because 
\[
\mathbb{P}_{xxx}(\{LLR\})=\mathbb{P}_{xxx}(\{LRL\})=\mathbb{P}_{xxx}(\{RLL\})=\mathbb{P}_{xxx}(\{RRR\})=0\textrm{ ,}
\]
we have $w\left(\sigma_{x}^{1}\right)w\left(\sigma_{x}^{2}\right)w\left(\sigma_{x}^{3}\right)\in\{LLL,LRR,RLR,RRL\}$,
i.e., the number of $L$ in $w\left(\sigma_{x}^{1}\right)$, $w\left(\sigma_{x}^{2}\right)$,
and $w\left(\sigma_{x}^{3}\right)$ should be odd. 

The three electrons cannot be measured the spin only along the $x$
axis, but also along the $y$ axis with the result down ($D$) or
up ($U$). We only consider to measure even number of electrons along
the $y$ axis, and the probability measures could be defined by 
\begin{eqnarray*}
\begin{array}{rcl}
\mathbb{P}_{xyy}(\{LDD\}) & = & 0\\
\mathbb{P}_{xyy}(\{LDU\}) & = & \frac{1}{4}\\
\mathbb{P}_{xyy}(\{LUD\}) & = & \frac{1}{4}\\
\mathbb{P}_{xyy}(\{LUU\}) & = & 0\\
\mathbb{P}_{xyy}(\{RDD\}) & = & \frac{1}{4}\\
\mathbb{P}_{xyy}(\{RDU\}) & = & 0\\
\mathbb{P}_{xyy}(\{RUD\}) & = & 0\\
\mathbb{P}_{xyy}(\{RUU\}) & = & \frac{1}{4}
\end{array} & \begin{array}{rcl}
\mathbb{P}_{yxy}(\{DLD\}) & = & 0\\
\mathbb{P}_{yxy}(\{DLU\}) & = & \frac{1}{4}\\
\mathbb{P}_{yxy}(\{DRD\}) & = & \frac{1}{4}\\
\mathbb{P}_{yxy}(\{DRU\}) & = & 0\\
\mathbb{P}_{yxy}(\{ULD\}) & = & \frac{1}{4}\\
\mathbb{P}_{yxy}(\{ULU\}) & = & 0\\
\mathbb{P}_{yxy}(\{URD\}) & = & 0\\
\mathbb{P}_{yxy}(\{URU\}) & = & \frac{1}{4}
\end{array} & \begin{array}{rcl}
\mathbb{P}_{yyx}(\{DDL\}) & = & 0\\
\mathbb{P}_{yyx}(\{DDR\}) & = & \frac{1}{4}\\
\mathbb{P}_{yyx}(\{DUL\}) & = & \frac{1}{4}\\
\mathbb{P}_{yyx}(\{DUR\}) & = & 0\\
\mathbb{P}_{yyx}(\{UDL\}) & = & \frac{1}{4}\\
\mathbb{P}_{yyx}(\{UDR\}) & = & 0\\
\mathbb{P}_{yyx}(\{UUL\}) & = & 0\\
\mathbb{P}_{yyx}(\{UUR\}) & = & \frac{1}{4}
\end{array}
\end{eqnarray*}
with $\mathbb{P}_{ijk}(E)=\sum_{\omega\in E}\mathbb{P}_{ijk}(\left\{ \omega\right\} )$.
Similarly, the nature should predetermine $w\left(\sigma_{x}^{j}\right)$
and $w\left(\sigma_{y}^{j}\right)$ for them. Furthermore, because
she do not know along which axis we are going to measure, she should
predetermine the same $w\left(\sigma_{x}^{j}\right)$ and $w\left(\sigma_{y}^{j}\right)$
for all different probability measures. By the same reason as above,
the number of $L$ or $D$ in $\left\{ w\left(\sigma_{x}^{1}\right),w\left(\sigma_{y}^{2}\right),w\left(\sigma_{y}^{3}\right)\right\} $,
$\left\{ w\left(\sigma_{y}^{1}\right),w\left(\sigma_{x}^{2}\right),w\left(\sigma_{y}^{3}\right)\right\} $,
and $\left\{ w\left(\sigma_{y}^{1}\right),w\left(\sigma_{y}^{2}\right),w\left(\sigma_{x}^{3}\right)\right\} $
should be even. If we look these 9 letters carefully, we can find
that every $w\left(\sigma_{x}^{j}\right)$ appears once and every
$w\left(\sigma_{y}^{j}\right)$ appears twice. Hence, the number of
$L$ in $w\left(\sigma_{x}^{1}\right)$, $w\left(\sigma_{x}^{2}\right)$,
and $w\left(\sigma_{x}^{3}\right)$ should be even. This contradict
to the conclusion in our last paragraph. Therefore, EPR's assumption
is wrong, and it is not always true that the nature can predetermine
the measurement result before we perform the measurement. \end{example}


%%%%
\subsection{Finite Precision of Measurements}

In a laboratory setting or a computational setting, there are neither
uncountable entities nor uncomputable entities. We are thus looking at
alternative probability spaces which do not depend on the real numbers
and revisit the mysteries of quantum mechanics in that
setting. In other words, is it possible that at least part of the quantum mysteries related to probability and measurement are due to the reliance on uncomputable probability values? 

Following previous work on probability, we will replace the
closed interval $[0,1]$ by the \emph{finite set} $S = \{
\textbf{possible}, \textbf{impossible} \}$ and adapt the definition of
probability measure as follows.

A set-valued probability measure $\mathbb{P} : \mathcal{F} \rightarrow
S$ assigns to each event either the tag \textbf{possible} or the tag
\textbf{impossible} subject to the following conditions:
\begin{itemize}
\item $\mathbb{P}(\Omega) = \textbf{possible}$, and 
\item For any collection of pairwise disjoint events $A_i$, we have 
$\mathbb{P}(\bigcup_i A_i) = \textbf{possible}$ if any event $A_i$ is
\textbf{possible} and \textbf{impossible} otherwise. 
\end{itemize}

We begin by reviewing the conventional presentation of classical
probability spaces and then give an alternative formulation that is
``quantum-like'' but still classical. We conclude this section with a
definition of quantum probability spaces given as a modest
generalization of the alternative classical definition. 

%%%%%
\subsection{Conventional Classical Probability Spaces}

Textbook probability
theory~\cite{inun.425605319950101,GrahamKnuthPatashnik1994,rohatgi2011introduction}
is defined using the notions of a \emph{sample space} $\Omega$, a
space of \emph{events}~$\events$, and a \emph{probability
  measure}~$\pmeas$. In this paper, we will only consider
\emph{finite} sample spaces: we therefore define a sample space
$\Omega$ as an arbitrary non-empty finite set and the space of events
$\events$ as, $2^\Omega$, the powerset of $\Omega$. A \emph{probability
measure} is a function $\pmeas : \events \rightarrow [0,1]$ such that:
\begin{itemize}
\item $\pmeas(\Omega) = 1$, and 
\item for a collection of pairwise disjoint events $E_i$, we have
  $\pmeas(\bigcup E_i) = \sum \pmeas(E_i)$. 
\end{itemize}

\begin{example}[Two coin experiment] Consider an experiment that
  tosses two coins. We have four possible outcomes that constitute the
  sample space $\Omega = \{ HH, HT, TH, TT \}$. The event that the
  first coin lands heads up is $\{ HH, HT \}$; the event that the two
  coins land on opposite sides is $\{ HT, TH \}$; the event that at
  least one coin lands tails up is $\{ HT, TH, TT\}$. Depending on the
  assumptions regarding the coins, we can define several probability
  measures. Here is a possible one:
\[\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) &=& 0 \\
\pmeas(\{ HH \}) &=& 1/3 \\
\pmeas(\{ HT \}) &=& 0 \\
\pmeas(\{ TH \}) &=& 2/3 \\
\pmeas(\{ TT \}) &=& 0 \\
\pmeas(\{  HH, HT \}) &=& 1/3 \\
\pmeas(\{  HH, TH \}) &=& 1 \\
\pmeas(\{  HH , TT \}) &=& 1/3 
\end{array} & \begin{array}{rcl}
\pmeas(\{  HT, TH \}) &=& 2/3 \\
\pmeas(\{  HT , TT \}) &=& 0 \\
\pmeas(\{  TH , TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH \}) &=& 1 \\
\pmeas(\{  HH, HT, TT \}) &=& 1/3 \\
\pmeas(\{  HH, TH, TT \}) &=& 1 \\
\pmeas(\{  HT, TH, TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH, TT \}) &=& 1
\end{array}
\end{array}\]
\end{example}

%%%%%
\subsection{Alternative Definition of Classical Probability Spaces}

In the conventional presentation, we have viewed the space of events
$2^\Omega$ are the powerset of $\Omega$. We can equivalently view
$2^\Omega$ as the space of functions from $\Omega$ to the set
$2 = \{0,1\}$. For example, the event $\{HT,TH\}$ is the function $e$
such that:
\[
e (HH) = 0, \quad e (HT) = 1, \quad e (TH) = 1, \quad e (TT) = 0 
\]
We will in fact do a sweeping generalization and view events as
functions from $\Omega$ to $\mathbb{C}$, the set of complex
numbers. This accommodates the previous events such as $e$ and allows
many more events such as event $e'$ below:
\[
e' (HH) = \sqrt{2} + i \sqrt{3}, \quad e' (HT) = 1, \quad e' (TH) = \pi, \quad e' (TT) = 0 
\]
The events are not going to be completely arbitrary functions,
however. We will insist on some conditions:...

This generalization

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conventional Quantum Mechanics}

Attempting to modify the probability measure to be set-valued, while keeping the rest of the mathematical framework of quantum mechanics intact leads to a contradiction. More precisely, it is not possible to maintain infinite precision probability amplitudes in the presence of set-valued probabilities without violating essential aspects of quantum theory. 

\ldots explain and give theorem

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete Quantum Theory}

The next question to ask is therefore whether the infinite precision of probability amplitudes is itself justified. If all measurements are finite and all probabilities are computable, then it is plausible that the internal mathematical representation of quantum states should also be based on countable computable entities. 
