\documentclass{article}
\usepackage{fullpage}
\usepackage{url}
\usepackage{bbm}
\usepackage{bbold}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{array}
\usepackage{mathrsfs}
\usepackage[all]{xy}
\usepackage{amsmath}
\usepackage{wesa}
\usepackage[T1]{fontenc}

\makeatletter
\newtheoremstyle{indented}
  {3pt}% space before
  {3pt}% space after
  {\addtolength{\@totalleftmargin}{3.5em}
   \addtolength{\linewidth}{-3.5em}
   \parshape 1 3.5em \linewidth}% body font
  {}% indent
  {\bfseries}% header font
  {.}% punctuation
  {.5em}% after theorem header
  {}% header specification (empty for default)
\makeatother
\theoremstyle{indented}
%% \theoremstyle{remark}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{cor}{Corollary}
\newcommand{\events}{\ensuremath{\mathcal{E}}}
\newcommand{\qevents}{\ensuremath{\mathcal{E}}}
\newcommand{\pmeas}{\ensuremath{\mu}}
\newcommand{\Hilb}{\mathcal{H}}
\newcommand{\pr}[2]{\langle #1, #2 \rangle}
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle#1|}
\newcommand{\ip}[2]{\langle #1 | #2 \rangle}
\newcommand{\proj}[1]{|#1 \rangle\langle #1 |}
\newcommand{\ps}{\texttt{+}}
\newcommand{\ms}{\texttt{-}}
\newcommand{\poss}{{\mbox{\wesa{possible}}}}
\newcommand{\imposs}{{\mbox{\wesa{impossible}}}}
\newcommand{\likely}{{\mbox{\wesa{likely}}}}
\newcommand{\unlikely}{{\mbox{\wesa{unlikely}}}}
\newcommand{\necess}{{\mbox{\wesa{necessary}}}}
\newcommand{\overflow}{{\mbox{\wesa{overflow}}}}

\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\yutsung}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{purple}{Yu-Tsung says: #1}\end{minipage}}}
\newcommand{\amr}[1]{\fbox{\begin{minipage}{0.9\textwidth}\color{green}{Amr says: #1}\end{minipage}}}
\newcommand{\ffzd}[1]{{\mathbb{F}^{d\;*}_{#1}}}
\def\C{{\mathbb{C}}}
\newcommand{\ff}[1]{\mathbb{F}_{#1}}
\newcommand{\melement}[2]{ \langle #1 | #2 | #1 \rangle}
\newcommand{\expect}[2]{ \langle #1 | #2 | #1 \rangle}
\newcommand{\Tr}{\mathop{\mathrm{Tr}}\nolimits}

\begin{document}
\title{Discrete Probability for Discrete Quantum Computing}
\author{}
\date{\today}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classical Probability Spaces}
 
We review the conventional presentation of probability spaces and then
discuss several variations that avoid using the real interval $[0,1]$.

%%%
\subsection{Real-Valued Probability Spaces}

A \emph{probability
  space}~\cite{inun.425605319950101,GrahamKnuthPatashnik1994,rohatgi2011introduction}
specifies the necessary conditions for reasoning coherently about
collections of uncertain events. It consists of a \emph{sample space}
$\Omega$, a space of \emph{events}~$\events$, and a \emph{probability
  measure}~$\pmeas$. In this paper, we will only consider
\emph{finite} sets of events: we therefore define a sample space
$\Omega$ as an arbitrary non-empty finite set and the space of events
$\events$ as $2^\Omega$, the powerset of $\Omega$. Given the set of
events $\events$, a \emph{probability measure} is a function
$\pmeas : \events \rightarrow [0,1]$ such that:
\begin{itemize}
\item $\pmeas(\Omega) = 1$, and 
\item for a collection $E_i$ of pairwise disjoint events, 
  $\pmeas(\bigcup_i E_i) = \sum_i \pmeas(E_i)$.
\end{itemize}

\begin{example}[Two-coin probability space]\label{ex1}
  Consider an experiment that tosses two coins. We have four possible
  outcomes that constitute the sample space
  $\Omega = \{ HH, HT, TH, TT \}$. There are 16 total events including
  for example the event $\{ HH, HT \}$ that the first coin lands heads
  up, the event $\{ HT, TH \}$ that the two coins land on opposite
  sides, and the event $\{ HT, TH, TT\}$ that at least one coin lands
  tails up. Here is a possible probability measure for these events:
\[\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) &=& 0 \\
\pmeas(\{ HH \}) &=& 1/3 \\
\pmeas(\{ HT \}) &=& 0 \\
\pmeas(\{ TH \}) &=& 2/3 \\
\pmeas(\{ TT \}) &=& 0 \\
\pmeas(\{  HH, HT \}) &=& 1/3 \\
\pmeas(\{  HH, TH \}) &=& 1 \\
\pmeas(\{  HH , TT \}) &=& 1/3 
\end{array} & \begin{array}{rcl}
\pmeas(\{  HT, TH \}) &=& 2/3 \\
\pmeas(\{  HT , TT \}) &=& 0 \\
\pmeas(\{  TH , TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH \}) &=& 1 \\
\pmeas(\{  HH, HT, TT \}) &=& 1/3 \\
\pmeas(\{  HH, TH, TT \}) &=& 1 \\
\pmeas(\{  HT, TH, TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH, TT \}) &=& 1
\end{array}
  \end{array}\]
  The assignment satisfies the two constraints for probability measures:
  the probability of the entire sample space is 1, and the probability
  of every collection of disjoint events (e.g.,
  $\{ HT \} \cup \{ TH \} = \{ HT, TH \}$) is the sum of the individual
  probabilities. The probability of collections of non-disjoint events
  (e.g., $\{ HT, TH \} \cup \{ TH , TT \} = \{ HT, TH, TT \}$) may add
  to something different than the probabilities of the individual
  events. It is useful to think that this probability measure is
  completely induced by the two coins in question and their
  characteristics in the sense that each pair of coins induces a
  measure, and each measure must correspond to some pair of coins.  The
  measure above is induced by two coins such that the first coin is
  twice as likely to land tails up than heads up and the second coin is
  double-headed.  
  \qed\end{example}

In a strict computational or experimental setting, one may question
the reliance of the definition of probability space on the uncountable
and uncomputable real interval $[0,1]$. This interval includes numbers
like $0.h_1h_2h_3\ldots$ where $h_i$ is 1 or 0 depending on whether
Turing machine $M_i$ halts or not. Such numbers cannot be
computed. This interval also includes numbers like $\frac{\pi}{4}$
which can only be computed with increasingly large resources as the
precision increases.
% \yutsung{Check the meaning of the ``computing $\frac{\pi}{4}$'', because
% Bailey Borwein Plouffe formula can computing the $n$th binary digit of $\pi$ 
% using base 16 directly.}
Therefore, in a resource-aware setting, it is more appropriate to
consider probability measures that map events to a finite set of
elements computable with a fixed set of resources. We will consider
two approaches: set-valued probability
measures~\cite{Artstein1972,PuriRalescu1983} and interval-valued
probability
measures~\cite{Dempster1967,Zadeh1986,Weichselberger2000,JamisonLodwick2004}.

%%%%%
\subsection{Set-valued Probability Measures}  

Instead of using every point in the real interval $[0,1]$ we can
partition the interval into disjoint sets and only consider
probability measures up to set membership. The simplest such situation
is to partition the interval $[0,1]$ into $\{0\}$ (which we will call
$\imposs$) and the half-open interval $(0,1]$ (which we will call
$\poss$). The addition that was used to aggregate probabilities is now
abstracted to $\vee$ such that $x \vee y=\imposs$ if and only if
$x=y=\imposs$. We will call the resulting set
$\left\{ \imposs, \poss\right\}$ together with the with associated
operation $\vee$, the set $\mathscr{L}_{2}$. The definition of a
probability measure in this case is modified to a function
$\pmeas : \events \rightarrow \mathscr{L}_{2}$ such that:
\begin{itemize}
\item $\pmeas(\Omega) = \poss$, and
\item for a collection $E_i$ of pairwise disjoint events, $\pmeas(\bigcup_i
E_i) = \bigvee_i \pmeas(E_i)$.
\end{itemize}

\begin{example}[Two-coin probability space with finite set-valued
  probability measure] \label{ex2} Under the new set-valued
  requirement, the probability measure in the first example becomes:
\[\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) &=& \imposs \\
\pmeas(\{ HH \}) &=& \poss \\
\pmeas(\{ HT \}) &=& \imposs \\
\pmeas(\{ TH \}) &=& \poss \\
\pmeas(\{ TT \}) &=& \imposs \\
\pmeas(\{  HH, HT \}) &=& \poss \\
\pmeas(\{  HH, TH \}) &=& \poss \\
\pmeas(\{  HH , TT \}) &=& \poss \\
\end{array} & \begin{array}{rcl}
\pmeas(\{  HT, TH \}) &=& \poss \\
\pmeas(\{  HT , TT \}) &=& \imposs \\
\pmeas(\{  TH , TT \}) &=& \poss \\
\pmeas(\{  HH, HT, TH \}) &=& \poss \\
\pmeas(\{  HH, HT, TT \}) &=& \poss \\
\pmeas(\{  HH, TH, TT \}) &=& \poss \\
\pmeas(\{  HT, TH, TT \}) &=& \poss \\
\pmeas(\{  HH, HT, TH, TT \}) &=& \poss 
\end{array}
\end{array}\]
Despite the fact that we have lost all numeric information, the
probability measure still reveals that the second coin is
double-headed. We have however lost the information regarding the bias
in the first coin. This information can be recovered with a more
refined probability measure as we show next.  
\qed\end{example}

If we consider set with more values, the probability measure may give
us more information about the coins. For example, despite $\imposs$
and $\poss$, we may also adopt three more values: $\unlikely$\ as
the interval~$\left(0,\frac{1}{2}\right]$, $\likely$\ as the interval~$\left(\frac{1}{2},1\right]$,
and $\overflow$\ as the empty set~$\emptyset$ which means the
total probability may be excessed than one. In particular, $\likely\vee\likely=\overflow$
because the probability of two disjoint events should not both bigger
than $\frac{1}{2}$. The complete rule for operator~$\vee$ on $\mathscr{L}_{5}=\left\{ \imposs,\poss,\unlikely,\likely,\overflow\right\} $
is defined in table~\ref{tab:The-operator-on}, or more abstractly,
we define $(a,b]\vee(c,d]=(a+c,b+d]\cap[0,1]$. The definition of
a probability measure can be modified as a function $\pmeas:\events\rightarrow\mathscr{L}_{5}$
such that: 
\begin{itemize}
\item $\pmeas(\Omega)\in\left\{ \poss,\likely\right\} $ or equivalently
$1\in\pmeas(\Omega)$, and 
\item for a collection $E_{i}$, of pairwise disjoint events, $\pmeas(\bigcup_{i}E_{i})=\bigvee_{i}\pmeas(E_{i})$. 
\end{itemize}
\begin{table}
\center{%
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
$\vee$ & $\imposs$ & $\poss$ & $\unlikely$ & $\likely$ & $\overflow$\tabularnewline
\hline 
\hline 
$\imposs$ & $\imposs$ & $\poss$ & $\unlikely$ & $\likely$ & $\overflow$\tabularnewline
\hline 
$\poss$ & $\poss$ & $\poss$ & $\poss$ & $\likely$ & $\overflow$\tabularnewline
\hline 
$\unlikely$ & $\unlikely$ & $\poss$ & $\poss$ & $\likely$ & $\overflow$\tabularnewline
\hline 
$\likely$ & $\likely$ & $\likely$ & $\likely$ & $\overflow$ & $\overflow$\tabularnewline
\hline 
$\overflow$ & $\overflow$ & $\overflow$ & $\overflow$ & $\overflow$ & $\overflow$\tabularnewline
\hline 
\end{tabular}}

\caption{\label{tab:The-operator-on}The operator~$\vee$ on $\mathscr{L}_{5}=\left\{ \imposs,\poss,\unlikely,\likely,\overflow\right\} $}
\end{table}
\begin{example}{[}Two-coin probability space with $\mathscr{L}_{5}$-valued
probability measure{]} Under the new $\mathscr{L}_{5}$-valued requirement,
the probability measure in the first example becomes: 
\[
\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) & = & \imposs\\
\pmeas(\{HH\}) & = & \unlikely\\
\pmeas(\{HT\}) & = & \imposs\\
\pmeas(\{TH\}) & = & \likely\\
\pmeas(\{TT\}) & = & \imposs\\
\pmeas(\{HH,HT\}) & = & \unlikely\\
\pmeas(\{HH,TH\}) & = & \likely\\
\pmeas(\{HH,TT\}) & = & \unlikely
\end{array} & \begin{array}{rcl}
\pmeas(\{HT,TH\}) & = & \likely\\
\pmeas(\{HT,TT\}) & = & \imposs\\
\pmeas(\{TH,TT\}) & = & \likely\\
\pmeas(\{HH,HT,TH\}) & = & \likely\\
\pmeas(\{HH,HT,TT\}) & = & \unlikely\\
\pmeas(\{HH,TH,TT\}) & = & \likely\\
\pmeas(\{HT,TH,TT\}) & = & \likely\\
\pmeas(\{HH,HT,TH,TT\}) & = & \likely
\end{array}\end{array}
\]
In this example, we can get the information that the first coin is
weighted and the second coin is double-headed. \qed\end{example}

We will return to finite set-valued probability measures in Sec.~\ref{sec:?}.

%%%%%
\subsection{Interval-valued probability measures}  

A natural generalization of the disjoint set-valued measure above is
to allow the sets to overlap. In this case, we split the interval
$[0,1]$ in a collection of \emph{overlapping} closed
sub-intervals. First we illustrate the main ideas using a simple
example.

\begin{example}[Two-coin probability space with four
  intervals] \label{ex3} We split the unit interval $[0,1]$ in the
  following four closed sub-intervals: $[0,0]$ which we call \imposs,
  $[0,\frac{1}{2}]$ which we call \unlikely, $[\frac{1}{2},1]$ which
  we call \likely, and $[1,1]$ which we call \necess.  Using these new
  values, we can modify the probability measure of Ex.~\ref{ex1} by
  mapping each numeric value to the smallest sub-interval containing
  it to get the following:
\[
\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) & = & \imposs\\
\pmeas(\{HH\}) & = & \unlikely\\
\pmeas(\{HT\}) & = & \imposs\\
\pmeas(\{TH\}) & = & \likely\\
\pmeas(\{TT\}) & = & \imposs\\
\pmeas(\{HH,HT\}) & = & \unlikely\\
\pmeas(\{HH,TH\}) & = & \necess\\
\pmeas(\{HH,TT\}) & = & \unlikely
\end{array} & \begin{array}{rcl}
\pmeas(\{HT,TH\}) & = & \likely\\
\pmeas(\{HT,TT\}) & = & \imposs\\
\pmeas(\{TH,TT\}) & = & \likely\\
\pmeas(\{HH,HT,TH\}) & = & \necess\\
\pmeas(\{HH,HT,TT\}) & = & \unlikely\\
\pmeas(\{HH,TH,TT\}) & = & \necess\\
\pmeas(\{HT,TH,TT\}) & = & \likely\\
\pmeas(\{HH,HT,TH,TT\}) & = & \necess
\end{array}\end{array}
\]
This probability measure is more informative than the one in
Ex.~\ref{ex2}: not only does it reveal that the second coin is
double-headed but it also reveals the bias in the first coin.
\qed\end{example}

The probability measure above appears quite intuitive but it is not
really evident that it is well-defined. For example, how do we justify
the following combination of assignments:
\[ 
\pmeas(\{HH\}) = \unlikely, \quad\pmeas(\{TH\}) = \likely, \quad
\pmeas(\{HH\}\cup\{TH\}) = \necess 
\]
which assert that an \unlikely-event whose probability is in the range
$[0,\frac{1}{2}]$ and a \likely-event whose probability is in the
range $[\frac{1}{2},1]$ combine to a \necess-event whose probability
is in the range $[1,1]$. To understand the calculation, we provide the
formal definition of a probability measure in this case.

Fix a collection $\mathscr{I}$ of closed sub-intervals of $[0,1]$ that
must include $[0,0]$ and $[1,1]$. An $\mathscr{I}$--interval-valued
probability measure is a function
$\mu : \events \rightarrow \mathscr{I}$ such that:
\begin{itemize}
\item $\mu(\emptyset) = [0,0]$, 
\item $\mu(\Omega) = [1,1]$, and 
\item for any mutually disjoint events $E_1$, $E_2$, $E_3$, and $E_4$
  where $\pmeas(E_i) = [l_i,h_i]$ and where
  $E_1 \cup E_2 \cup E_3 \cup E_4 = \Omega$, we have:
\[
\pmeas(\{E_1\cup E_2\}) \subseteq
  [\max(l_1+l_2 , 1-(h_3+h_4)), \min(h_1+h_2 , 1-(l_3+l_4))]
\]
We view the events $E_1$ and $E_2$ as providing evidence for the
combined event $E_1\cup E_2$: their values set positive bounds on the
resulting interval. But as $E_3 \cup E_4$ is the complement of
$E_1 \cup E_2$, the events $E_3$ and $E_4$ provide negative bounds on
the resulting interval. The calculated interval may not be in our
fixed set $\mathscr{I}$ of chosen intervals so we embed it in the
smallest existing interval.
\end{itemize}






% to think of the above combination in the context of the evidence
% against the events established by the dual events. Writing in terms of
% the explicit intervals we see:
% \[\begin{array}{rcl@{\qquad}rcl}
% \pmeas(\{HH\}) & = & [0,\frac{1}{2}] & \pmeas(\{HT\}) & = & [0,0] \\
% \pmeas(\{TH\}) & = & [\frac{1}{2},1] &  \pmeas(\{TT\}) & = & [0,0]
% \end{array}\]
% The probability for $\pmeas(\{HH,TH\})$ is $[X,Y]$ where $X$ comes
% from the positive evidence for $\{HH\}$ and $\{TH\}$ and from the
% evidence against  is $[X,Y]$ where $X$ comes
% \[
% [\max(1,\frac{1}{2}),\min(1,\frac{3}{2})]
% \]

% must be at least as strong as the
% evidence for $\{HH\}$ and $\{TH\}$ individually and hence we have the
% tentative bounds on $\pmeas(\{HH,TH\})$ as
% $[\frac{1}{2},\frac{3}{2}]$. The dual events, however, establish
% evidence against $\{HH,TH\}$ with bounds $[

% A=HH
% B=TH
% C=HT
% D=TT


% To calculate the probability of $\{HH\}\cup\{TH\}$ we gather 
% evidence not only from the events $\{HH\}$ and $\{TH\}$, but also from 
% the dual events $\{HT,TH,TT\}$ and $\{HH,HT,TT\}$. The minimal
% probability for $\{HH\}\cup\{TH\}$ is\max(






% To understand this mystery, we look at
% another more general example and then give the formal mathematical
% definition of interval-valued probabilities. 

% \begin{example}[Dempster-Shafer Theory of Evidence] We have three
%   employees whose precise ages $A_1$, $A_2$, and $A_3$ are not
%   known. All is given is a range of ages for each employee:
%   $A_1 \in \{ 23,24 \}$, $A_2 \in \{ 20,21,22\}$, and
%   $A_3 \in \{ 21,22 \}$. The sample space in this case is
%   $A_1 \times A_2 \times A_3$ which represents all the possible
%   combinations of ages for the three employees:
% \[\begin{array}{rcl}
% \Omega &=& \{ 
%         (23,20,21), (23,20,22), (23,21,21), (23,21,22), (23,22,21), (23,22,22), \\
% && ~(24,20,21), (24,20,22), (24,21,21), (24,21,22), (24,22,21), (24,22,22) \}
% \end{array}\]
% Subsets of $\Omega$ represent events as usual. Consider the event
% $\Omega$ that \emph{some} employee's age is in the range
% $\{20,21,22\}$: since that event covers the entire sample space its
% probability must be 1. We can however produce a more informative
% answer by reasoning as follows: it is \emph{impossible} for the first
% employee's age to be in the range $\{20,21,22\}$; it is \emph{certain}
% that the second employee's age is in that range; and it is
% \emph{possible} that the third's employee age is in that
% range. Aggregating the results, we see that it is \emph{necessary} for
% one out of three employees to have their age in the required range,
% and it is \emph{possible} for an additional employee to have their age
% in the required range. We summarize this information by reporting that
% the probability of this event is $[\frac{1}{3},\frac{2}{3}]$ where the
% first number reports the \emph{certainty} of the event and the second
% reports the \emph{possibility} of the event. We could also have
% reasoned about the dual event that \emph{no} employee's age is in the
% range $\{20,21,22\}$ to get the probability $[\frac{1}{3},\frac{2}{3}]$

%  dually
% about the evidence \emph{against} the event. The probability in this
% case 


% Now consider the event
% that some employee's age is in the range $\{23,24\}$. Reasoning as
% above, the probability of this event is $[\frac{1}{3},\frac{1}{3}]$ as
% only the first employee qualifies. Clearly if we were to ask the
% probability that some employee's age is in the range
% $\{20,21,22,23,24\}$ the result should be $[1,1]$ as this range covers
% all the possibilities. The way to calculate this result from the two
% previous ones is as follows: the evidence for the combined event to be
% necessary is at least as strong as the evidence that each disjoint
% event that contributes to it is necessary. So the lower bound
% probability for the combined event is at least
% $\frac{1}{3}$. Similarly the upper bound probability is at least
% $1$. But now we can reason using the complements of the events about
% the necessity and possibility of refuting each event. For the event
% that some employee's age is \emph{not} in the range $\{20,21,22\}$ we
% have a probability $[\frac{1}{3},\frac{1}{3}]$ because it is necessary
% that $A_1$ is not in that range. Similary, the event that some employee's age is
% \emph{not} in the range $\{23,24\}$ is
% $[\frac{2}{3},\frac{2}{3}]$. Thus although the positive evidence for
% necessity 





% \qed\end{example}

% \newpage



% \begin{example}[Dempster-Shafer Theory of Evidence] We have five
%   employees whose precise age is not known. All is given is a range for
%   each employee:
% \[\begin{array}{rcl}
% \textrm{Age}(M_{1}) & \in & \{ 23,24 \}=D_{1}\\
% \textrm{Age}(M_{2}) & \in & \{ 20,21,22\}=D_{2}\\
% \textrm{Age}(M_{3}) & \in & \{ 20,21 \}=D_{3}
% \end{array}\]
% What is the probability that an employee's age is in the range
% $\{22,23,24\}$? Looking at the data, it is \emph{possible} for $M_2$'s age
% to be within the range, it is \emph{not possible} for $M_3$'s age to
% be within the range, and it is \emph{certain} or \emph{necessary} that
% $M_1$'s age is in the range. Clearly saying that an event is
% \emph{necessary} is equivalent to saying that its complement is
% \emph{not possible}. Aggregating the results for the three employees,
% we can calculate that is necessary that 1 employees have their ages
% within the range, and it is possible that 2 employees have their ages
% within the range. We express this formally as saying that the
% probability of the event is $\left[\frac{1}{3},\frac{2}{3}\right]$. Now consider
% another event asking whether the ages are some other disjoint range
% $\{20,21\}$. Reasoning in a similar way we calculate that the
% probability for this event is $\left[\frac{1}{3},\frac{2}{3}\right]$. Now let's
% take the two events together and ask about the possibility of the ages to be in the range
% $\{20,21,22,23,24\}$. Clearly that probability must be $[1,1]$ as every
% employee's age is in that range. This problem
% looks puzzle if we want to attack it directly, but will be more clear
% if we think in terms of conditional probability. We will compute the
% conditional probability for the usual real-valued probability first
% and for the interval-valued probability later.
% \begin{itemize}
% \item We randomly draw an employee among the three, so each employee has
% probability $\frac{1}{3}$ to be drawn, i.e.,
% \[
% \pmeas\left(\left\{ i=1\right\} \right)=\pmeas\left(\left\{ i=2\right\} \right)=\pmeas\left(\left\{ i=3\right\} \right)=\frac{1}{3}
% \]
% In order to compute the usual real-valued probability, we need a probability
% distribution within the possible range $D_{1}$, $D_{2}$, and $D_{3}$.
% Let's assume they are equally probable, i.e., 
% \[
% \begin{array}{c@{\qquad\qquad}c}
% \begin{array}{rcl}
% \pmeas\left(\left\{ \textrm{Age}(M_{1})=23\right\} \right) & = & \frac{1}{2}\\
% \pmeas\left(\left\{ \textrm{Age}(M_{1})=24\right\} \right) & = & \frac{1}{2}
% \end{array} & \begin{array}{rcl}
% \pmeas\left(\left\{ \textrm{Age}(M_{2})=20\right\} \right) & = & \frac{1}{3}\\
% \pmeas\left(\left\{ \textrm{Age}(M_{2})=21\right\} \right) & = & \frac{1}{3}\\
% \pmeas\left(\left\{ \textrm{Age}(M_{2})=22\right\} \right) & = & \frac{1}{3}
% \end{array}\end{array}\begin{array}{rcl}
% \pmeas\left(\left\{ \textrm{Age}(M_{3})=20\right\} \right) & = & \frac{1}{2}\\
% \pmeas\left(\left\{ \textrm{Age}(M_{3})=21\right\} \right) & = & \frac{1}{2}
% \end{array}
% \]
% Then, the probability that an employee's age is in the range $\{22,23,24\}$
% can be computed as follow:
% \begin{eqnarray*}
%  &  & \pmeas\left(\left\{ i\middle|\textrm{Age}(M_{i})\in\{22,23,24\}\right\} \right)\\
%  & = & \pmeas\left(\left\{ i=1\right\} \right)\pmeas\left(\left\{ \textrm{Age}(M_{1})\in\{22,23,24\}\right\} \right)\\
%  &  & +\pmeas\left(\left\{ i=2\right\} \right)\pmeas\left(\left\{ \textrm{Age}(M_{2})\in\{22,23,24\}\right\} \right)\\
%  &  & +\pmeas\left(\left\{ i=3\right\} \right)\pmeas\left(\left\{ \textrm{Age}(M_{3})\in\{22,23,24\}\right\} \right)\\
%  & = & \frac{1}{3}\cdot1+\frac{1}{3}\cdot\frac{1}{3}+\frac{1}{3}\cdot0=\frac{4}{9}
% \end{eqnarray*}
% \item Assume we don't know the probability distributions within the possible
% range $D_{1}$, $D_{2}$, and $D_{3}$. All we know is whether they
% are \emph{possible} or \emph{necessary}. Then we replace the above
% computation from exact value to interval and consider
% \begin{eqnarray*}
%  &  & \pmeas\left(\left\{ i\middle|\textrm{Age}(M_{i})\in\{22,23,24\}\right\} \right)\\
%  & = & \pmeas\left(\left\{ i=1\right\} \right)\pmeas\left(\left\{ \textrm{Age}(M_{1})\in\{22,23,24\}\right\} \right)\\
%  &  & +\pmeas\left(\left\{ i=2\right\} \right)\pmeas\left(\left\{ \textrm{Age}(M_{2})\in\{22,23,24\}\right\} \right)\\
%  &  & +\pmeas\left(\left\{ i=3\right\} \right)\pmeas\left(\left\{ \textrm{Age}(M_{3})\in\{22,23,24\}\right\} \right)\\
%  & = & \frac{1}{3}\cdot[1,1]+\frac{1}{3}\cdot[0,1]+\frac{1}{3}\cdot[0,0]=\left[\frac{1}{3},\frac{2}{3}\right]
% \end{eqnarray*}
% \yutsung{Check the conditional probability rule in the Dempster-Shafer
% Theory!}
% \end{itemize}
% \qed\end{example}

% In interval-valued probability measures, if the probability of an
% event $E$ is $[a,b]$, we think of the left-endpoint~$a$ as
% representing the strength of the evidence that supports $E$, and the
% right-endpoint~$b$ as the strength of the evidence that contradicts
% $E$. 


% Thus if we have an event $E$ with probability $[a,b]$ where
% $a=0.1$ and $b=0.7$, we have that:
% \begin{itemize}
% \item the strength of evidence supporting $E$ is 0.1; since either $E$
%   or its complement must happen, we conclude that there is 0.9
%   evidence supporting the complement of $E$; 
% \item the strength of evidence contradicting $E$ is 0.7; again since
%   either $E$ or its complement must happen, we conclude that there is
%   0.3 evidence contradicting the complement of $E$.
% \end{itemize}

% \yutsung{Do we use the law of excluded middle here? You remind me Agda : ) Did
% Homotopy Type Theory people said anything about the probability?}

% Turning things around, the strength of evidence that contradicts
% $E$ is evidence supporting the complement of $E$. The complement of $E$ must
% therefore have probability $\left[1-b,1-a\right]$ which we abbreviate $1-\left[a,b\right]$:

% \begin{eqnarray*}
% \pmeas\left(\emptyset\right) & = & \imposs\\
% \pmeas\left(\Omega\right) & = & \necess\\
% \pmeas\left(\Omega\backslash E\right) & = & 1-\pmeas\left(E\right)
% \end{eqnarray*}
% Next, if we define $\sum_{i}\left[a_{i},b_{i}\right]=\left[\sum_{i}a_{i},\sum_{i}b_{i}\right]$,
% then 
% \begin{itemize}
% \item for a collection $E_{i}$ of pairwise disjoint events, we have $\pmeas\left(\bigcup_{i}E_{i}\right)\subseteq\sum_{i}\pmeas\left(E_{i}\right)$. 
% \end{itemize}
% Notice that the equality may not hold in general. This statement says
% that the evidences of $\bigcup_{i}E_{i}$ is at least as strong as
% putting all the evidences of $E_{i}$ together, but some evidence
% may only be acquired for $\bigcup_{i}E_{i}$ as the whole. Therefore,
% $\pmeas\left(\bigcup_{i}E_{i}\right)$ is a subset of $\sum_{i}\pmeas\left(E_{i}\right)$,
% but may not equal. In our example, 
% \begin{eqnarray*}
%  &  & \pmeas(\{HH,TH\})=\necess=[1,1]\\
%  & \subseteq & \left[\frac{1}{2},\frac{3}{2}\right]=\left[0,\frac{1}{2}\right]+\left[\frac{1}{2},1\right]=\unlikely+\likely=\pmeas(\{HH\})+\pmeas(\{TH\})\textrm{ .}
% \end{eqnarray*}
% However, $\pmeas(\{HH,TH\})$ is a proper subset of $\pmeas(\{HH\})+\pmeas(\{TH\})$
% because if we check the complement of $\{HH,TH\}$, we have 
% \begin{eqnarray*}
%  &  & \pmeas(\{HH,TH\})=1-\pmeas(\{HT,TT\})=1-\imposs=\necess\textrm{ ,}
% \end{eqnarray*}
% and $\pmeas(\{HT,TT\})=\imposs$ cannot be used to reasoning the probability
% of $\{HH\}$ and $\{TH\}$ individually. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Quantum Probability Spaces}

The mathematical framework above assumes that one has complete
knowledge of the events and their relationships. However, in many
practical situations, the structure of the event space is only
partially known and the precise dependence of two events on each other
cannot, a priori, be determined with certainty. In the quantum case,
this partial knowledge is compounded by the fact that there exist
non-commuting events which cannot happen simultaneously. To
accommodate these more complex situations, we abandon the sample
space~$\Omega$ and reason directly about events. A quantum probability
space therefore consists of just two components: a set of events
$\qevents$ and a probability measure
$\mu : \qevents \rightarrow [0,1]$. We give an example before giving
the formal definition.

\begin{example}[One-qubit quantum probability space] 
  Consider a one-qubit Hilbert space with states
  $\alpha \ket{0} + \beta \ket{1}$ such that
  $|\alpha|^2 + |\beta|^2 = 1$. The set of events associated with this
  Hilbert space consists of all projection operators. Each event is
  interpreted as a possible post-measurement state of a quantum system
  in current state $\ket{\phi}$. For example, the event $\proj{0}$
  indicates that the post-measurement state will be~$\ket{0}$; the
  event $\proj{1}$ indicates that the post-measurement state will
  be~$\ket{1}$; the event $\proj{\ps}$ where
  $\ket{\ps} = \frac{1}{\sqrt{2}}(\ket{0}+\ket{1})$ indicates that the
  post-measurement state will be $\ket{\ps}$; the event
  $\mathbb{1} = \proj{0}+\proj{1}$ indicates that the post-measurement
  state will be a linear combination of $\ket{0}$ and $\ket{1}$; and
  the empty event $\mathbb{0}$ states that the post-measurement state
  will be the empty state. As in the classical case, a probability
  measure is a function that maps events to $[0,1]$: here is a partial
  specification of a possible probability measure:
\[\begin{array}{rcl}
\mu\left(\mathbb{0}\right) = 0, \quad
\mu\left(\mathbb{1}\right) =  1, \quad
\mu\left(\proj{0}\right) = 1, \quad
\mu\left(\proj{1}\right) = 0, \quad
\mu\left(\proj{\ps}\right) = 1/2, \quad \ldots
\end{array}\]
Note that, similarly to the classical case, the probability of
$\mathbb{1}$ is 1 and the probability of collections of orthogonal
events (e.g., $\proj{0}+\proj{1}$) is the sum of the individual
probabilities. In contrast, a collection of non-orthogonal events
(e.g., $\proj{0}$ and $\proj{\ps}$) is not itself an event.
In the classical example, we argued that each probability measure is
uniquely determined by two actual coins. A similar (but much more
subtle) argument is valid also in the quantum case. By postulates of
quantum mechanics and Gleason's theorem, it turns out that for large
enough quantum systems, each probability measure is uniquely
determined by an actual quantum state.
\qed\end{example}

To properly explain the previous example and generalize to arbitrary
quantum systems, we formally discuss projection operators and then
define a quantum probability space. 

\begin{definition}[Projection Operators; Orthogonality;
Commutativity~\cite{10.2307/2308516,Redhead1987-REDINA,peres1995quantum,Swart2013}]
Given a Hilbert space $\Hilb$, a projection operator $P$ is a linear transformation from $\Hilb$
to itself such that $P^2 = P = P^{\dagger}$. Projection operators have the
following properties:\footnote {``Projection'' is sometimes called
``orthogonal projection" or ``self-adjoint projection" to
emphasize $P^{\dagger} = P$~\cite{Maassen2010}.}
\begin{itemize}
\item Projection operators $P_1$ and $P_2$ are orthogonal if $P_1P_2 =
  P_2P_1 = \mathbb{0}$;
\item Projection operators $P_1$ and $P_2$ commute if $P_1P_2 =
  P_2P_1$;
\item If the projections $P_1$ and $P_2$ are orthogonal then $P_1 +
  P_2$ is also a projection;
\item If the projections $P_1$ and $P_2$ commute then $P_1P_2$ is also
  a projection.
\end{itemize}
\end{definition}

\amr{Here it would be good to refer to the notion of ``quantum test''
  and define events as sums of quantum tests. This will automatically
  include everything except the products of commutative projections
  which we will have to explain that they can be expressed as sums of
  orthogonal projections.}

% \amr{ 
%   Projections include $\mathbb{0}, \proj{\psi}$ and $P_1 + P_2$ and
%   $P_1 . P_2$ if $P_1$ and $P_2$ commute.
%   When we write an event as $\sum E_i$ we are implicitly assuming they
%   are commutative.
%   Orthogonal implies commutative but not vice-versa
% }
% \yutsung{
%   I checked \\
%   \url{http://math.stackexchange.com/questions/1146893/on-the-sum-of-projection-operators}.\\
%   Actually, they said $P_1 P_2 = P_2 P_1 = 0$, and the last $= 0$ is orthogonal.
% }

\begin{definition}[Quantum Probability Space~\cite{BirkhoffVonNeumann1936,gleason1957,Redhead1987-REDINA,DBLP:journals/corr/abs-0910-2393,Maassen2010}]
  Given a Hilbert space $\Hilb$, a \emph{quantum probability space}
  consists of a set of events $\events$ and a probability measure
  $\mu : \events \rightarrow [0,1]$ such that:\footnote {It is
    possible to define a more general space of events consisting of
    all operators~$\mathcal{A}$ on $\Hilb$ and consider
    $\mu:\mathcal{A}\rightarrow\C$~\cite{Maassen2010,Swart2013}.  When
    an operator $A\in\mathcal{A}$ is Hermitian, $\mu\left(A\right)$ is
    the expectation value of $A$. We does not take this approach
    because we want to focus only on probability. }
\begin{itemize}
\item The set of events consists of all projections. This set includes
  the empty projection, projection operators $\proj{\psi}$ for each
  state $\ket{\psi}$, sums of \emph{orthogonal} projections, and
  products of \emph{commuting} projections; 
\item $\mu(\mathbb{1})=1$, and 
\item for mutually orthogonal projections $E_i$, we have
  $\mu\left(\sum_{i}E_{i}\right)=\sum_{i}\mu\left(E_{i}\right)$.
\end{itemize}
\qed\end{definition}

%%%%%
\subsection{Quantum Probability Measures}

For a given set of events $\events$, there are many possible
probability measures $\mu : \events \rightarrow [0,1]$. The Born rule,
a postulate of quantum mechanics, states that each quantum state
$\ket{\phi}$ induces a probability measure $\mu_\phi$ as follows:
\[ 
\mu_\phi(E) = \ip{\phi}{E\phi}
\]
Conversely, Gleason's theorem states that given a probability measure
$\mu$, there exist a quantum state $\ket{\phi}$ that induces such a
measure using the Born rule. The theorem is only valid in Hilbert
spaces with dimension $d \geq 3$. It is instructive to study
counterexamples in $d=2$, i.e., the case of a one-qubit
system. Consider five states $\ket{\psi_0}$ to $\ket{\psi_4}$ that
form five orthogonal bases $\{ \ket{\psi_0}, \ket{\psi_1} \}$,
$\{ \ket{\psi_1}, \ket{\psi_2} \}$,
$\{ \ket{\psi_2}, \ket{\psi_3} \}$,
$\{ \ket{\psi_3}, \ket{\psi_4} \}$, and
$\{ \ket{\psi_4}, \ket{\psi_0} \}$ and consider the probability
measure defined as follows. For all $i \in \{0,1,2,3,4\}$, we have
$\mu_X(\proj{\psi_i}) = 1/2$. For each orthogonal basis, the
probability is 1 as desired and yet it is impossible to find a single
quantum state that realizes such a probability
measure (see \url{http://tph.tuwien.ac.at/~svozil/publ/2006-gleason.pdf})

\newpage
\amr{the rest needs cleaning up and perhaps does not even belong in
  this section}

Although it seems that we need an infinite long table to specify the
quantum probability measure~$\mu$, our $\mu$ is actually
given by a simple formula~$\melement{0}{E}$. In general, Born discovered
each quantum state $\ket{\psi}\in\Hilb\backslash\left\{ 0\right\} $
induces a probability measure $\tilde{\mu}_{\psi}:\qevents\rightarrow[0,1]$
on the space of events defined for any event $E\in\qevents$ as follows~\cite{Born1984,Mermin2007}:
\begin{equation}
\tilde{\mu}_{\psi}(E)=\frac{\melement{\psi}{E}}{\ip{\psi}{\psi}}\label{eq:Born}
\end{equation}
The Born rule satisfies the following properties:
\begin{itemize}
\item It can be extend to mixed states. Given a mixed state represented
by a density matrix $\rho=\sum_{j=1}^{N}q_{j}\frac{\proj{\psi_{j}}}{\ip{\psi_{j}}{\psi_{j}}}$,
where $\sum_{j=1}^{N}q_{j}=1$, i.e., $\Tr\left(\rho\right)=1$, then
the Born rule can be extended to $\rho$ by 
\begin{eqnarray}
\tilde{\mu}_{\rho}\left(E\right) & = & \Tr\left(\rho E\right)=\sum_{j=1}^{N}q_{j}\tilde{\mu}_{\Psi_{j}}\left(E\right)\textrm{ .}\label{BornRule.mixed}
\end{eqnarray}
Notice that $\left(\left\{ 1,\ldots,N\right\} ,2^{\left\{ 1,\ldots,N\right\} },\pmeas\left(J\right)=\sum_{j\in J}q_{j}\right)$
is a classical probability space. Therefore, when we discretize the
Hilbert space later, we may need to discretize this probability space
as well.
\item $\tilde{\mu}_{\rho}$ is a probability measure for all mixed state~$\rho$.
\item $\ip{\psi}{\phi}=0\Leftrightarrow\tilde{\mu}_{\psi}\left(\proj{\phi}\right)=0$.
\item $\tilde{\mu}_{\psi}\left(E\right)=\tilde{\mu}_{\mathbf{U}\ket{\psi}}\left(\mathbf{U}E\mathbf{U}^{\dagger}\right)\textrm{ ,}$where
$\mathbf{U}$ is any unitary map, i.e., $\mathbf{U}^{\dagger}\mathbf{U}=\mathbb{1}$. 
\end{itemize}

Naturally, we may ask: is every probability measure induced from a
state by the Born rule? The answer is yes by Gleason's theorem when
the dimension~$\ge3$~\cite{gleason1957,peres1995quantum,Redhead1987-REDINA}.
Furthermore, a simple corollary of Gleason's theorem can show the
Born rule is the unique function satisfying conditions 1. to 3.
\begin{cor}
The Born rule is the unique function satisfying conditions 1. to 3.
\end{cor}
\begin{proof}
Assume there is another function $\tilde{\mu}'$ such that $\tilde{\mu}'_{\rho}$
is a quantum probability measure for all mixed state~$\rho$. We
are going to prove $\tilde{\mu}'=\tilde{\mu}$.

Fix a pure normalized state $\phi$, $\tilde{\mu}'_{\phi}$ is a quantum
probability measure by condition 2. By Gleason's theorem, there is
a mixed state ~$\rho'$, such that $\tilde{\mu}'_{\phi}\left(E\right)=\Tr\left(\rho'E\right)=\sum_{j=1}^{N}q_{j}\tilde{\mu}_{\psi_{j}}\left(E\right)$
for all event $E$. 

Consider the event $E'=\mathbb{1}-\proj{\phi}$, we have 
\begin{eqnarray*}
0 & \overset{\textrm{Condition 3}}{=} & \tilde{\mu}_{\phi}\left(E'\right)\\
 & = & \sum_{j=1}^{N}q_{j}\tilde{\mu}_{\psi_{j}}\left(E'\right)
\end{eqnarray*}
Because $q_{j}>0$, we have $\tilde{\mu}_{\psi_{j}}\left(E\right)=0$,
i.e., $\psi_{j}$ is orthogonal to a co-dimension-$1$ subspace $E'$.
However, the only subspace orthogonal to $E'$ is span by $\ket{\phi}$.
Hence, $\tilde{\mu}'_{\phi}=\tilde{\mu}_{\phi}$.
\end{proof}

%%%%%
\subsection{Plan}

In the remainder of the paper, we consider variations of quantum
probability spaces motivated by computation of numerical quantities in
a world with limited resources:
\begin{itemize}
\item Instead of the Hilbert space $\Hilb$ (constructed over the
  uncountable and uncomputable complex numbers $\mathbb{C}$), we will
  consider variants constructed over finite
  fields~\cite{HansonOrtizSabryEtAl2015,DQT2014,geometry2013}.
\item Instead of real-valued probability measures producing results in
  the uncountable and uncomputable interval $[0,1]$, we will consider
  finite set-valued probability measures~\cite{Artstein1972,PuriRalescu1983}.
\end{itemize}
We will then ask if it is possible to construct variants of quantum
probability spaces under these conditions. The main question is
related to the definition of probability measures: is it possible to
still define a probability measure as a function that depends on a
single state? Specifically,
\begin{itemize}
\item given a state $\ket{\psi}$, is there a probability measure
  mapping events to probabilities that only depends on $\ket{\psi}$?
  In the conventional quantum probability space, the answer is yes by
  the Born rule~\cite{Born1984,Mermin2007} and the map is given by:
  $E \mapsto \ip{\psi}{E\psi}$.
\item given a probability measure $\mu$
  mapping each event $E$
  to a probability, is there a \emph{unique} state $\psi$
  such that $\mu(E)
  =
  \ip{\psi}{E\psi}$? In the conventional case, the answer is yes by
  Gleason's
  theorem~\cite{gleason1957,peres1995quantum,Redhead1987-REDINA}.
\end{itemize}


% If there may be more than one probability measure, we will discuss
% whether we will keep using the Born rule (\ref{eq:Born}) or there
% is another formula $\tilde{\mu}$ such that $\tilde{\mu}_{\psi}$
% is a probability measure for all $\ket{\psi}\in\Hilb\backslash\left\{ 0\right\} $.

% Then, Gleason's theorem states given a probability measure $\mu$
% there is a mixed state $\ket{\psi}$ such that $\mu=\mu_{\psi}$,
% i.e., the Born rule is surjective\footnote{If we extend the domain of $\mu_{\psi}$ including the mixed states.}.
% For any other formula~$\tilde{\mu}$, we can ask whether $\tilde{\mu}$
% is surjective as well.

% Obviously, we don't want arbitrarily assign a state~$\ket{\psi}\in\Hilb\backslash\left\{ 0\right\} $
% with a probability measure $\tilde{\mu}_{\psi}$, for example, assigning
% every state to the same probability measure. We want $\tilde{\mu}_{\psi}$
% satisfying the following properties with some physical meaning:
% \begin{itemize}
% \item $\ip{\psi}{\phi}=0\Leftrightarrow\tilde{\mu}_{\psi}\left(\proj{\phi}\right)=\tilde{0}$,
% where $\tilde{0}$ is $0$ for $\left[0,1\right]$ and $\tilde{0}$
% is impossible for $\mathscr{L}_{2}=\left\{ \imposs,\poss\right\} $. 
% \item $\tilde{\mu}_{\psi}\left(\proj{\phi}\right)=\tilde{\mu}_{\mathbf{U}\ket{\psi}}\left(\mathbf{U}\proj{\phi}\mathbf{U}^{\dagger}\right)\textrm{ ,}$where
% $\ket{\psi},\ket{\phi}$ are states and $\mathbf{U}$ is any unitary
% map, i.e., $\mathbf{U}^{\dagger}\mathbf{U}=\mathbb{1}$. 
% \end{itemize}
% And the results can be summarized in the following table:\\
% \center{ %
% \begin{tabular}{>{\raggedright}m{0.2\columnwidth}>{\raggedright}m{0.2\columnwidth}>{\raggedright}m{0.2\columnwidth}>{\raggedright}m{0.2\columnwidth}}
% \hline 
% State space $\Hilb$  & Probability values  & Is there a $\tilde{\mu}$ satisfying the given conditions?  & Is the $\tilde{\mu}$ surjective?\tabularnewline
% \hline 
% \hline 
% $\C^{d}$ for $d\ge3$  & $\left[0,1\right]$  & Yes  & Yes\tabularnewline
% \hline 
% $\C^{d}$  & $\mathscr{L}_{2}$  & Yes  & No\tabularnewline
% \hline 
% $\ffzd{p^{2}}$for $d\ge3$ except $d=p=3$  & $\left[0,1\right]$  & No  & \tabularnewline
% \hline 
% $\ffzd{p^{2}}$  & $\mathscr{L}_{2}$  & Yes & No\tabularnewline
% \hline 
% \end{tabular}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{All Continuous or All Discrete}

Before we turn to the main part of the paper, we quickly dismiss the
possibility of having one but not the other of the discrete
variations. Specifically, it is impossible to maintain the Hilbert
space and have a finite set-valued probability measure and it is also
impossible to have a vector space constructed over a finite field with
a real-valued probability measure. 

%%%
\subsection{Hilbert Space with Finite Set-Valued Probability Measure}

However, there is a $\mathscr{L}_{2}$-valued probability measure
\[
\hat{\mu}_{1}\left(E\right)=\begin{cases}
\imposs & \textrm{, if }E=\proj{+};\\
\bar{\mu}(E) & \textrm{, otherwise.}
\end{cases}
\]
such that $\hat{\mu}_{1}\ne\bar{\mu}_{\psi}$ for all mixed state
$\ket{\psi}$.

%%%
\subsection{Discrete Vector Space with Real-Valued Probability Measure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrt}
\bibliography{discreteGBKS}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Probability space (restricted to finite sample spaces)

* Sample space Ω (arbitrary finite non-empty set)
* Set of events F: pick 2^Ω
* Probability measure (real-valued): P : F → [0,1] such that:
  * For any collection of pairwise disjoint Aᵢ ∈ F, we have P ( ∪ᵢ Aᵢ) = ∑ᵢ P(Aᵢ) 
  * P(Ω) = 1


Set-value probability measures. Change the last bullet to:
* P : F → 2^(ℝⁿ) such that: ...

%%%%%
\subsection{Sample Space $\Omega$} 

In this paper, we will only consider \textbf{finite} sample spaces. We
therefore define a sample space $\Omega$ as a non-empty finite set.

\begin{example}[A Classical Sample Space.]
Consider an experiment that tosses three coins. A possible outcome of
the experiment is $HHT$ which means that the first and second coins
landed with ``heads'' as the face-up side and that the third coin
landed with ``tails'' as the face-up side. There are clearly a total
of eight possible outcomes, and this collection constitutes the sample
space:
\[
\Omega_C = \{ HHH, HHT, HTH, HTT, THH, THT, TTH, TTT \}
\]
\end{example}

\begin{example}[A Quantum Sample Space.]
Consider a quantum system composed of three electrons. By the
postulates of quantum mechanics, an experiment designed to measure
whether the spin of each electron along the $x$ axis is left ($L$) or
right ($R$) can only result in one of eight outcomes:
\[
\Omega_H = \{ LLL, LLR, LRL, LRR, RLL, RLR, RRL, RRR \}
\]
\end{example}

%%%%%
\subsection{Events $\mathcal{F}$} 

The space of events $\mathcal{F}$ associated with a sample space
$\Omega$ is $2^\Omega$, the powerset of $\Omega$. In other words,
every subset of $\Omega$ is a possible event.

\begin{example}[Some classical events.] 
The following are events associated with $\Omega_C$:
\begin{itemize}
\item $E_0$, exactly zero coins are $H$, is the set $\{ TTT \}$.
\item $E_1$, exactly one coin is $H$, is the set $\{ HTT, THT, TTH \}$. 
\item $E_2$, exactly two coins are $H$, is the set $\{ HHT, HTH, THH \}$.
\item $E_3$, exactly three coins are $H$, is the set $\{ HHH \}$. 
\item $E_{>0}$, at least one coin is $H$, is the set $\{ HHH, HHT, HTH, HTT, THH, THT, TTH \}$. 
\end{itemize}
As the examples illustrate, events are \emph{indirect} questions built from elementary elements of the sample space using logical connectives. Also
note that some events may be disjoint and that some events may be
expressed as combinations of other events. For example, we have
$E_{>0} = E_1 \cup E_2 \cup E_3$ and each of these four events is
disjoint from event $E_0$.
\end{example}

\begin{example}[Some quantum events.] 
The following are events associated with $\Omega_H$:
\begin{itemize}
\item $F_0$, exactly zero electrons are spinning $L$, is the set $\{ RRR \}$.
\item $F_1$, exactly one electron is spinning $L$, is the set $\{ LRR, RLR, RRL \}$. 
\item $F_2$, exactly two electrons are spinning $L$, is the set $\{ LLR, LRL, RLL \}$.
\item $F_3$, exactly three electrons are spinning $L$, is the set $\{ LLL \}$. 
\item $F_{>0}$, at least one electron is spinning $L$, is the set $\{ LLL, LLR, LRL, LRR, RLL, RLR, RRL \}$. 
\end{itemize}
As the examples illustrate, quantum events are, at first glance,
similar to classical events. There are however some subtle
differences that we point out in the next section.
\end{example}

%%%%%
\subsection{Measures $\mathbb{P}$} 

The last ingredient of a probability space is a probability measure
$\mathbb{P} : \mathcal{F} \rightarrow [0,1]$ that assigns to each
event a real number in the closed interval $[0,1]$ subject to the
following conditions:
\begin{itemize}
\item $\mathbb{P}(\Omega) = 1$, and 
\item For any collection of pairwise disjoint events $A_i$, we have 
$\mathbb{P}(\bigcup_i A_i) = \Sigma_i ~\mathbb{P}(A_i)$.
\end{itemize}

\begin{example}[Classical probability measure]
There are $2^8$ events associated with $\Omega_C$. A possible probability measure for
these events is:
\[\begin{array}{c}
\mathbb{P}(E) = \left\{ \begin{array}{ll} 
  1 & \mbox{if}~E = \Omega \\
  0 & \mbox{otherwise} 
  \end{array}\right.
\end{array}\]
\yutsung{ Actually, the above $\mathbb{P}$ is not a probability
measure because 
\[
\mathbb{P}\left(\Omega\right)=1\ne0+0=\mathbb{P}\left(E_{0}\right)+\mathbb{P}\left(E_{>0}\right)
\]
} \\
A more interesting measure is defined recursively as follows:
\renewcommand\arraystretch{1.4}
\[\begin{array}{rcl}
\mathbb{P}(\emptyset) &=& 0 \\
\mathbb{P}(\{ HHH \} \cup E) &=& \frac{1}{5} + \mathbb{P}(E) \\
\mathbb{P}(\{ HHT \} \cup E) &=& \mathbb{P}(E) \\
\mathbb{P}(\{ HTH \} \cup E) &=& \frac{3}{10} + \mathbb{P}(E) \\
\mathbb{P}(\{ HTT \} \cup E) &=& \mathbb{P}(E) \\
\mathbb{P}(\{ THH \} \cup E) &=& \frac{1}{5} + \mathbb{P}(E) \\
\mathbb{P}(\{ THT \} \cup E) &=& \mathbb{P}(E) \\
\mathbb{P}(\{ TTH \} \cup E) &=& \frac{3}{10} + \mathbb{P}(E) \\
\mathbb{P}(\{ TTT \} \cup E) &=& \mathbb{P}(E) 
\end{array}\]
\yutsung{Because $\mathbb{P}(\bigcup_{i}A_{i})=\Sigma_{i}~\mathbb{P}(A_{i})$
requires disjoint events, the above formula should write like:
\[
\begin{array}{rcl}
\mathbb{P}(\emptyset) & = & 0\\
\mathbb{P}(\{HHH\}\cup E) & = & \frac{1}{5}+\mathbb{P}(E)\textrm{, if }HHH\notin E\\
\mathbb{P}(\{HHT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }HHT\notin E\\
\mathbb{P}(\{HTH\}\cup E) & = & \frac{3}{10}+\mathbb{P}(E)\textrm{, if }HTH\notin E\\
\mathbb{P}(\{HTT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }HTT\notin E\\
\mathbb{P}(\{THH\}\cup E) & = & \frac{1}{5}+\mathbb{P}(E)\textrm{, if }THH\notin E\\
\mathbb{P}(\{THT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }THT\notin E\\
\mathbb{P}(\{TTH\}\cup E) & = & \frac{3}{10}+\mathbb{P}(E)\textrm{, if }TTH\notin E\\
\mathbb{P}(\{TTT\}\cup E) & = & \mathbb{P}(E)\textrm{, if }TTT\notin E
\end{array}
\]
Or add a sentence ``where the element in the singleton set is not
belong to $E$ for each equation.'' Or write like: 
\[
\begin{array}{rcl}
\mathbb{P}(\{HHH\}) & = & \frac{1}{5}\\
\mathbb{P}(\{HHT\}) & = & 0\\
\mathbb{P}(\{HTH\}) & = & \frac{3}{10}\\
\mathbb{P}(\{HTT\}) & = & 0\\
\mathbb{P}(\{THH\}) & = & \frac{1}{5}\\
\mathbb{P}(\{THT\}) & = & 0\\
\mathbb{P}(\{TTH\}) & = & \frac{3}{10}\\
\mathbb{P}(\{TTT\}) & = & 0\\
\mathbb{P}(E) & = & \sum_{\omega\in E}\mathbb{P}(\left\{ \omega\right\} )
\end{array}
\]
} \\
Because this is a \emph{classical} situation, the probability
assignments can be understood \emph{locally} and
\emph{non-contextually}. In other words, we can reason about each coin
separately and perform experiments on it ignoring the rest of the
context. If we were to perform such experiments we may find that for
the first coin, the probability of either outcome
$H$ or $T$ is $\frac{1}{2}$; for coin two, the probabilities are
skewed a little with the probability of outcome $H$ being
$\frac{2}{5}$ and the probability of outcome $T$ being $\frac{3}{5}$; and that
coin 3 is a fake double-headed coin where the probability of
outcome $H$ is 1 and the probability of outcome $T$ is 0. The reader
may check that these local observations are consistent with the
probability measure above. 
\end{example}

\begin{example}{[}Quantum probability measure{]} Like in the classical
case, there are $2^{8}$ events. But as Mermin explains in a simple
example~\cite{MerminPRL1990}, here is a possible probability measure:
\[
\begin{array}{rcl}
\mathbb{P}_{xxx}(\{LLL\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{LLR\}) & = & 0\\
\mathbb{P}_{xxx}(\{LRL\}) & = & 0\\
\mathbb{P}_{xxx}(\{LRR\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{RLL\}) & = & 0\\
\mathbb{P}_{xxx}(\{RLR\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{RRL\}) & = & \frac{1}{4}\\
\mathbb{P}_{xxx}(\{RRR\}) & = & 0\\
\mathbb{P}_{xxx}(E) & = & \sum_{\omega\in E}\mathbb{P}_{xxx}(\left\{ \omega\right\} )
\end{array}
\]
In contrast with the previous classical example, the event of different
electrons are not independent. More precisely, consider the event
for each electron separately:
\begin{eqnarray*}
F_{1,L} & = & \left\{ LLL,LLR,LRL,LRR\right\} \\
F_{2,L} & = & \left\{ LLL,LLR,RLL,RLR\right\} \\
F_{3,L} & = & \left\{ LLL,LRL,RLL,RRL\right\} 
\end{eqnarray*}
They are not independent means 
\[
\mathbb{P}_{xxx}(F_{1,L}\cap F_{2,L}\cap F_{3,L})=\mathbb{P}_{xxx}(\{LLL\})=\frac{1}{4}\ne\frac{1}{8}=\mathbb{P}_{xxx}(F_{1,L})\mathbb{P}_{xxx}(F_{2,L})\mathbb{P}_{xxx}(F_{3,L})\textrm{ .}
\]


Classical events may also not be independent even if they seems unrelated.
For example, events defined by the temperature is usually not independent
to ones defined by how much Coca-Cola is sold. Another example can
be formulated by tossing three coins as we discussed previously. However,
this time the coins are tossed behind a veil where someone tosses
the coins for you. Because we cannot see how she tosses the coins,
she might actually roll a four-sided tetrahedral die with $\{HHH,HTT,THT,TTH\}$
in its four faces. If $HTT$ is on the downward face, she places $H$,
$T$, and $T$ as the face-up sides of of the three coins by hand,
respectively. Then, she uncovers the veil, and claims she has tossed
the coins. If the coins are tossed in this way, the result of coin-tossing
is correlated, and we will never see $TTT$ no matter how many times
we toss these coins. 

Because we do not know how the spin of an electron is decided, Einstein,
Podolsky, and Rosen (EPR)~\cite{EPR1935} suggested the nature might
give us the probability measure~$\mathbb{P}_{xxx}$ because she rolled
a tetrahedral die or performed other classical and deterministic process
behind the veil. This claim may be convincing if $\mathbb{P}_{xxx}$
is the only probability measure we have, but will lead to a contradiction
if we consider other probability measures as well. Notice that after
the coins are placed by hand and before uncovering the veil, which
side up has already been decided although we do not know. This would
be also true for the quantum probability measure. Because the three
electrons can be spatially separated, and each electron can be measured
along the $x$ axis separately, if the nature rolled a tetrahedral
die, this die should be rolled before the electrons are separated
and measured, and she should know the result of measurement before
we measure the electrons. Let the result of the $j$-th electron measured
along the $x$ axis be $w\left(\sigma_{x}^{j}\right)$. Because 
\[
\mathbb{P}_{xxx}(\{LLR\})=\mathbb{P}_{xxx}(\{LRL\})=\mathbb{P}_{xxx}(\{RLL\})=\mathbb{P}_{xxx}(\{RRR\})=0\textrm{ ,}
\]
we have $w\left(\sigma_{x}^{1}\right)w\left(\sigma_{x}^{2}\right)w\left(\sigma_{x}^{3}\right)\in\{LLL,LRR,RLR,RRL\}$,
i.e., the number of $L$ in $w\left(\sigma_{x}^{1}\right)$, $w\left(\sigma_{x}^{2}\right)$,
and $w\left(\sigma_{x}^{3}\right)$ should be odd. 

The three electrons cannot be measured the spin only along the $x$
axis, but also along the $y$ axis with the result down ($D$) or
up ($U$). We only consider to measure even number of electrons along
the $y$ axis, and the probability measures could be defined by 
\begin{eqnarray*}
\begin{array}{rcl}
\mathbb{P}_{xyy}(\{LDD\}) & = & 0\\
\mathbb{P}_{xyy}(\{LDU\}) & = & \frac{1}{4}\\
\mathbb{P}_{xyy}(\{LUD\}) & = & \frac{1}{4}\\
\mathbb{P}_{xyy}(\{LUU\}) & = & 0\\
\mathbb{P}_{xyy}(\{RDD\}) & = & \frac{1}{4}\\
\mathbb{P}_{xyy}(\{RDU\}) & = & 0\\
\mathbb{P}_{xyy}(\{RUD\}) & = & 0\\
\mathbb{P}_{xyy}(\{RUU\}) & = & \frac{1}{4}
\end{array} & \begin{array}{rcl}
\mathbb{P}_{yxy}(\{DLD\}) & = & 0\\
\mathbb{P}_{yxy}(\{DLU\}) & = & \frac{1}{4}\\
\mathbb{P}_{yxy}(\{DRD\}) & = & \frac{1}{4}\\
\mathbb{P}_{yxy}(\{DRU\}) & = & 0\\
\mathbb{P}_{yxy}(\{ULD\}) & = & \frac{1}{4}\\
\mathbb{P}_{yxy}(\{ULU\}) & = & 0\\
\mathbb{P}_{yxy}(\{URD\}) & = & 0\\
\mathbb{P}_{yxy}(\{URU\}) & = & \frac{1}{4}
\end{array} & \begin{array}{rcl}
\mathbb{P}_{yyx}(\{DDL\}) & = & 0\\
\mathbb{P}_{yyx}(\{DDR\}) & = & \frac{1}{4}\\
\mathbb{P}_{yyx}(\{DUL\}) & = & \frac{1}{4}\\
\mathbb{P}_{yyx}(\{DUR\}) & = & 0\\
\mathbb{P}_{yyx}(\{UDL\}) & = & \frac{1}{4}\\
\mathbb{P}_{yyx}(\{UDR\}) & = & 0\\
\mathbb{P}_{yyx}(\{UUL\}) & = & 0\\
\mathbb{P}_{yyx}(\{UUR\}) & = & \frac{1}{4}
\end{array}
\end{eqnarray*}
with $\mathbb{P}_{ijk}(E)=\sum_{\omega\in E}\mathbb{P}_{ijk}(\left\{ \omega\right\} )$.
Similarly, the nature should predetermine $w\left(\sigma_{x}^{j}\right)$
and $w\left(\sigma_{y}^{j}\right)$ for them. Furthermore, because
she do not know along which axis we are going to measure, she should
predetermine the same $w\left(\sigma_{x}^{j}\right)$ and $w\left(\sigma_{y}^{j}\right)$
for all different probability measures. By the same reason as above,
the number of $L$ or $D$ in $\left\{ w\left(\sigma_{x}^{1}\right),w\left(\sigma_{y}^{2}\right),w\left(\sigma_{y}^{3}\right)\right\} $,
$\left\{ w\left(\sigma_{y}^{1}\right),w\left(\sigma_{x}^{2}\right),w\left(\sigma_{y}^{3}\right)\right\} $,
and $\left\{ w\left(\sigma_{y}^{1}\right),w\left(\sigma_{y}^{2}\right),w\left(\sigma_{x}^{3}\right)\right\} $
should be even. If we look these 9 letters carefully, we can find
that every $w\left(\sigma_{x}^{j}\right)$ appears once and every
$w\left(\sigma_{y}^{j}\right)$ appears twice. Hence, the number of
$L$ in $w\left(\sigma_{x}^{1}\right)$, $w\left(\sigma_{x}^{2}\right)$,
and $w\left(\sigma_{x}^{3}\right)$ should be even. This contradict
to the conclusion in our last paragraph. Therefore, EPR's assumption
is wrong, and it is not always true that the nature can predetermine
the measurement result before we perform the measurement. \end{example}


%%%%
\subsection{Finite Precision of Measurements}

In a laboratory setting or a computational setting, there are neither
uncountable entities nor uncomputable entities. We are thus looking at
alternative probability spaces which do not depend on the real numbers
and revisit the mysteries of quantum mechanics in that
setting. In other words, is it possible that at least part of the quantum mysteries related to probability and measurement are due to the reliance on uncomputable probability values? 

Following previous work on probability, we will replace the
closed interval $[0,1]$ by the \emph{finite set} $S = \{
\textbf{possible}, \textbf{impossible} \}$ and adapt the definition of
probability measure as follows.

A set-valued probability measure $\mathbb{P} : \mathcal{F} \rightarrow
S$ assigns to each event either the tag \textbf{possible} or the tag
\textbf{impossible} subject to the following conditions:
\begin{itemize}
\item $\mathbb{P}(\Omega) = \textbf{possible}$, and 
\item For any collection of pairwise disjoint events $A_i$, we have 
$\mathbb{P}(\bigcup_i A_i) = \textbf{possible}$ if any event $A_i$ is
\textbf{possible} and \textbf{impossible} otherwise. 
\end{itemize}

We begin by reviewing the conventional presentation of classical
probability spaces and then give an alternative formulation that is
``quantum-like'' but still classical. We conclude this section with a
definition of quantum probability spaces given as a modest
generalization of the alternative classical definition. 

%%%%%
\subsection{Conventional Classical Probability Spaces}

Textbook probability
theory~\cite{inun.425605319950101,GrahamKnuthPatashnik1994,rohatgi2011introduction}
is defined using the notions of a \emph{sample space} $\Omega$, a
space of \emph{events}~$\events$, and a \emph{probability
  measure}~$\pmeas$. In this paper, we will only consider
\emph{finite} sample spaces: we therefore define a sample space
$\Omega$ as an arbitrary non-empty finite set and the space of events
$\events$ as, $2^\Omega$, the powerset of $\Omega$. A \emph{probability
measure} is a function $\pmeas : \events \rightarrow [0,1]$ such that:
\begin{itemize}
\item $\pmeas(\Omega) = 1$, and 
\item for a collection of pairwise disjoint events $E_i$, we have
  $\pmeas(\bigcup E_i) = \sum \pmeas(E_i)$. 
\end{itemize}

\begin{example}[Two coin experiment] Consider an experiment that
  tosses two coins. We have four possible outcomes that constitute the
  sample space $\Omega = \{ HH, HT, TH, TT \}$. The event that the
  first coin lands heads up is $\{ HH, HT \}$; the event that the two
  coins land on opposite sides is $\{ HT, TH \}$; the event that at
  least one coin lands tails up is $\{ HT, TH, TT\}$. Depending on the
  assumptions regarding the coins, we can define several probability
  measures. Here is a possible one:
\[\begin{array}{c@{\qquad\qquad}c}
\begin{array}{rcl}
\pmeas(\emptyset) &=& 0 \\
\pmeas(\{ HH \}) &=& 1/3 \\
\pmeas(\{ HT \}) &=& 0 \\
\pmeas(\{ TH \}) &=& 2/3 \\
\pmeas(\{ TT \}) &=& 0 \\
\pmeas(\{  HH, HT \}) &=& 1/3 \\
\pmeas(\{  HH, TH \}) &=& 1 \\
\pmeas(\{  HH , TT \}) &=& 1/3 
\end{array} & \begin{array}{rcl}
\pmeas(\{  HT, TH \}) &=& 2/3 \\
\pmeas(\{  HT , TT \}) &=& 0 \\
\pmeas(\{  TH , TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH \}) &=& 1 \\
\pmeas(\{  HH, HT, TT \}) &=& 1/3 \\
\pmeas(\{  HH, TH, TT \}) &=& 1 \\
\pmeas(\{  HT, TH, TT \}) &=& 2/3 \\
\pmeas(\{  HH, HT, TH, TT \}) &=& 1
\end{array}
\end{array}\]
\end{example}

%%%%%
\subsection{Alternative Definition of Classical Probability Spaces}

In the conventional presentation, we have viewed the space of events
$2^\Omega$ are the powerset of $\Omega$. We can equivalently view
$2^\Omega$ as the space of functions from $\Omega$ to the set
$2 = \{0,1\}$. For example, the event $\{HT,TH\}$ is the function $e$
such that:
\[
e (HH) = 0, \quad e (HT) = 1, \quad e (TH) = 1, \quad e (TT) = 0 
\]
We will in fact do a sweeping generalization and view events as
functions from $\Omega$ to $\mathbb{C}$, the set of complex
numbers. This accommodates the previous events such as $e$ and allows
many more events such as event $e'$ below:
\[
e' (HH) = \sqrt{2} + i \sqrt{3}, \quad e' (HT) = 1, \quad e' (TH) = \pi, \quad e' (TT) = 0 
\]
The events are not going to be completely arbitrary functions,
however. We will insist on some conditions:...

This generalization

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conventional Quantum Mechanics}

Attempting to modify the probability measure to be set-valued, while keeping the rest of the mathematical framework of quantum mechanics intact leads to a contradiction. More precisely, it is not possible to maintain infinite precision probability amplitudes in the presence of set-valued probabilities without violating essential aspects of quantum theory. 

\ldots explain and give theorem

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete Quantum Theory}

The next question to ask is therefore whether the infinite precision of probability amplitudes is itself justified. If all measurements are finite and all probabilities are computable, then it is plausible that the internal mathematical representation of quantum states should also be based on countable computable entities. 
