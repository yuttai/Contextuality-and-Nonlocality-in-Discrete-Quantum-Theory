\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsfonts, verbatim}
\usepackage{proof}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{comment}
\usepackage{natbib}
\usepackage{url}
\usepackage{cmll}
\usepackage{bussproofs}
\usepackage{bbold}
\bibpunct();A{},
\bibliographystyle{plainnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Environments

\newenvironment{code}{\verbatim}{\endverbatim}
\newenvironment{smallcode}{\verbatim}{\endverbatim}
\newenvironment{verysmallcode}{\scriptsize\verbatim}{\endverbatim\normalsize}
\newtheorem{thm}{Theorem}[section]
\newtheorem{definition}[thm]{Definition}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{example}[thm]{Example}

%% Commands

\newcommand{\hide}[1]{}
\newcommand{\zeroH}{\mathbb{0}}
\newcommand{\oneH}{\mathbb{1}}
\newcommand{\alt}{~|~} 
\newcommand{\liftt}[1]{\llbracket #1 \rrbracket}
\newcommand{\ie}{\textit{i.e.}\xspace}
\newcommand{\eg}{\textit{e.g.}\xspace}
\newcommand{\QM}{QM}
\newcommand{\blob}{\rule[.2ex]{1ex}{1ex}}
\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle#1|}
\newcommand{\ip}[2]{\langle #1 \alt #2 \rangle}
\newcommand{\op}[2]{|#1\rangle\langle #2|}
\newcommand{\expect}[2]{\langle #1 \alt #2 \alt #1 \rangle}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\inleft}[1]{\textsf{inL}~#1}
\newcommand{\inright}[1]{\textsf{inR}~#1}
\newcommand{\fst}[1]{\textsf{fst}~#1}
\newcommand{\snd}[1]{\textsf{snd}~#1}
\newcommand{\scalarZ}{\texttt{impossible}}
\newcommand{\scalarO}{\texttt{possible}}
\newcommand{\norm}[1]{||#1||}
\newcommand{\scalarPlus}{\ensuremath{\oplus}}
\newcommand{\scalarTimes}{\ensuremath{\wedge}}
\newcommand{\vect}[1]{\overline{#1}}
\newcommand{\dvect}[1]{\overline{\overline{#1}}}
\newcommand{\ltree}{\swarrow}
\newcommand{\rrtree}{\begin{array}{c@{\hspace{-0pt}}c}\searrow\\&\searrow\end{array}}
\newcommand{\rltree}{\begin{array}{c@{\hspace{-7pt}}c}\searrow\\&\swarrow\end{array}}

\newcommand{\todo}[1]{\textbf{TODO}: #1}
\newcommand{\note}[1]{\textsc{\textbf{NOTE: #1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Facilities, Equipment, and Other Resources at Indiana University}
\author{}
\date{}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent There are substantial central computing resources offered by
the University: our cyberinfrastructure is rated in the top 10 in the
U.S, and includes the Big Red cluster (30 TeraFlops), the Quarry
cluster (9 TeraFlops, general-purpose Unix computing), and supports
TeraGrid, I-Light, and the Open Science Grid. Dr. Ortiz (Department of
Physics) and Dr. Sabry (School of Informatics and Computing) have
access to departmental license software, as well as access to most
electronic and hard copy journals through Indiana University
Libraries, and access to conference facilities for meetings.

\bigskip
\centerline{\Large{\underline{Department of Physics}}}
\bigskip

\noindent A computer server facility specifically serving the Departments of
Physics and Astronomy is installed in the basement of Swain Hall. This $\sim
110~m^2$ room has sufficient power and cooling capacity to house 24 racks of
hardware in a single area for ease of security and maintenance resulting in a
high-quality, small-scale facility meeting local needs. In addition to the
ATLAS/DØ Tier-3 Analysis Cluster (22 eight-core 2.66/2.88 GHz Xeon nodes, >60
TB of raid disk storage), it also houses experimental computing for lattice
gauge and related calculations, the special partial-wave computing facility
for the GlueX experiment (including both regular nodes and an experimental
GPU cluster), computing access for the neutrino group, and servers for the
Physics and Astronomy Departments. Finally, it also provides an appropriate
home for interfaces to hardware such as IU\u2019s Data Capacitor, a
large-scale data storage/access system developed by Indiana University IT
Services and used heavily by Astronomy.  Indiana University and the
University of Chicago jointly manage the Midwest Tier2 (MWT2) computing
center, with hardware split approximately equally between them. The Indiana
University portion of the MWT2 is located primarily in Indianapolis on the
campus of Indiana University Purdue University Indianapolis
(IUPUI). Currently the IUPUI portion of the MWT2 has 1.8M kSi2k of computing
power and ~200 TB of storage. The CPUs are various AMD and Intel processors
purchased from several vendors \u2013 a total of 768 cores at IUPUI with
approximately 2 GB of memory per core. A 1 Gb/s link connects each compute
node to the Indiana University backbone while storage nodes are connected to
the backbone with 10 Gb/s connections. The MWT2 is connected to the outside
world with a 10 Gb/s link.  In addition to the usual desktop units, a
dedicated room is available containing projection units and a high-definition
Polycom videoconferencing unit recently acquired using DoE stimulus
funds. The Department has also requested renovation of a different space to
house this system as well as other equipment for use in distance learning.

\bigskip
\centerline{\Large{\underline{School of Informatics and Computing}}}
\bigskip

\noindent The School of Informatics and Computing maintains a highly
distributed, heterogeneous, networked computing environment consisting of
UNIX/Linux, Windows, and Macintosh workstations and servers across 3 primary
facilities (2 auxiliary). This includes over 250 Linux workstations/servers,
over 450 Windows workstations/servers and over 150 Apple
workstations/servers. There are 3 Linux clusters (128-node dual Opteron,
16-node dual Opteron, and 8-node dual Xeon) to support School
research. Additionally, there are 2 dedicated virtual machine servers
available to host VMs for both research and instruction.

The network infrastructure at the School provides 1000Mbps gigabit ethernet
connections to all servers and most workstations and as well as 802.11a/b/g
wireless connectivity. The facilities have 1 gigabit or greater connectivity
(one facility has 10 gigabit) with campus and research backbones which
provides high-speed access to university and worldwide computing resources.

Special-purpose teaching and instructional labs are provided, including 2
electronic classrooms (24-seat, 48-seat), hardware laboratories, and 4
multi-purpose Linux and Windows laboratories to support the course work for
both undergraduate and graduate students.  There are 4 rooms (2 classrooms)
within the School enabled with video conferencing technology that can be used
for both distance education and research collaboration.

The School maintains 10 full time staff members dedicated to IT
infrastructure and facilities with the mission of enabling world class
research and instruction

\end{document}
